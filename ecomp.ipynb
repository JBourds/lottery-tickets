{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6278b811-d488-4ff7-8a1d-23fd73d96c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.harness import architecture as arch\n",
    "\n",
    "import copy\n",
    "import functools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Any, Callable, Dict, Iterable, List, Literal, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45844a01-f5a7-453a-b295-fb37781ef74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Typedefs\n",
    "Genome = List[tf.Tensor]\n",
    "GenomeInit = Callable[[keras.Model], Genome]\n",
    "GenomeMetricCallback = Callable[[Dict, Genome], Any]\n",
    "\n",
    "class Individual:\n",
    "    # One shared copy throughout the class (all individuals in population share same original weights)\n",
    "    ARCHITECTURE = None\n",
    "    MODEL = None\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        architecture: arch.Architecture, \n",
    "        genome_init: GenomeInit,\n",
    "    ):\n",
    "        # If this is the first instance of the class, initialize it with read only copies of data\n",
    "        if self.ARCHITECTURE is None:\n",
    "            self.ARCHITECTURE = architecture\n",
    "            self.MODEL = self.ARCHITECTURE.get_model_constructor()()\n",
    "            self.DATA = self.ARCHITECTURE.load_data()\n",
    "            \n",
    "        self.genome = genome_init(self.model)\n",
    "        self.rng = np.random.default_rng()\n",
    "        self.metrics = {}\n",
    "        \n",
    "    @staticmethod\n",
    "    def copy_from(individual: Literal['Individual']) -> Literal['Individual']:\n",
    "        copied = copy.deepcopy(individual)\n",
    "        copied.metrics.clear()\n",
    "        copied.rng = np.random.default_rng()\n",
    "        return copied\n",
    "        \n",
    "    @property\n",
    "    def fitness(self) -> float | None:\n",
    "        return self.metrics.get(\"mean_accuracy\")\n",
    "    \n",
    "    @property\n",
    "    def architecture(self) -> arch.Architecture | None:\n",
    "        return self.ARCHITECTURE\n",
    "    \n",
    "    @property\n",
    "    def model(self) -> keras.Model | None:\n",
    "        return self.MODEL\n",
    "    \n",
    "    @property\n",
    "    def data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray] | None:\n",
    "        return self.DATA\n",
    "    \n",
    "    @property\n",
    "    def training_data(self) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        if self.data is not None:\n",
    "            X_train, _, Y_train, _ = self.data\n",
    "            return X_train, Y_train\n",
    "        \n",
    "    @property\n",
    "    def test_data(self) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        if self.data is not None:\n",
    "            _, X_test, _, Y_test = self.data\n",
    "            return X_test, Y_test\n",
    "        \n",
    "    def copy_model(self) -> keras.Model | None:\n",
    "        if self.model is not None:\n",
    "            return copy.deepcopy(self.model)\n",
    "        \n",
    "    def sample_mask(self) -> Genome:\n",
    "        return [\n",
    "            tf.cast(\n",
    "                np.random.uniform(low=0, high=1, size=probabilities.shape)\n",
    "                > probabilities,\n",
    "                tf.float32,\n",
    "            )\n",
    "            for probabilities in self.genome\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def eval_fitness(individual: Literal['Individual'], num_evaluations: int = 5):\n",
    "        model = individual.copy_model()\n",
    "        model.compile(\n",
    "            loss=keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=[keras.metrics.CategoricalAccuracy()]\n",
    "        )\n",
    "        X_test, Y_test = individual.test_data\n",
    "        \n",
    "        losses = np.zeros(num_evaluations)\n",
    "        accuracies = np.zeros(num_evaluations)\n",
    "        for i in range(num_evaluations):\n",
    "            mask = individual.sample_mask()\n",
    "            model.set_weights([w * m for w, m in zip(individual.model.get_weights(), mask)])\n",
    "            losses[i], accuracies[i] = model.evaluate(X_test, Y_test)\n",
    "        individual.metrics[\"mean_loss\"] = np.mean(losses)\n",
    "        individual.metrics[\"mean_accuracy\"] = np.mean(accuracies)\n",
    "    \n",
    "    # Mutation methods\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutate_perturb(individual: Literal['Individual'], rate: float, scale: float):\n",
    "        for layer_index, layer in enumerate(individual.genome):\n",
    "            perturb_mask = (np.random.uniform(\n",
    "                low=0, \n",
    "                high=1, \n",
    "                size=layer.shape,\n",
    "            ) > rate).astype(np.int8)\n",
    "            perturbations = individual.rng.normal(\n",
    "                loc=0,\n",
    "                scale=scale,\n",
    "                size=layer.shape,\n",
    "            ) * perturb_mask\n",
    "            individual.genome[layer_index] = np.clip(\n",
    "                layer + perturbations, \n",
    "                a_min=0,\n",
    "                a_max=1,\n",
    "            )\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutate_resample(individual: Literal['Individual'], rate: 0.05):\n",
    "        for layer_index, layer in enumerate(individual.genome):\n",
    "            perturb_mask = np.random.uniform(\n",
    "                low=0, \n",
    "                high=1, \n",
    "                size=layer.shape,\n",
    "            ) > rate\n",
    "            resampled = np.random.uniform(\n",
    "                low=0, \n",
    "                high=1, \n",
    "                size=layer.shape,\n",
    "            )\n",
    "            np.place(layer, perturb_mask, resampled)\n",
    "    \n",
    "    # Initialization methods\n",
    "    \n",
    "    @staticmethod\n",
    "    def full_mask_init(model: keras.Model) -> Genome:\n",
    "        return [\n",
    "            np.ones_like(\n",
    "                input=weights,\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "            for weights in model.get_weights()\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_init(model: keras.Model) -> Genome:\n",
    "        return [\n",
    "            np.random.uniform(low=0, high=1, size=weights.shape)\n",
    "            for weights in model.get_weights()\n",
    "        ]\n",
    "    \n",
    "    # Crossover methods\n",
    "    \n",
    "    @staticmethod\n",
    "    def layer_crossover(p1: Literal['Individual'], p2: Literal['Individual']) -> Iterable[Literal['Individual']]:\n",
    "        child1, child2 = list(map(Individual.copy_from, (p1, p2)))\n",
    "        p1_weights = p1.genome\n",
    "        p2_weights = p2.genome\n",
    "        parents = np.random.randint(low=0, high=2, size=len(p1_weights))\n",
    "        child1.genome = copy.deepcopy([\n",
    "            p1_weights[layer_index] if parent == 0 \n",
    "            else p2_weights[layer_index] \n",
    "            for layer_index, parent in enumerate(parents)\n",
    "        ])\n",
    "        child2.genome = copy.deepcopy([\n",
    "            p2_weights[layer_index] if parent == 0 \n",
    "            else p1_weights[layer_index] \n",
    "            for layer_index, parent in enumerate(parents)\n",
    "        ])\n",
    "        return child1, child2\n",
    "    \n",
    "    @staticmethod\n",
    "    def neuron_crossover(p1: Literal['Individual'], p2: Literal['Individual']) -> Iterable[Literal['Individual']]:\n",
    "        child1, child2 = list(map(Individual.copy_from, (p1, p2)))\n",
    "        p1_weights = p1.genome\n",
    "        p2_weights = p2.genome\n",
    "        for layer_index, weights in enumerate(p1_weights):\n",
    "            # Generate a 0/1 for each row, then extend it across all outgoing synapses\n",
    "            parents = np.repeat(\n",
    "                np.random.randint(low=0, high=2, size=weights.shape[0]),\n",
    "                1 if weights.ndim == 1 else weights.shape[1],\n",
    "                axis=0,\n",
    "            ).reshape((weights.shape))\n",
    "            inverse_parents = np.logical_not(parents).astype(np.int8)\n",
    "            \n",
    "            # This multiplication uses masks to perform selection\n",
    "            child1.genome[layer_index] = p1_weights[layer_index] * parents \\\n",
    "                + p2_weights[layer_index] * inverse_parents\n",
    "            child2.genome[layer_index] = p2_weights[layer_index] * parents \\\n",
    "                + p1_weights[layer_index] * inverse_parents\n",
    "        return child1, child2\n",
    "    \n",
    "    @staticmethod\n",
    "    def synapse_crossover(p1: Literal['Individual'], p2: Literal['Individual']) -> Iterable[Literal['Individual']]:\n",
    "        child1, child2 = list(map(Individual.copy_from, (p1, p2)))\n",
    "        p1_weights = p1.genome\n",
    "        p2_weights = p2.genome\n",
    "        for layer_index, weights in enumerate(p1_weights):\n",
    "            # Generate a 0/1 for each row, then extend it across all outgoing synapses\n",
    "            parents = np.random.randint(low=0, high=2, size=weights.shape)\n",
    "            inverse_parents = np.logical_not(parents).astype(np.int8)\n",
    "            \n",
    "            # This multiplication uses masks to perform selection\n",
    "            child1.genome[layer_index] = p1_weights[layer_index] * parents \\\n",
    "                + p2_weights[layer_index] * inverse_parents\n",
    "            child2.genome[layer_index] = p2_weights[layer_index] * parents \\\n",
    "                + p1_weights[layer_index] * inverse_parents\n",
    "        return child1, child2\n",
    "\n",
    "# Typedefs\n",
    "Mutation = Callable[[Individual], None]\n",
    "Crossover = Callable[[Individual, Individual], Individual]\n",
    "FitnessFunction = Callable[[Individual], float]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3e1c5ad-9692-427e-941e-be9632c13e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = arch.Architecture('lenet', 'mnist')\n",
    "p1 = Individual(a, Individual.random_init)\n",
    "p2 = Individual(a, Individual.random_init)\n",
    "c1, c2 = Individual.layer_crossover(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "060a1f85-90c4-47da-826e-cbc07fd06c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (µ + λ) strategy\n",
    "def evolutionary_algorithm(\n",
    "    num_generations: int,\n",
    "    num_parents: int,\n",
    "    num_children: int,\n",
    "    tournament_size: int,\n",
    "    num_tournament_winners: int,\n",
    "    individual_constructor: Callable[[], Individual],\n",
    "    fitness_eval: FitnessFunction,\n",
    "    crossover: Crossover | None = None,\n",
    "    mutations: List[Mutation] = [],\n",
    "    genome_metric_callbacks: List[GenomeMetricCallback] = [],\n",
    "):\n",
    "    if num_tournament_winners > tournament_size:\n",
    "        raise ValueError(\"Cannot have more tournament winners than participants\")\n",
    "        \n",
    "    best_solution = None\n",
    "    best_fitness = -np.inf\n",
    "    genome_metrics = {\"best_solution_fitness\": np.zeros(num_generations)}\n",
    "    \n",
    "    # Create and evaluate the initial population\n",
    "    population = []\n",
    "    for _ in range(num_parents):\n",
    "        individual = individual_constructor()\n",
    "        fitness_eval(individual)\n",
    "        if individual.fitness > best_fitness:\n",
    "            best_solution = copy.deepcopy(individual)\n",
    "            best_fitness = best_solution.fitness\n",
    "        population.append(individual)\n",
    "    \n",
    "    for generation_index in range(num_generations):\n",
    "        children = []\n",
    "        while len(children) < num_children:\n",
    "            parents = np.random.choice(population, 2)\n",
    "            new_children = crossover(*parents) if crossover else list(map(Individual.copy_from, parents))\n",
    "            \n",
    "            for child in new_children:\n",
    "                for mutation in mutations:\n",
    "                    mutation(child)\n",
    "                fitness_eval(child)\n",
    "                if child.fitness > best_fitness:\n",
    "                    best_solution = child\n",
    "                    best_fitness = best_solution.fitness\n",
    "            children.extend(new_children)\n",
    "        population.extend(children)\n",
    "        \n",
    "        # Seed next generation with best solution found thus far\n",
    "        next_generation = [best_solution]\n",
    "        while len(next_generation) < num_parents:\n",
    "            tournament = sorted(\n",
    "                np.random.choice(population, size=tournament_size),\n",
    "                key = lambda x: x.fitness,\n",
    "                reverse=True,\n",
    "            )\n",
    "            next_generation.extend(tournament[:num_tournament_winners])\n",
    "        population = next_generation\n",
    "        \n",
    "        for callback in genome_metric_callbacks:\n",
    "            callback(genome_metrics, population)\n",
    "        genome_metrics[\"best_solution_fitness\"][generation_index] = best_fitness\n",
    "        \n",
    "    return genome_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb092467-0f63-45c7-bb94-7ff152acd326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1222 - loss: 2.3061\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1474 - loss: 2.2946\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1317 - loss: 2.3073\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1134 - loss: 2.3011\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.0713 - loss: 2.3242\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1028 - loss: 2.3164\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.0803 - loss: 2.3151\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0820 - loss: 2.3052\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0893 - loss: 2.3000\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1177 - loss: 2.2939\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.0960 - loss: 2.3046\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1107 - loss: 2.3070\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0760 - loss: 2.3217\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0711 - loss: 2.3052\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1059 - loss: 2.3016\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1331 - loss: 2.2961\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1163 - loss: 2.3068\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1270 - loss: 2.2900\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0803 - loss: 2.3175\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0667 - loss: 2.2995\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0940 - loss: 2.3149\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1177 - loss: 2.3049\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0773 - loss: 2.3135\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.0851 - loss: 2.3106\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1155 - loss: 2.3000\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1538 - loss: 2.3015\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1165 - loss: 2.3148\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1075 - loss: 2.3166\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1040 - loss: 2.3047\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1373 - loss: 2.3182\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.1238 - loss: 2.3012\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0682 - loss: 2.3114\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.0714 - loss: 2.3174\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - categorical_accuracy: 0.1191 - loss: 2.3001\n"
     ]
    }
   ],
   "source": [
    "num_runs = 2\n",
    "\n",
    "individual_constructor = functools.partial(\n",
    "    Individual, \n",
    "    architecture=arch.Architecture('lenet', 'mnist'), \n",
    "    genome_init=Individual.random_init,\n",
    ")\n",
    "fitness_eval = functools.partial(Individual.eval_fitness, num_evaluations=1)\n",
    "\n",
    "mutations = [\n",
    "    functools.partial(Individual.mutate_resample, rate=0.05),\n",
    "    functools.partial(Individual.mutate_perturb, rate=0.1, scale=0.025),\n",
    "]\n",
    "def dummy_callback(data: Dict, *args):\n",
    "    if data.get(\"dummy\") is None:\n",
    "        data[\"dummy\"] = []\n",
    "    data[\"dummy\"].append(len(data[\"dummy\"]))\n",
    "    \n",
    "genome_metric_callbacks = [\n",
    "    dummy_callback,\n",
    "]\n",
    "kwargs = {\n",
    "    \"num_generations\": 2,\n",
    "    \"num_parents\": 5,\n",
    "    \"num_children\": 5,\n",
    "    \"tournament_size\": 4,\n",
    "    \"num_tournament_winners\": 2,\n",
    "    \"individual_constructor\": individual_constructor,\n",
    "    # Construction kwargs\n",
    "    \"fitness_eval\": fitness_eval,\n",
    "    \"mutations\": mutations,\n",
    "    \"crossover\": Individual.neuron_crossover,\n",
    "    \"genome_metric_callbacks\": genome_metric_callbacks,\n",
    "}\n",
    "\n",
    "metrics = []\n",
    "for _ in range(num_runs):\n",
    "    metrics.append(evolutionary_algorithm(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1abde69-68ce-44d0-b278-06a399d9a785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'best_solution_fitness': array([0.1468, 0.1468]), 'dummy': [0, 1]},\n",
       " {'best_solution_fitness': array([0.14399999, 0.14399999]), 'dummy': [0, 1]}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lt",
   "language": "python",
   "name": "lt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
