{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "experimentation.ipynb\n",
    "\n",
    "File for experimenting with new things\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "\n",
    "from src.harness.constants import Constants as C\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "# from src.harness.experiment import experiment, ExperimentData\n",
    "from src.harness.model import create_model, LeNet300, load_model\n",
    "from src.harness.pruning import prune_by_percent\n",
    "from src.harness.training import TrainingRound\n",
    "from src.lottery_ticket.foundations import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DEBUG: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_and_process_mnist()\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "Y_train = tf.convert_to_tensor(Y_train, dtype=tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "Y_test = tf.convert_to_tensor(Y_test, dtype=tf.float32)\n",
    "\n",
    "# Create a model with the same architecture using all Keras components to check its accuracy with the same parameters\n",
    "def create_lenet_300_100(random_seed: int, input_shape: tuple[int, ...], num_classes: int) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Simple hardcoded class definition for creating the sequential Keras equivalent to LeNet-300-100.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set seeds for reproducability\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "    model = keras.Sequential(name=\"LeNet-300-100\")\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "    model.add(keras.layers.Dense(300, activation='relu'))\n",
    "    model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def pruned_nn(\n",
    "        random_seed: int, \n",
    "        input_shape: tuple[int, ...], \n",
    "        num_classes: int, \n",
    "        pruning_params: dict, \n",
    "        loss: keras.losses.Loss = keras.losses.categorical_crossentropy, \n",
    "        optimizer=C.OPTIMIZER()) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Function to define the architecture of a neural network model\n",
    "    following 300 100 architecture for MNIST dataset and using\n",
    "    provided parameter which are used to prune the model.\n",
    "    \n",
    "    Input: 'pruning_params' Python 3 dictionary containing parameters which are used for pruning\n",
    "    Output: Returns designed and compiled neural network model\n",
    "    \"\"\"\n",
    "    # Set seeds for reproducability\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "    model = sparsity.prune_low_magnitude(keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=input_shape),\n",
    "        keras.layers.Dense(units = 300, activation='relu', kernel_initializer=tf.initializers.GlorotUniform()),\n",
    "        # model.add(l.Dropout(0.2))\n",
    "        keras.layers.Dense(units = 100, activation='relu', kernel_initializer=tf.initializers.GlorotUniform()),\n",
    "        # model.add(l.Dropout(0.1))\n",
    "        keras.layers.Dense(units = num_classes, activation='softmax')\n",
    "    ]), **pruning_params)\n",
    "    \n",
    "    # Compile pruned CNN-\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_masked_nn(*args) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Create a masked neural network where all the weights are initialized to 1s.\n",
    "    \"\"\"\n",
    "    model: keras.Model = pruned_nn(*args)\n",
    "    model_stripped = sparsity.strip_pruning(model)\n",
    "    # Assign all weights to 1 to start\n",
    "    for weights in model_stripped.trainable_weights:\n",
    "        weights.assign(\n",
    "            tf.ones_like(\n",
    "                input = weights,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pruning parameters and callback\n",
    "def create_pruning_parameters(target_sparsity: float, begin_Step: int, end_step: int, frequency: int) -> dict:\n",
    "    \"\"\"\n",
    "    Create the dictionary of pruning parameters to be used.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'pruning_schedule': sparsity.ConstantSparsity(\n",
    "            target_sparsity=target_sparsity, \n",
    "            begin_step=begin_Step,\n",
    "            end_step=end_step, \n",
    "            frequency=frequency\n",
    "        )\n",
    "    }\n",
    "\n",
    "def create_pruning_callback(monitor: str, patience: int, minimum_delta: float) -> list:\n",
    "    \"\"\"\n",
    "    Create a callback to be performed during pruning.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        sparsity.UpdatePruningStep(),\n",
    "        # sparsity.PruningSummaries(log_dir = logdir, profile_batch=0),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=monitor, \n",
    "            patience=patience,\n",
    "            min_delta=minimum_delta\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def create_pruning_percentages():\n",
    "    pass\n",
    "\n",
    "pruning_params_unpruned: dict = create_pruning_parameters(0.01, 0, 0, 100)\n",
    "pruning_callback: list = create_pruning_callback('val_loss', 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each layer, there are synaptic connections from the previous layer and the neurons\n",
    "def get_layer_weight_counts(model: keras.Model) -> list[int]:\n",
    "    \"\"\"\n",
    "    Function to return a list of integer values for the number of \n",
    "    parameters in each layer.\n",
    "    \"\"\"\n",
    "    def get_num_layer_weights(layer: keras.layers.Layer) -> int:\n",
    "        layer_weight_count: int = 0\n",
    "        weights: list[np.array] = layer.get_weights()\n",
    "\n",
    "        for idx in range(len(weights))[::2]:\n",
    "            synapses: np.ndarray = weights[idx]\n",
    "            neurons: np.array = weights[idx + 1]\n",
    "            layer_weight_count += np.prod(synapses.shape) + np.prod(neurons.shape)\n",
    "\n",
    "        return layer_weight_count\n",
    "    \n",
    "    return list(map(get_num_layer_weights, model.layers))\n",
    "\n",
    "def get_pruning_percents(\n",
    "        layer_weight_counts: list[int], \n",
    "        first_step_pruning_percent: float,\n",
    "        target_sparsity: float\n",
    "        ) -> list[np.array]:\n",
    "    \"\"\"\n",
    "    Function to get arrays of model sparsity at each step of pruning.\n",
    "    \"\"\"\n",
    "\n",
    "    def total_sparsity(\n",
    "            original_weight_counts: list[int], \n",
    "            current_weight_counts: list[int]\n",
    "            ) -> float:\n",
    "        \"\"\"\n",
    "        Helper function to calculate total sparsity of parameters.\n",
    "        \"\"\"\n",
    "        return np.sum(current_weight_counts) / np.sum(original_weight_counts)\n",
    "    \n",
    "    def sparsify(\n",
    "            original_weight_counts: list[int], \n",
    "            current_weight_counts: list[int], \n",
    "            original_pruning_percent: float\n",
    "            ) -> list[float]:\n",
    "        sparsities: list[float] = []\n",
    "        for idx, (original, current) in enumerate(zip(original_weight_counts, current_weight_counts)):\n",
    "            if original == 0:\n",
    "                continue\n",
    "            new_weight_count: int = np.round(current * (1 - original_pruning_percent))\n",
    "            sparsities.append((original - new_weight_count) / original)\n",
    "            current_weight_counts[idx] = new_weight_count\n",
    "        return np.round(np.mean(sparsities), decimals=5)\n",
    "    \n",
    "    sparsities: list[float] = []\n",
    "    \n",
    "    # Elementwise copy\n",
    "    current_weight_counts: list[int] = [weight_count for weight_count in layer_weight_counts]\n",
    "    \n",
    "    while total_sparsity(layer_weight_counts, current_weight_counts) > target_sparsity:\n",
    "        sparsities.append(sparsify(layer_weight_counts, current_weight_counts, first_step_pruning_percent))\n",
    "\n",
    "    return sparsities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nonzero_parameters(model: keras.Model):\n",
    "    \"\"\"\n",
    "    Print summary for the number of nonzero parameters in the model.\n",
    "    \"\"\"\n",
    "    model_sum_params: int = 0\n",
    "    model_stripped: keras.Model = sparsity.strip_pruning(model)\n",
    "    weights: list[np.ndarray] = model_stripped.trainable_weights\n",
    "    for idx in range(len(weights))[::2]:\n",
    "        layer_number: int = int(idx / 2)\n",
    "        synapses: np.ndarray = weights[idx]\n",
    "        nonzero_synapses: int = tf.math.count_nonzero(synapses, axis=None).numpy()\n",
    "        neurons: np.array = weights[idx + 1]\n",
    "        nonzero_neurons: int = tf.math.count_nonzero(neurons, axis=None).numpy()\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f'Nonzero parameters in layer {layer_number} synapses:', nonzero_synapses)\n",
    "            print(f'Nonzero parameters in layer {layer_number} neurons:', nonzero_neurons)\n",
    "        \n",
    "        model_sum_params += nonzero_synapses + nonzero_neurons\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(f'Total nonzero parameters: {model_sum_params}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics() -> tuple[tf.keras.metrics.Metric, ...]:\n",
    "    \"\"\"\n",
    "    Create metrics to measure the error and accuracy of the model.\n",
    "    \"\"\"\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    return train_loss, train_accuracy, test_loss, test_accuracy\n",
    "\n",
    "train_loss, train_accuracy, test_loss, test_accuracy = create_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MNIST for training\n",
    "X_train, Y_train, X_test, Y_test = load_and_process_mnist()\n",
    "train_dataset: tuple[np.array, np.array] = (X_train, Y_train)\n",
    "test_dataset: tuple[np.array, np.array] = (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordan/anaconda3/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Sequential.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a masked model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m args: \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, X_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;241m10\u001b[39m, pruning_params_unpruned)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m sparsity\u001b[38;5;241m.\u001b[39mstrip_pruning(pruned_nn(\u001b[38;5;241m*\u001b[39margs))\n\u001b[1;32m      4\u001b[0m mask_model \u001b[38;5;241m=\u001b[39m sparsity\u001b[38;5;241m.\u001b[39mstrip_pruning(create_masked_nn(\u001b[38;5;241m*\u001b[39margs))\n\u001b[1;32m      5\u001b[0m layer_weight_counts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m get_layer_weight_counts(mask_model)\n",
      "Cell \u001b[0;32mIn[39], line 47\u001b[0m, in \u001b[0;36mpruned_nn\u001b[0;34m(random_seed, input_shape, num_classes, pruning_params, loss, optimizer)\u001b[0m\n\u001b[1;32m     44\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(random_seed)\n\u001b[1;32m     45\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(random_seed)\n\u001b[0;32m---> 47\u001b[0m model \u001b[38;5;241m=\u001b[39m sparsity\u001b[38;5;241m.\u001b[39mprune_low_magnitude(keras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m     48\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten(input_shape\u001b[38;5;241m=\u001b[39minput_shape),\n\u001b[1;32m     49\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_initializer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mGlorotUniform()),\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# model.add(l.Dropout(0.2))\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_initializer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mGlorotUniform()),\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# model.add(l.Dropout(0.1))\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(units \u001b[38;5;241m=\u001b[39m num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m ]), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpruning_params)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Compile pruned CNN-\u001b[39;00m\n\u001b[1;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     58\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     59\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     60\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:74\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_FAILURE_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow_model_optimization/python/core/keras/metrics.py:69\u001b[0m, in \u001b[0;36mMonitorBoolGauge.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     results \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbool_gauge\u001b[38;5;241m.\u001b[39mget_cell(MonitorBoolGauge\u001b[38;5;241m.\u001b[39m_SUCCESS_LABEL)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/prune.py:216\u001b[0m, in \u001b[0;36mprune_low_magnitude\u001b[0;34m(to_prune, pruning_schedule, block_size, block_pooling_type, pruning_policy, sparsity_m_by_n, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m pruning_wrapper\u001b[38;5;241m.\u001b[39mPruneLowMagnitude(to_prune, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`prune_low_magnitude` can only prune an object of the following \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtypes: keras.models.Sequential, keras functional model, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    219\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeras.layers.Layer, list of keras.layers.Layer. You passed \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    220\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man object of type: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mto_prune\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    221\u001b[0m   )\n",
      "\u001b[0;31mValueError\u001b[0m: `prune_low_magnitude` can only prune an object of the following types: keras.models.Sequential, keras functional model, keras.layers.Layer, list of keras.layers.Layer. You passed an object of type: Sequential."
     ]
    }
   ],
   "source": [
    "# Create a masked model\n",
    "args: tuple = (0, X_train[0].shape, 10, pruning_params_unpruned)\n",
    "model = sparsity.strip_pruning(pruned_nn(*args))\n",
    "mask_model = sparsity.strip_pruning(create_masked_nn(*args))\n",
    "layer_weight_counts: list[int] = get_layer_weight_counts(mask_model)\n",
    "sparsities: list[float] = get_pruning_percents(layer_weight_counts, .2, .01)\n",
    "num_pruning_rounds: int = len(sparsities)\n",
    "count_nonzero_parameters(mask_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_step(model: keras.Model,\n",
    "                   mask_model: keras.Model, \n",
    "                   inputs: tf.Tensor, \n",
    "                   labels: tf.Tensor,\n",
    "                   loss_fn: callable = keras.losses.CategoricalCrossentropy(), \n",
    "                   optimizer: keras.optimizers.Optimizer = C.OPTIMIZER(), \n",
    "                   ):\n",
    "    \"\"\"\n",
    "    Function to compute one step of gradient descent optimization\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make predictions using defined model-\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(labels, y_pred)\n",
    "\n",
    "    # Compute gradients with respect to defined loss and weights and biases\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # List to hold element-wise multiplication between\n",
    "    # computed gradient and masks\n",
    "    grad_mask_mul = []\n",
    "\n",
    "    # Perform element-wise multiplication between computed gradients and masks\n",
    "    for grad_layer, mask in zip(gradients, mask_model.trainable_weights):\n",
    "        grad_mask_mul.append(tf.math.multiply(grad_layer, mask))\n",
    "\n",
    "    # Apply computed gradients to model's weights and biases\n",
    "    optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))\n",
    "\n",
    "    # Compute accuracy\n",
    "    return train_loss(loss), train_accuracy(labels, y_pred)\n",
    "\n",
    "@tf.function\n",
    "def test_step(model: keras.Model, \n",
    "              data: tf.Tensor, \n",
    "              labels: tf.Tensor,\n",
    "              loss_fn: callable = keras.losses.CategoricalCrossentropy(),\n",
    "              ):\n",
    "    \"\"\"\n",
    "    Function to test model performance on testing dataset\n",
    "    \"\"\"\n",
    "    predictions = model(data)\n",
    "    return loss_fn(labels, predictions), test_accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'prune_low_magnitude_dense_432/kernel:0' shape=(784, 300) dtype=float32, numpy=\n",
      "array([[ 0.00233377, -0.02841079,  0.05042759, ...,  0.04471111,\n",
      "         0.0469005 , -0.00633286],\n",
      "       [ 0.04599051,  0.07108735, -0.06869254, ...,  0.0193451 ,\n",
      "        -0.03234061,  0.03169315],\n",
      "       [ 0.04332277,  0.00188898, -0.05098354, ...,  0.05931461,\n",
      "         0.0038614 , -0.06489589],\n",
      "       ...,\n",
      "       [ 0.06133612, -0.00425664, -0.03556142, ...,  0.05923072,\n",
      "        -0.04446548,  0.03342723],\n",
      "       [ 0.02262612, -0.01283204,  0.03786094, ..., -0.03319833,\n",
      "         0.0259892 , -0.02313284],\n",
      "       [ 0.00880335,  0.00193495, -0.07007851, ..., -0.05422467,\n",
      "        -0.0134997 , -0.03317983]], dtype=float32)>, <tf.Variable 'prune_low_magnitude_dense_432/bias:0' shape=(300,) dtype=float32, numpy=\n",
      "array([ 1.32970903e-02, -1.56251859e-04,  8.03746004e-03, -9.55416169e-03,\n",
      "        8.31857696e-03,  8.97290185e-03,  1.79232322e-02,  9.47246980e-03,\n",
      "        2.53446326e-02, -1.38653256e-03,  5.99070918e-03, -3.10830702e-03,\n",
      "        1.42925791e-02,  1.15674110e-02,  2.97107622e-02,  1.50591871e-02,\n",
      "        2.07202155e-02,  1.75761953e-02,  2.39425898e-02, -4.04225988e-03,\n",
      "        9.42185335e-03,  1.64223798e-02,  1.44285830e-02,  1.70698389e-03,\n",
      "       -1.40732236e-03,  3.39099690e-02,  2.28172401e-03,  3.61580290e-02,\n",
      "        1.70763694e-02,  1.29694566e-02,  3.43366060e-03, -7.26569863e-03,\n",
      "        1.09742954e-02,  2.30904147e-02, -3.64168151e-03,  1.79560650e-02,\n",
      "        2.28456259e-02,  3.24857868e-02,  1.63866002e-02,  1.91887636e-02,\n",
      "        1.28114363e-02,  5.00722835e-03,  3.47551480e-02,  2.04039831e-02,\n",
      "       -5.43523626e-03,  5.40012727e-03,  2.53270790e-02,  1.74738728e-02,\n",
      "       -3.19298264e-03,  2.26971647e-03, -1.39323631e-04,  3.29288957e-03,\n",
      "        6.37720618e-03,  1.67242438e-02, -1.29284943e-02,  1.28304483e-02,\n",
      "        2.39954144e-02, -3.15244240e-03, -2.13455362e-03,  6.77984767e-03,\n",
      "        1.72690079e-02,  9.26616229e-03, -5.01510641e-03,  1.73149873e-02,\n",
      "        2.25370377e-02,  1.38213206e-02,  1.12580070e-02,  1.58311855e-02,\n",
      "        7.88961421e-04,  4.08077706e-03,  2.15571430e-02,  9.84891783e-04,\n",
      "        1.43490098e-02,  4.41824272e-03,  1.34561025e-02, -3.30397568e-04,\n",
      "        7.72708468e-03, -8.12872779e-03,  8.73083435e-03,  2.55105412e-03,\n",
      "       -5.80060249e-03,  1.56139769e-02,  3.01646162e-02,  1.34385582e-02,\n",
      "       -3.99193596e-05,  1.09573156e-02, -4.58925962e-03,  1.98458619e-02,\n",
      "        1.75672825e-02,  1.79064795e-02,  2.63675489e-02,  1.34529220e-02,\n",
      "        1.13958223e-02,  1.89011067e-03,  6.65826211e-03, -2.54968903e-03,\n",
      "        1.49311563e-02,  1.95577741e-02, -1.04870193e-03,  2.83179618e-03,\n",
      "        1.26404101e-02,  2.63119768e-02,  6.46849349e-03,  7.80786807e-03,\n",
      "        2.05245204e-02, -7.51994259e-04,  5.24373725e-04,  6.49671210e-03,\n",
      "        2.58115772e-03, -3.73082352e-04,  1.67688839e-02,  6.34462526e-03,\n",
      "       -5.18252607e-03,  2.72412188e-02,  1.27763264e-02,  7.79445469e-03,\n",
      "        1.33003267e-02,  4.37239464e-03, -4.44657076e-03, -1.00546028e-03,\n",
      "        9.88783874e-03,  4.24661394e-03,  8.65377765e-03,  5.87252784e-04,\n",
      "        2.24082414e-02,  8.15047231e-03,  5.94303524e-03,  2.33066753e-02,\n",
      "        8.62195622e-03, -1.00576244e-02,  8.00006930e-03,  2.15876326e-02,\n",
      "        5.49563952e-03,  4.50341515e-02, -1.84238350e-04,  8.07005446e-03,\n",
      "        9.40984301e-03,  2.33735628e-02, -5.60497865e-03,  2.04546917e-02,\n",
      "        2.71790251e-02,  2.29672287e-02,  2.71625444e-03, -1.39606036e-02,\n",
      "        1.94672216e-02, -9.36052296e-03,  5.85118658e-04,  1.98372751e-02,\n",
      "        4.94774664e-03,  2.29662005e-03,  4.58098296e-03, -1.18387127e-02,\n",
      "        1.85482390e-02,  1.18907122e-02,  2.82213986e-02,  4.95481398e-03,\n",
      "       -7.33421464e-03,  1.73283871e-02, -2.97045149e-03,  1.12691866e-02,\n",
      "        1.14294747e-02,  1.61086321e-02,  1.49556231e-02,  1.50586059e-03,\n",
      "        1.30695999e-02,  7.67156901e-03, -2.20693275e-03, -2.04206468e-03,\n",
      "        5.29076485e-03,  1.04584415e-02, -5.97366656e-04, -1.29804779e-02,\n",
      "        8.14979058e-03,  2.13686954e-02,  1.55036869e-02, -6.57417229e-04,\n",
      "        3.11180651e-02,  3.15270573e-03,  8.09465535e-03,  2.41232421e-02,\n",
      "        3.25035714e-02,  1.15012340e-02,  4.64040227e-02,  1.45050855e-02,\n",
      "        1.50072286e-02, -5.39157353e-03,  1.60016622e-02,  5.44175691e-06,\n",
      "        1.17876548e-02, -1.21290400e-03,  2.08001733e-02,  1.26014352e-02,\n",
      "        3.59797501e-03,  2.10772012e-03,  1.62018742e-02,  1.38127049e-02,\n",
      "       -4.47385153e-03, -4.34236322e-03,  3.22344387e-03,  1.29632056e-02,\n",
      "       -1.49907432e-02,  6.34873798e-03,  4.20769723e-03, -3.96722089e-03,\n",
      "        4.16771928e-03,  3.06509249e-03, -1.97700877e-03,  2.30608415e-02,\n",
      "       -4.28083539e-03,  4.29889839e-03,  4.35299054e-03,  2.60122772e-03,\n",
      "       -3.11270216e-03, -7.90546299e-04,  3.25294584e-02,  1.47196958e-02,\n",
      "       -4.00453527e-03,  1.53531618e-02,  4.93410882e-03,  1.28234718e-02,\n",
      "       -4.80214600e-03,  9.61283874e-03,  7.81218521e-03,  1.22304894e-02,\n",
      "        5.58036333e-03, -2.64058635e-03,  3.98647366e-03,  4.55940841e-03,\n",
      "       -8.30925349e-03, -4.52026958e-03,  3.27653252e-02,  1.13454927e-02,\n",
      "        3.03007779e-03,  1.31378258e-02,  9.99365468e-03, -2.07164278e-03,\n",
      "        2.74128448e-02,  2.90704072e-02, -7.13674119e-03, -1.45868782e-03,\n",
      "        3.43060843e-03,  2.98894942e-02,  1.42040588e-02,  7.67849060e-03,\n",
      "        2.55569001e-03,  1.43870581e-02, -4.41141613e-03,  1.58504769e-02,\n",
      "        1.29660154e-02, -8.69260984e-04,  7.81332026e-04, -6.17991143e-04,\n",
      "       -6.68471400e-03,  9.73126106e-03,  2.09698617e-03, -1.40175596e-02,\n",
      "        7.58690527e-03,  8.85021407e-04, -1.18586579e-02,  1.12387436e-02,\n",
      "        3.39227566e-03,  1.35165323e-02, -1.24006439e-03,  9.91680659e-04,\n",
      "       -6.04759250e-03, -7.28777330e-03,  1.52960951e-02,  7.09676230e-03,\n",
      "        2.76625920e-02, -4.12904611e-03,  3.92233543e-02,  1.13552657e-03,\n",
      "       -1.11169275e-03,  4.72102966e-03, -4.12126770e-04,  1.26518589e-03,\n",
      "        3.38434096e-04,  1.33150034e-02,  1.29499305e-02, -4.01593233e-03,\n",
      "        9.52062756e-03,  4.90124850e-03,  9.53633711e-03,  1.46263149e-02,\n",
      "        1.17076188e-02, -2.53375899e-03,  1.87450717e-03, -1.18850707e-03,\n",
      "        2.12650318e-02,  1.53087627e-03,  2.46180687e-02,  1.40673469e-03,\n",
      "        2.27665789e-02,  2.07728446e-02,  4.93671000e-03, -8.61035660e-03,\n",
      "        2.77838157e-03,  3.96766374e-03,  2.99051497e-03,  2.78661307e-02],\n",
      "      dtype=float32)>, <tf.Variable 'prune_low_magnitude_dense_433/kernel:0' shape=(300, 100) dtype=float32, numpy=\n",
      "array([[-0.03088414, -0.05244637, -0.04771322, ...,  0.12873572,\n",
      "        -0.10387812,  0.01836991],\n",
      "       [-0.00331893, -0.02570526,  0.04918172, ..., -0.10790829,\n",
      "        -0.0656857 ,  0.02499688],\n",
      "       [-0.03059555, -0.03970266,  0.02034297, ..., -0.06138986,\n",
      "        -0.08907834, -0.04482209],\n",
      "       ...,\n",
      "       [-0.00420196,  0.05428075, -0.00685403, ..., -0.08648016,\n",
      "         0.01095167,  0.10548951],\n",
      "       [-0.09610064,  0.07181392, -0.08166139, ...,  0.08670443,\n",
      "         0.01242031,  0.10447887],\n",
      "       [ 0.11042126,  0.10308824, -0.04472924, ..., -0.12018657,\n",
      "         0.06954589,  0.01914153]], dtype=float32)>, <tf.Variable 'prune_low_magnitude_dense_433/bias:0' shape=(100,) dtype=float32, numpy=\n",
      "array([-9.7805569e-03, -1.0187398e-02,  1.1013431e-02,  4.3805461e-02,\n",
      "       -2.3364494e-02, -4.0540169e-03,  9.8598711e-03,  6.0073738e-03,\n",
      "       -1.1871136e-02,  2.7845600e-03, -1.7039837e-02, -2.6209024e-03,\n",
      "        1.0348057e-03,  5.3008262e-02, -3.8524156e-03,  1.5294999e-02,\n",
      "       -9.3274927e-03,  9.3725910e-03, -2.2891951e-03,  1.0834922e-02,\n",
      "        4.0153436e-02,  5.2716170e-02, -4.0412396e-03,  4.4493210e-02,\n",
      "        3.0524032e-02, -1.1386647e-02,  5.7027064e-02, -7.8177493e-04,\n",
      "       -1.4565078e-03, -2.2949528e-02, -1.4634542e-02, -1.1568150e-03,\n",
      "       -1.6445315e-02, -1.8618133e-02,  1.6384108e-02,  2.2247750e-02,\n",
      "       -3.1556902e-04, -6.6415663e-04,  1.5455819e-02, -4.1265339e-03,\n",
      "       -5.5312216e-03,  4.6262558e-04,  2.6303712e-02,  1.0490076e-02,\n",
      "        5.5667926e-02, -9.0057608e-03, -2.6606659e-03,  2.3961041e-02,\n",
      "        2.5650701e-02, -1.5859089e-03,  3.3334717e-02, -3.0102901e-02,\n",
      "        2.1515992e-02,  3.7124809e-02, -3.3156805e-02,  1.8966718e-02,\n",
      "        5.0936256e-02,  2.8543444e-02,  6.0326136e-03,  8.2765799e-03,\n",
      "        3.3155400e-02, -1.0230712e-02,  6.3051395e-03, -5.0184936e-03,\n",
      "       -3.6027122e-03,  9.7485110e-03,  1.1748695e-02, -5.0399634e-03,\n",
      "        3.1998917e-02,  2.7298363e-02,  3.8989928e-02,  2.7853984e-03,\n",
      "        5.1512294e-03,  1.7663095e-03, -7.2028575e-04, -9.1836303e-03,\n",
      "        4.7653176e-02,  1.1221770e-02,  1.2224923e-03,  4.3838476e-03,\n",
      "        2.6347127e-04, -6.8767709e-03, -1.2597176e-03,  1.6983399e-02,\n",
      "        2.4595184e-02, -1.3886742e-02,  2.8536810e-02, -5.1894225e-04,\n",
      "        1.9820938e-02,  7.3063151e-05,  3.2737724e-02, -2.4184946e-02,\n",
      "       -8.6113223e-04, -7.9745324e-03,  1.7135896e-02,  1.7252022e-02,\n",
      "        1.1260900e-02,  3.0710619e-02,  2.1926928e-03,  2.5035103e-04],\n",
      "      dtype=float32)>, <tf.Variable 'prune_low_magnitude_dense_434/kernel:0' shape=(100, 10) dtype=float32, numpy=\n",
      "array([[ 9.68768671e-02, -2.11986542e-01,  2.72291958e-01,\n",
      "         2.74504334e-01,  3.20971645e-02, -1.05628431e-01,\n",
      "         1.22727677e-01, -2.48742208e-01, -8.22312012e-02,\n",
      "         7.53441919e-03],\n",
      "       [ 1.33845229e-02, -2.34306499e-01,  2.52183050e-01,\n",
      "         1.97612241e-01, -1.52191904e-03, -2.68111169e-01,\n",
      "         2.38978237e-01,  1.27484113e-01, -6.27624337e-03,\n",
      "         2.03550279e-01],\n",
      "       [ 4.90047894e-02, -1.23949304e-01, -1.58897359e-02,\n",
      "         1.86924890e-01,  2.92538051e-02,  1.41852140e-01,\n",
      "        -4.97237593e-02,  1.88217804e-01, -7.94875156e-03,\n",
      "         1.97465554e-01],\n",
      "       [-3.36013943e-01,  1.05670765e-01,  3.72833908e-02,\n",
      "         9.92071778e-02, -4.02709357e-02, -6.50115833e-02,\n",
      "         1.05143696e-01,  1.51735321e-01, -5.61443012e-05,\n",
      "         1.21709645e-01],\n",
      "       [-1.07101105e-01, -1.60210416e-01, -1.83349296e-01,\n",
      "         1.66371509e-01,  2.20478978e-02, -1.48807481e-01,\n",
      "         1.94178402e-01, -1.43057987e-01, -1.30429611e-01,\n",
      "        -2.75461197e-01],\n",
      "       [ 7.67199844e-02, -2.16430292e-01,  9.34159309e-02,\n",
      "         2.03151572e-02,  1.53463975e-01,  8.38998780e-02,\n",
      "        -1.11006185e-01,  1.29116997e-01,  1.70751840e-01,\n",
      "        -6.24348968e-02],\n",
      "       [-1.92668110e-01, -2.00465903e-01,  1.05758801e-01,\n",
      "         1.93732649e-01,  5.21815419e-02, -2.45960411e-02,\n",
      "         1.05372444e-02,  2.09129930e-01,  1.08507290e-01,\n",
      "         1.28150480e-02],\n",
      "       [ 2.09804744e-01,  2.38724709e-01, -5.96563378e-03,\n",
      "        -1.07718073e-01,  6.51439726e-02, -1.97275028e-01,\n",
      "         1.68647971e-02,  1.32915750e-01,  2.20627502e-01,\n",
      "        -2.03191899e-02],\n",
      "       [ 9.13891345e-02, -1.38757706e-01, -3.06374219e-04,\n",
      "         1.79661781e-01, -2.89951377e-02,  1.40328839e-01,\n",
      "         3.77269387e-02, -6.84345216e-02, -1.26495227e-01,\n",
      "         1.56473398e-01],\n",
      "       [ 1.89324647e-01,  3.52354944e-02,  1.95245638e-01,\n",
      "         2.72717420e-02, -7.17788795e-03,  5.47286719e-02,\n",
      "         2.47528404e-01, -9.08177495e-02,  1.14549110e-02,\n",
      "        -2.21792161e-01],\n",
      "       [ 2.37239316e-01, -7.45192766e-02,  3.74359399e-01,\n",
      "        -6.26610145e-02,  1.43488303e-01, -1.08181842e-01,\n",
      "         3.70382249e-01, -2.37273768e-01, -5.14251404e-02,\n",
      "        -3.24345119e-02],\n",
      "       [ 9.75039974e-02, -1.86647236e-01, -1.44899040e-01,\n",
      "         4.32975553e-02, -2.59573728e-01,  1.10551611e-01,\n",
      "        -2.68702656e-01, -2.15418741e-01,  2.79135704e-02,\n",
      "        -2.04370394e-01],\n",
      "       [ 1.14338711e-01, -1.56472297e-03, -1.78018600e-01,\n",
      "        -1.08217403e-01, -5.12179323e-02,  5.66274151e-02,\n",
      "        -1.78204045e-01, -1.40793651e-01, -1.62877198e-02,\n",
      "        -8.40144523e-04],\n",
      "       [-1.23052374e-01,  2.36721009e-01,  6.26219586e-02,\n",
      "         1.42942341e-02, -1.95033059e-01,  2.68328842e-02,\n",
      "         1.33915126e-01,  2.19704017e-01, -1.39319852e-01,\n",
      "        -2.22479105e-01],\n",
      "       [ 2.02751398e-01, -2.20316276e-01, -2.76751816e-01,\n",
      "        -5.04711159e-02, -1.83175728e-01,  3.35634053e-02,\n",
      "         2.43772015e-01, -7.79681876e-02,  7.73824155e-02,\n",
      "         1.76545933e-01],\n",
      "       [ 2.45139092e-01, -2.38716185e-01,  8.98406506e-02,\n",
      "        -9.52554271e-02,  9.81253833e-02, -1.00884493e-03,\n",
      "         1.97820008e-01,  2.48809457e-01,  9.23972949e-02,\n",
      "         1.30152434e-01],\n",
      "       [ 1.98504403e-01, -2.19910160e-01,  1.12347804e-01,\n",
      "         1.97324708e-01, -3.25638115e-01, -3.12791206e-03,\n",
      "        -2.07789615e-01,  1.21985763e-01,  2.70294473e-02,\n",
      "        -2.04466626e-01],\n",
      "       [-6.09647296e-02,  3.55542451e-02,  1.75807253e-01,\n",
      "        -1.96974888e-01, -1.34823933e-01,  2.99451619e-01,\n",
      "        -2.13893071e-01,  2.17397809e-02,  3.01334679e-01,\n",
      "        -8.57865214e-02],\n",
      "       [ 4.73385677e-02, -1.28043965e-01, -1.35954386e-02,\n",
      "         8.21519271e-02,  1.65865496e-01, -2.63498276e-02,\n",
      "        -1.00760296e-01, -2.90830851e-01,  9.16107148e-02,\n",
      "        -2.28700727e-01],\n",
      "       [ 2.67851472e-01, -1.44256562e-01,  2.76482373e-01,\n",
      "        -2.23013654e-01,  1.28417999e-01,  1.12553045e-01,\n",
      "        -2.54302710e-01,  1.81598634e-01,  3.53938788e-02,\n",
      "         1.64376765e-01],\n",
      "       [-2.08731115e-01, -1.44168317e-01, -1.54475302e-01,\n",
      "        -1.92837536e-01,  2.07788378e-01,  1.69842884e-01,\n",
      "        -2.45130554e-01,  2.96430081e-01, -2.04360541e-02,\n",
      "        -1.62936181e-01],\n",
      "       [ 1.31819993e-01, -1.95597354e-02, -2.58357763e-01,\n",
      "         1.93300292e-01,  1.58085987e-01,  2.22074047e-01,\n",
      "        -5.45982234e-02,  3.17038119e-01, -2.51657665e-01,\n",
      "         5.62302805e-02],\n",
      "       [-4.36162911e-02,  8.57438594e-02,  2.36001730e-01,\n",
      "        -6.17699251e-02,  2.79729486e-01, -1.13547310e-01,\n",
      "        -8.71729925e-02, -6.06607124e-02,  2.58315384e-01,\n",
      "        -7.32429922e-02],\n",
      "       [-2.66375452e-01,  2.40355298e-01, -1.20675936e-01,\n",
      "         3.12180966e-01,  1.65021382e-02,  4.57444303e-02,\n",
      "        -2.91762024e-01, -1.52216271e-01, -2.61409700e-01,\n",
      "         3.82031053e-02],\n",
      "       [-1.09500751e-01,  5.62891029e-02,  9.45459828e-02,\n",
      "        -1.96573913e-01,  1.64688870e-01,  1.92926109e-01,\n",
      "         2.30332404e-01, -8.25675353e-02, -2.68826522e-02,\n",
      "        -9.45858508e-02],\n",
      "       [ 5.86888492e-02, -2.55720466e-01, -1.12713072e-02,\n",
      "         1.71671122e-01, -2.52412289e-01,  1.89249888e-01,\n",
      "        -2.11399212e-01, -1.77242264e-01, -6.14033230e-02,\n",
      "         1.17368832e-01],\n",
      "       [ 1.63454428e-01,  2.82084256e-01, -2.61487156e-01,\n",
      "         7.45039759e-03, -2.17588797e-01,  1.16383322e-01,\n",
      "        -4.24650908e-02,  8.79177675e-02, -2.46207863e-01,\n",
      "        -1.31614387e-01],\n",
      "       [-1.21812031e-01,  6.56388327e-02, -4.87881266e-02,\n",
      "         3.32396571e-03,  1.46947563e-01,  2.65862066e-02,\n",
      "         1.43784493e-01,  2.27704540e-01,  2.07720652e-01,\n",
      "        -1.77241012e-01],\n",
      "       [ 8.95744339e-02,  4.46937140e-03, -1.16340362e-01,\n",
      "         9.25697088e-02,  2.82237288e-02, -8.79834741e-02,\n",
      "        -2.12182343e-01,  1.87212497e-01, -1.33544877e-01,\n",
      "        -1.36736855e-01],\n",
      "       [-3.74416858e-02, -2.03937426e-01,  1.72629431e-01,\n",
      "         2.26446077e-01, -1.50362879e-01,  1.20543897e-01,\n",
      "         5.15583865e-02,  1.42125800e-01,  2.27700859e-01,\n",
      "        -1.47319913e-01],\n",
      "       [-1.75637960e-01, -1.02781944e-01,  1.97756574e-01,\n",
      "         1.39169499e-01, -5.62139414e-02, -1.45848945e-01,\n",
      "        -2.68396974e-01,  4.54936512e-02,  2.53162831e-01,\n",
      "         1.85807988e-01],\n",
      "       [ 7.38935024e-02, -1.61232337e-01, -1.68951184e-01,\n",
      "        -1.84304163e-01,  8.57793202e-04,  2.11189687e-01,\n",
      "        -5.22984006e-03,  1.28623530e-01,  1.04840353e-01,\n",
      "         5.67917414e-02],\n",
      "       [ 6.12957329e-02, -5.43724298e-02, -1.30004257e-01,\n",
      "         3.01192841e-03, -2.21973911e-01,  2.43191078e-01,\n",
      "         8.19210932e-02, -1.44604202e-02,  2.61509061e-01,\n",
      "        -2.35271424e-01],\n",
      "       [ 1.43764660e-01, -6.61937613e-03, -9.15698931e-02,\n",
      "        -1.72570735e-01, -2.55378515e-01, -1.18255243e-01,\n",
      "         1.99569643e-01, -2.73474306e-01,  2.06662819e-01,\n",
      "         1.40467525e-01],\n",
      "       [-1.01052605e-01, -1.31400973e-01, -9.65149775e-02,\n",
      "         5.15378416e-02,  1.43143967e-01, -1.10763326e-01,\n",
      "        -2.06286252e-01,  3.25679988e-01,  7.67542273e-02,\n",
      "        -8.39539319e-02],\n",
      "       [-1.39953181e-01,  1.76379398e-01,  2.29020298e-01,\n",
      "         1.82913348e-01, -1.13037467e-01, -9.31174904e-02,\n",
      "         1.92546919e-01, -1.39372572e-01, -1.61581218e-01,\n",
      "        -2.10992560e-01],\n",
      "       [-2.66254634e-01, -1.18536882e-01,  1.40220925e-01,\n",
      "        -1.49569944e-01,  3.51887316e-01,  2.04025596e-01,\n",
      "         1.52243793e-01, -3.22656304e-01,  3.05523157e-01,\n",
      "         1.12848014e-01],\n",
      "       [-3.44697759e-02,  1.95986480e-01,  1.47765432e-03,\n",
      "         8.88370723e-02, -7.38987625e-02, -9.97349620e-02,\n",
      "        -2.98971236e-02, -7.21086934e-02, -3.69134359e-02,\n",
      "        -8.19311440e-02],\n",
      "       [ 2.12652341e-01,  1.61931798e-01,  1.79864019e-01,\n",
      "        -2.34338641e-01, -9.41851959e-02, -1.28200129e-02,\n",
      "        -4.68103215e-02,  2.00280815e-01, -1.73809648e-01,\n",
      "         1.28966168e-01],\n",
      "       [-2.87943333e-02, -1.67441130e-01,  9.39755365e-02,\n",
      "        -2.19125435e-01,  6.55851588e-02,  4.83532511e-02,\n",
      "        -7.47082829e-02,  1.68540198e-02, -1.79933645e-02,\n",
      "        -3.63317989e-02],\n",
      "       [-2.66792685e-01, -1.22540794e-01,  6.26723543e-02,\n",
      "         5.26470207e-02, -1.56681150e-01,  2.68325150e-01,\n",
      "         1.10590845e-01, -1.33988589e-01,  1.48822367e-01,\n",
      "         1.79422632e-01],\n",
      "       [ 1.35471284e-01,  1.40213877e-01,  2.24909961e-01,\n",
      "        -1.89764798e-01, -2.03142360e-01, -1.03671208e-01,\n",
      "        -5.86377196e-02, -8.17071423e-02, -6.52439222e-02,\n",
      "         4.95434292e-02],\n",
      "       [-8.85227323e-02,  1.54141843e-01, -1.11037105e-01,\n",
      "        -2.09179848e-01,  1.41343668e-01, -3.34663749e-01,\n",
      "        -2.25533739e-01,  2.11648196e-01,  1.20123714e-01,\n",
      "         1.65859774e-01],\n",
      "       [ 7.96919316e-02, -9.90363359e-02, -2.57080108e-01,\n",
      "        -7.70088881e-02, -1.74735531e-01, -1.78393275e-01,\n",
      "        -9.66284350e-02,  7.05538597e-03, -1.50582552e-01,\n",
      "         5.84573150e-02],\n",
      "       [-3.18458676e-01,  2.97058374e-01,  2.47355834e-01,\n",
      "        -2.43234243e-02,  3.02224010e-01, -1.31558388e-01,\n",
      "        -1.42867967e-01, -1.21880114e-01, -2.63777226e-01,\n",
      "         1.21151701e-01],\n",
      "       [ 3.18722457e-01, -2.61786401e-01,  3.95326018e-02,\n",
      "         2.00459331e-01, -1.80300504e-01,  1.40324131e-01,\n",
      "        -2.23639905e-01, -2.04128562e-04,  1.67178154e-01,\n",
      "        -2.37790540e-01],\n",
      "       [-2.80935373e-02, -2.03875795e-01,  2.39591271e-01,\n",
      "        -1.88168287e-01, -9.70027149e-02, -3.53587538e-01,\n",
      "         2.21147135e-01,  8.79264995e-02,  2.72732675e-02,\n",
      "         1.16743959e-01],\n",
      "       [ 1.35392815e-01,  2.44935110e-01,  1.44602522e-01,\n",
      "         8.82768482e-02, -8.83584097e-02,  2.40017340e-01,\n",
      "         5.30448258e-02,  2.75118593e-02,  2.10989609e-01,\n",
      "        -1.08323820e-01],\n",
      "       [ 1.26486495e-01,  1.84316143e-01, -1.12871654e-01,\n",
      "        -9.15686507e-03, -3.01354937e-02,  2.32343018e-01,\n",
      "         1.31828889e-01,  1.21520810e-01, -2.28634879e-01,\n",
      "        -4.31163833e-02],\n",
      "       [-2.01884106e-01,  1.33221615e-02,  1.94421366e-01,\n",
      "        -1.16928808e-01, -4.45585288e-02, -1.64112940e-01,\n",
      "         7.47812316e-02, -2.22731709e-01,  1.84903711e-01,\n",
      "         2.08190277e-01],\n",
      "       [ 1.77579224e-01,  2.14561567e-01, -7.46289939e-02,\n",
      "        -1.17582314e-01,  2.17302993e-01,  2.61532247e-01,\n",
      "        -1.88455880e-01,  9.08397734e-02,  1.05769284e-01,\n",
      "         3.43826227e-02],\n",
      "       [-2.57631779e-01, -1.72188818e-01,  1.63039789e-01,\n",
      "         1.57156035e-01, -1.59987703e-01, -1.65950730e-01,\n",
      "         1.15788147e-01, -1.22112006e-01,  2.11175427e-01,\n",
      "        -8.21403135e-03],\n",
      "       [-1.82464033e-01, -1.43476546e-01, -2.82597303e-01,\n",
      "        -3.20699990e-01,  3.46514761e-01,  1.27279246e-02,\n",
      "         1.81952760e-01, -6.04744405e-02,  9.69912857e-02,\n",
      "         1.86044872e-01],\n",
      "       [ 4.26582294e-03,  3.31913888e-01, -5.90429991e-04,\n",
      "        -8.11413024e-03, -2.18746796e-01, -9.92796719e-02,\n",
      "         1.52496705e-02,  8.77880603e-02,  3.33787836e-02,\n",
      "         2.62313902e-01],\n",
      "       [ 8.55818540e-02, -5.58931455e-02,  2.28604183e-01,\n",
      "         2.44235054e-01, -1.12164751e-01, -1.47720277e-01,\n",
      "        -1.56136572e-01, -7.97729045e-02,  1.40370846e-01,\n",
      "        -3.24505344e-02],\n",
      "       [ 9.08983231e-04,  1.67577192e-02, -1.57515481e-01,\n",
      "        -2.20525771e-01, -9.99635309e-02, -1.34491831e-01,\n",
      "        -1.51398465e-01,  1.71277255e-01,  3.26736383e-02,\n",
      "        -4.78637852e-02],\n",
      "       [ 2.11529806e-01,  1.92294255e-01,  1.49695590e-01,\n",
      "        -2.30790660e-01, -1.86773673e-01,  6.70600832e-02,\n",
      "        -1.15921110e-01,  2.98276931e-01, -1.81756049e-01,\n",
      "        -7.57510960e-02],\n",
      "       [ 2.75542364e-02, -1.18114084e-01, -2.58530229e-01,\n",
      "         2.61453897e-01, -1.77427560e-01,  3.11445355e-01,\n",
      "         1.34619653e-01,  1.45197392e-01, -7.93075040e-02,\n",
      "         1.26768619e-01],\n",
      "       [-4.91742045e-02, -2.32229009e-01, -6.11851166e-04,\n",
      "         1.24414966e-01,  6.60044104e-02, -5.44941090e-02,\n",
      "         1.19655505e-01, -1.01675794e-01,  7.35750720e-02,\n",
      "         8.46771672e-02],\n",
      "       [ 1.24422550e-01,  1.53715201e-02, -1.69360563e-01,\n",
      "        -4.60969247e-02,  2.41600707e-01, -3.51921022e-02,\n",
      "         1.57391801e-01,  1.32613648e-02,  7.94766992e-02,\n",
      "        -2.40627125e-01],\n",
      "       [-2.48537734e-01,  2.20265329e-01,  1.35953515e-03,\n",
      "         9.76667553e-02,  1.06714293e-01,  1.34417683e-01,\n",
      "         2.11449251e-01,  1.33191064e-01,  1.16011679e-01,\n",
      "        -1.36126652e-02],\n",
      "       [ 3.79994698e-02, -1.39135674e-01, -1.04411067e-02,\n",
      "         3.90675217e-02,  1.21488482e-01, -1.77622154e-01,\n",
      "        -2.11131617e-01,  7.85270985e-03,  1.58377036e-01,\n",
      "         2.45817289e-01],\n",
      "       [ 1.07210386e-03, -2.01387301e-01, -1.23059340e-02,\n",
      "         1.93925336e-01,  1.72677487e-01, -2.26256642e-02,\n",
      "        -5.98440915e-02, -2.60283798e-01, -2.05738232e-01,\n",
      "        -1.98278680e-01],\n",
      "       [-2.58315504e-02, -1.97185650e-01,  1.29255995e-01,\n",
      "        -6.60816431e-02, -2.20924318e-01, -1.71844006e-01,\n",
      "         1.19729184e-01, -1.58967208e-02,  8.82047489e-02,\n",
      "         2.03812152e-01],\n",
      "       [ 8.88366774e-02, -2.10170954e-01,  2.53183041e-02,\n",
      "        -4.64541763e-02, -1.44556969e-01, -2.10450456e-01,\n",
      "        -5.31321093e-02, -1.27152056e-01, -2.04882354e-01,\n",
      "        -1.79118469e-01],\n",
      "       [-4.59649507e-03, -1.96527258e-01, -3.08714528e-02,\n",
      "         1.31583944e-01,  1.62633527e-02, -1.34133041e-01,\n",
      "         7.87312686e-02,  2.26598382e-01, -2.10411832e-01,\n",
      "         1.82199627e-01],\n",
      "       [-1.04275919e-01,  4.94285263e-02, -3.18933353e-02,\n",
      "         1.31353781e-01, -6.45318925e-02,  1.58284977e-01,\n",
      "         4.46870811e-02,  1.15454420e-01,  1.55416816e-01,\n",
      "        -9.43682864e-02],\n",
      "       [ 9.92532894e-02, -1.48487642e-01, -8.22615325e-02,\n",
      "         1.17605574e-01,  7.62705952e-02,  1.94701597e-01,\n",
      "        -1.14557976e-02,  1.20351069e-01,  2.01416612e-01,\n",
      "         9.42556262e-02],\n",
      "       [-1.13591880e-01, -1.32377028e-01, -6.39630556e-02,\n",
      "         2.07425840e-02,  3.14621389e-01, -4.70844880e-02,\n",
      "         9.87040475e-02,  1.86149880e-01, -4.67929021e-02,\n",
      "         3.27083349e-01],\n",
      "       [ 2.09911823e-01,  5.03768884e-02,  2.36347094e-01,\n",
      "        -1.90782119e-02,  5.62427603e-02,  2.71241218e-01,\n",
      "         2.74053544e-01, -1.20453894e-01, -2.20219076e-01,\n",
      "        -9.34471264e-02],\n",
      "       [ 4.68778163e-02, -1.57950908e-01,  6.73396746e-03,\n",
      "        -2.11466536e-01,  7.85938874e-02,  2.30574131e-01,\n",
      "         2.52388358e-01,  7.25574838e-03, -1.69176996e-01,\n",
      "         2.90472090e-01],\n",
      "       [-3.93950269e-02, -1.33224368e-01, -1.82047710e-01,\n",
      "         2.30099796e-03, -1.15199439e-01,  1.49125874e-01,\n",
      "        -5.50769083e-02, -2.93893307e-01,  1.17076360e-01,\n",
      "        -7.64522031e-02],\n",
      "       [ 3.84922791e-03, -2.99836770e-02, -1.44590005e-01,\n",
      "         3.06126196e-02,  1.38091100e-02,  1.47044793e-01,\n",
      "         1.10123679e-01,  4.71041799e-02,  1.99310690e-01,\n",
      "        -7.93453753e-02],\n",
      "       [ 9.34380479e-03,  1.61186308e-01, -2.59415865e-01,\n",
      "         1.72833145e-01,  2.55672820e-02, -1.67286128e-01,\n",
      "        -1.95206895e-01, -2.30640233e-01,  1.61566585e-01,\n",
      "        -1.54532850e-01],\n",
      "       [ 3.39934640e-02,  4.88832314e-03, -4.60697934e-02,\n",
      "         4.10672612e-02, -1.38743579e-01,  6.43764883e-02,\n",
      "         1.34863183e-01, -6.84131449e-03,  9.01093408e-02,\n",
      "        -1.62576616e-01],\n",
      "       [ 1.38927028e-01, -1.49951041e-01, -2.03274950e-01,\n",
      "        -1.85791656e-01, -1.08738005e-01, -1.49244606e-01,\n",
      "        -7.73750618e-02, -2.22596020e-01, -5.75074442e-02,\n",
      "        -9.16892067e-02],\n",
      "       [-1.94686741e-01,  2.98346341e-01, -2.70362288e-01,\n",
      "        -1.95924178e-01,  7.89064988e-02,  1.71921730e-01,\n",
      "         1.89078927e-01, -2.33261690e-01,  1.54697365e-04,\n",
      "        -4.63216417e-02],\n",
      "       [-4.18537296e-02, -2.16404185e-01, -7.76557624e-02,\n",
      "        -1.88570879e-02,  3.40921611e-01,  1.95712551e-01,\n",
      "         1.75569057e-01,  7.42748678e-02, -9.68364030e-02,\n",
      "        -1.64592147e-01],\n",
      "       [ 9.84090939e-02, -1.76455244e-01, -1.27016589e-01,\n",
      "         2.12224782e-01, -4.73906808e-02,  1.49212375e-01,\n",
      "         2.23344713e-01,  2.03760773e-01, -5.07795587e-02,\n",
      "        -1.27040483e-02],\n",
      "       [-1.46595180e-01, -6.13601394e-02, -7.19334707e-02,\n",
      "        -7.53829405e-02, -5.04214130e-02,  1.96904153e-01,\n",
      "         2.27169916e-01, -5.03367186e-02,  3.84962894e-02,\n",
      "         1.26104787e-01],\n",
      "       [-1.24644645e-01,  3.99048328e-01,  1.40924290e-01,\n",
      "         1.54453814e-01, -9.68343914e-02, -2.18519077e-01,\n",
      "        -2.49338269e-01, -3.21936160e-02,  2.90994048e-01,\n",
      "        -2.92279482e-01],\n",
      "       [-2.24137589e-01, -1.67687282e-01,  1.03981607e-02,\n",
      "         1.37752727e-01,  3.02955434e-02,  1.60478830e-01,\n",
      "        -1.75154418e-01,  7.72904903e-02,  1.48157895e-01,\n",
      "        -1.98661536e-01],\n",
      "       [ 2.10205242e-01,  7.20511097e-03,  2.07479641e-01,\n",
      "        -1.54318795e-01, -1.23156182e-01, -2.33383235e-02,\n",
      "         1.01458170e-01,  1.61557585e-01,  3.78368124e-02,\n",
      "        -1.26200885e-01],\n",
      "       [-9.88954678e-02,  7.22201541e-02,  5.28790019e-02,\n",
      "         8.05175230e-02,  7.48939812e-02, -6.54138550e-02,\n",
      "         2.53855914e-01, -7.92056695e-02, -1.78976685e-01,\n",
      "        -2.64647096e-01],\n",
      "       [ 1.45056009e-01, -1.93095133e-01, -8.48610327e-02,\n",
      "        -2.46116459e-01, -1.18952453e-01,  2.24033222e-01,\n",
      "         2.52153993e-01, -2.44477302e-01, -2.44017780e-01,\n",
      "         1.17927507e-01],\n",
      "       [-2.21986428e-01, -2.04052061e-01, -1.01247597e-02,\n",
      "         2.00418144e-01, -5.81751876e-02, -9.28711668e-02,\n",
      "         4.45481241e-02, -2.84233522e-02,  2.34726816e-02,\n",
      "         1.89822331e-01],\n",
      "       [-4.05365899e-02,  2.27517679e-01,  2.53538162e-01,\n",
      "         1.72087729e-01,  1.96403950e-01,  8.17304254e-02,\n",
      "         6.81585670e-02, -1.30900340e-02, -2.93203518e-02,\n",
      "        -2.47408360e-01],\n",
      "       [ 1.66021422e-01, -1.25664189e-01, -8.12041909e-02,\n",
      "        -1.26534238e-01,  1.20645098e-01, -2.36329496e-01,\n",
      "        -2.63640761e-01, -2.72617415e-02,  9.20864344e-02,\n",
      "         1.91332936e-01],\n",
      "       [ 1.85165584e-01,  5.90836592e-02, -1.68279380e-01,\n",
      "        -2.15762481e-01, -3.54988165e-02, -1.89122170e-01,\n",
      "        -2.13294029e-01,  2.99409777e-01,  2.80293226e-02,\n",
      "         2.14583382e-01],\n",
      "       [ 4.03851904e-02,  1.83024257e-02,  6.56611472e-02,\n",
      "         6.22469932e-02,  2.28057772e-01, -9.69759747e-02,\n",
      "        -1.34886056e-01, -2.30514675e-01, -8.91361311e-02,\n",
      "        -1.82934150e-01],\n",
      "       [-1.46452993e-01,  1.72506854e-01,  2.38088176e-01,\n",
      "        -2.14282304e-01, -1.41903982e-01, -1.29506111e-01,\n",
      "         1.46373913e-01, -1.83446184e-01, -1.56288043e-01,\n",
      "        -1.93948261e-02],\n",
      "       [ 2.45289639e-01, -1.43399164e-02, -2.49475330e-01,\n",
      "        -8.96571483e-03, -1.90605491e-01,  9.10758376e-02,\n",
      "         2.69048393e-01, -1.88479438e-01,  1.79534778e-01,\n",
      "        -1.80097148e-01],\n",
      "       [ 2.29332969e-02,  2.02975243e-01, -1.38494805e-01,\n",
      "        -2.21890584e-01, -7.01581910e-02, -1.77609906e-01,\n",
      "        -9.51646641e-02, -1.92811400e-01, -5.91573082e-02,\n",
      "         4.45807315e-02],\n",
      "       [ 1.34097815e-01,  1.72311254e-02, -4.92008030e-02,\n",
      "         1.02496184e-01, -2.31651887e-01,  6.41534626e-02,\n",
      "        -1.44245103e-01, -1.57933772e-01,  1.06137745e-01,\n",
      "         2.00966924e-01],\n",
      "       [-1.85390458e-01,  8.13036188e-02,  2.16408625e-01,\n",
      "         9.29688886e-02, -1.06938995e-01, -1.93358079e-01,\n",
      "         5.85768325e-03, -7.42103532e-02, -1.21414103e-01,\n",
      "        -7.45604634e-02],\n",
      "       [-7.97588378e-02,  1.20269634e-01,  8.55267346e-02,\n",
      "         2.03221828e-01, -6.67025074e-02, -2.39344224e-01,\n",
      "        -2.25356147e-01,  1.58915922e-01, -1.83863908e-01,\n",
      "         2.67260581e-01],\n",
      "       [ 1.35418311e-01, -2.39905883e-02, -1.00147821e-01,\n",
      "         1.95070922e-01, -2.58512683e-02,  1.67630017e-01,\n",
      "        -1.12581283e-01, -9.65952575e-02, -5.19547202e-02,\n",
      "         2.31072232e-01],\n",
      "       [ 2.76423901e-01, -6.90765306e-02,  2.91509572e-02,\n",
      "        -2.49566853e-01,  1.11429043e-01, -2.31540594e-02,\n",
      "        -8.98401216e-02,  2.05536023e-01, -2.02880457e-01,\n",
      "        -2.28604868e-01],\n",
      "       [ 9.68374014e-02,  1.85614228e-01,  3.90891619e-02,\n",
      "        -2.27445737e-01,  6.70488328e-02, -5.00340387e-02,\n",
      "         9.57168415e-02,  1.73329830e-01,  3.63408774e-02,\n",
      "        -1.70337155e-01],\n",
      "       [-1.37770146e-01,  1.75398886e-01,  1.18674010e-01,\n",
      "         1.35113716e-01, -2.16370121e-01,  6.50618970e-02,\n",
      "         6.82770312e-02, -8.62851664e-02,  1.39933720e-01,\n",
      "         5.45063056e-02]], dtype=float32)>, <tf.Variable 'prune_low_magnitude_dense_434/bias:0' shape=(10,) dtype=float32, numpy=\n",
      "array([-0.0599062 ,  0.12720644, -0.01548629, -0.04824696,  0.00399149,\n",
      "        0.03390283, -0.01301104,  0.05552029, -0.092008  ,  0.00803742],\n",
      "      dtype=float32)>]\n",
      "Trained Test: Loss: 2.3580663204193115, Accuracy: 0.10513333231210709\n"
     ]
    }
   ],
   "source": [
    "def make_args() -> tuple:\n",
    "    args: tuple = (0, X_train[0].shape, 10, pruning_params_unpruned)\n",
    "    model = sparsity.strip_pruning(pruned_nn(*args))\n",
    "    mask_model = sparsity.strip_pruning(create_masked_nn(*args))\n",
    "    return model, mask_model, X_train, Y_train\n",
    "\n",
    "# args: tuple = make_args()\n",
    "# test_loss, test_accuracy = test_step(model, X_train, Y_train)\n",
    "# print(f'Untrained Test: Loss: {test_loss.numpy()}, Accuracy: {test_accuracy.numpy()}')\n",
    "\n",
    "# for i in range(100):\n",
    "#     training_loss, training_accuracy = train_one_step(*args)\n",
    "#     print(f'Training iteration {i + 1}: Loss: {training_loss.numpy()}, Accuracy: {training_accuracy.numpy()}')\n",
    "print(args[0].trainable_weights)\n",
    "test_loss, test_accuracy = test_step(model, X_train, Y_train)\n",
    "print(f'Trained Test: Loss: {test_loss.numpy()}, Accuracy: {test_accuracy.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'src.harness.constants' has no attribute 'TRAINING_EPOCHS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[391], line 98\u001b[0m\n\u001b[1;32m     88\u001b[0m     round_data: TrainingRound \u001b[38;5;241m=\u001b[39m TrainingRound(pruning_step, initial_weights, final_weights, masks, train_losses, train_accuracies, test_losses, test_accuracies)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_stripped, round_data\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m     94\u001b[0m     pruning_step: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     95\u001b[0m     model: keras\u001b[38;5;241m.\u001b[39mModel, \n\u001b[1;32m     96\u001b[0m     mask_model: keras\u001b[38;5;241m.\u001b[39mModel,\n\u001b[1;32m     97\u001b[0m     make_dataset: \u001b[38;5;28mcallable\u001b[39m, \n\u001b[0;32m---> 98\u001b[0m     num_epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mTRAINING_EPOCHS,\n\u001b[1;32m     99\u001b[0m     patience: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mPATIENCE,\n\u001b[1;32m    100\u001b[0m     minimum_delta: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mMINIMUM_DELTA,\n\u001b[1;32m    101\u001b[0m     optimizer: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mOptimizer \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mOPTIMIZER(), \n\u001b[1;32m    102\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[keras\u001b[38;5;241m.\u001b[39mModel, keras\u001b[38;5;241m.\u001b[39mModel, TrainingRound]:\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Function to perform training for a model.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    :returns: Model, masked model, and training round objects with the final trained model and the training summary/.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pruning_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'src.harness.constants' has no attribute 'TRAINING_EPOCHS'"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "\n",
    "def save_model(model: keras.Model, seed: int, pruning_step: int, untrained: bool = False,):\n",
    "    \"\"\"\n",
    "    Function to save a single trained model.\n",
    "\n",
    "    :param model:        Model object being saved.\n",
    "    :param seed:         Random seed used in the model\n",
    "    :param pruning_step: Integer value for the number of pruning steps which had been completed for the model.\n",
    "    :param untrained:    Boolean for if it is the untrained version of a model. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    output_directory: str = paths.get_model_directory(seed, C.MODEL_DIRECTORY)\n",
    "    # Save the initial weights in an 'initial' directory in the top-level of the model directory\n",
    "    if untrained:\n",
    "        untrained_directory: str = paths.initial(output_directory)\n",
    "        model.save_weights(paths.weights(untrained_directory))\n",
    "    else:\n",
    "        # Create a trial directory within the model directory\n",
    "        trial_directory: str = paths.trial(output_directory, pruning_step)\n",
    "        model.save_weights(paths.weights(trial_directory))\n",
    "\n",
    "def training_loop(\n",
    "        pruning_step: int,\n",
    "        model_stripped: keras.Model, \n",
    "        mask_model_stripped: keras.Model,\n",
    "        make_dataset: callable,\n",
    "        num_epochs: int, \n",
    "        patience: int,\n",
    "        minimum_delta: float,\n",
    "        optimizer: keras.optimizers.Optimizer = C.OPTIMIZER(),\n",
    "    ) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Main training loop for the model.\n",
    "    \"\"\"\n",
    "    # Number of epochs without improvement\n",
    "    local_patience: int = 0\n",
    "    best_test_loss: float = float('inf')\n",
    "\n",
    "    # Extract input and target\n",
    "    X_train, Y_train, X_test, Y_test = make_dataset()\n",
    "\n",
    "    initial_weights: list[np.ndarray] = model_stripped.trainable_weights\n",
    "    masks: list[np.ndarray] = mask_model_stripped.trainable_weights\n",
    "    train_losses: np.array = np.zeros(Y_train.shape[0])\n",
    "    train_accuracies: np.array = np.zeros(Y_train.shape[0])\n",
    "    test_losses: np.array = np.zeros(Y_test.shape[0])\n",
    "    test_accuracies: np.array = np.zeros(Y_test.shape[0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Exit early if there are `patience` epochs without improvement\n",
    "        if local_patience >= patience:\n",
    "            if DEBUG:\n",
    "                print(f'Early stopping initiated')\n",
    "            break\n",
    "        \n",
    "        # Update model parameters for each point in the training set\n",
    "        for x, y in X_train, Y_train:\n",
    "            train_loss, train_accuracy = train_one_step(model_stripped, mask_model_stripped, optimizer, x, y)\n",
    "            train_losses[idx] = train_loss\n",
    "            train_accuracies[idx] = train_accuracy\n",
    "\n",
    "        # Evaluate model on each point in the test set\n",
    "        for idx, (x_t, y_t) in enumerate(zip(X_test, Y_test)):\n",
    "            test_loss, test_accuracy = test_step(model_stripped, optimizer, x_t, y_t)\n",
    "            test_losses[idx] = test_loss\n",
    "            test_accuracies[idx] = test_accuracy\n",
    "\n",
    "        # Display output\n",
    "        if DEBUG:\n",
    "            print(f'Epoch {epoch + 1}, Train/Test Loss: {train_loss:.4f}/{test_loss:.4f}, Train/Test Accuracy: {train_accuracy:.4f}/{test_accuracy:.4f}')\n",
    "            print(f'Total number of trainable parameters = {np.sum(count_nonzero_parameters(model_stripped))}')\n",
    "\n",
    "        # Check for early stopping criteria\n",
    "        mean_test_loss: float = np.mean(test_losses)\n",
    "        if mean_test_loss < best_test_loss and (best_test_loss - mean_test_loss) >= minimum_delta:\n",
    "            # update 'best_test_loss' variable to lowest loss encountered so far\n",
    "            best_test_loss = mean_test_loss\n",
    "            # Reset the counter\n",
    "            local_patience = 0\n",
    "        else:  # there is no improvement in monitored metric 'val_loss'\n",
    "            local_patience += 1  # number of epochs without any improvement\n",
    "\n",
    "    final_weights: list[np.ndarray] = model_stripped.trainable_weights\n",
    "\n",
    "    # Compile training round data\n",
    "    round_data: TrainingRound = TrainingRound(pruning_step, initial_weights, final_weights, masks, train_losses, train_accuracies, test_losses, test_accuracies)\n",
    "\n",
    "    return model_stripped, round_data\n",
    "    \n",
    "\n",
    "def train(\n",
    "    pruning_step: int,\n",
    "    model: keras.Model, \n",
    "    mask_model: keras.Model,\n",
    "    make_dataset: callable, \n",
    "    num_epochs: int = C.TRAINING_EPOCHS,\n",
    "    patience: int = C.PATIENCE,\n",
    "    minimum_delta: float = C.MINIMUM_DELTA,\n",
    "    optimizer: tf.keras.optimizers.Optimizer = C.OPTIMIZER(), \n",
    "    ) -> tuple[keras.Model, keras.Model, TrainingRound]:\n",
    "    \"\"\"\n",
    "    Function to perform training for a model.\n",
    "\n",
    "    :param pruning_step:     Integer value for the step in pruning. Defaults to 0.\n",
    "    :param model:            Model to optimize.\n",
    "    :param mask_model:       Model whose weights correspond to masks being applied.\n",
    "\n",
    "    :param make_dataset:     Function to produce the training/test sets.\n",
    "    :param num_epochs:       Number of epochs to train for.\n",
    "    :param patience:         Number of epochs which can be ran without improvement before calling early stopping.\n",
    "    :param minimum_delta:    Minimum increase to be considered an improvement.s\n",
    "    :param optimizer:        Optimizer to use during training.\n",
    "\n",
    "    :returns: Model, masked model, and training round objects with the final trained model and the training summary/.\n",
    "    \"\"\"\n",
    "\n",
    "    if pruning_step == 0:\n",
    "        save_model(model, pruning_step, untrained=True)\n",
    "\n",
    "    # Run the training loop\n",
    "    model, training_round = training_loop(\n",
    "        pruning_step, \n",
    "        sparsity.strip_pruning(model), \n",
    "        sparsity.strip_pruning(mask_model), \n",
    "        make_dataset, \n",
    "        num_epochs, \n",
    "        patience, \n",
    "        minimum_delta, \n",
    "        optimizer\n",
    "    )\n",
    "\n",
    "    # Save network final weights and masks to its folder in the appropriate trial folder\n",
    "    save_model(model, pruning_step)\n",
    "\n",
    "    return model, mask_model, training_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/\n",
    "\n",
    "\n",
    "def experiment(make_dataset: callable, \n",
    "               make_model: callable, \n",
    "               train_model: callable, \n",
    "               prune_masks: callable, \n",
    "               optimizer: keras.optimizers.Optimizer,\n",
    "               pruning_steps: int,\n",
    "               num_epochs: int,\n",
    "               patience: float,\n",
    "               minimum_delta: float,\n",
    " ) -> ExperimentData:\n",
    "    summary: ExperimentData = ExperimentData()\n",
    "\n",
    "    for i in range(1, pruning_steps + 1):\n",
    "        \n",
    "        print(\"\\n\\n\\nIterative pruning round: {0}\\n\\n\".format(i))\n",
    "        \n",
    "        # Define 'train_one_step()' and 'test_step()' functions here-\n",
    "\n",
    "        # Instantiate a model\n",
    "        model_gt = pruned_nn(pruning_params_unpruned)\n",
    "        \n",
    "        # Load winning ticket (from above)-\n",
    "        # model_gt.load_weights(\"LeNet_MNIST_Winning_Ticket.h5\")\n",
    "        model_gt.set_weights(winning_ticket_model.get_weights())\n",
    "        \n",
    "        # Strip model of pruning parameters-\n",
    "        model_gt_stripped = sparsity.strip_pruning(model_gt)\n",
    "        \n",
    "        \n",
    "        # Train model using 'GradientTape'-\n",
    "        \n",
    "        # Initialize parameters for Early Stopping manual implementation-\n",
    "        best_val_loss = 100\n",
    "        loc_patience = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "        \n",
    "            if loc_patience >= patience:\n",
    "                print(\"\\n'EarlyStopping' called!\\n\")\n",
    "                break\n",
    "            \n",
    "            # Reset the metrics at the start of the next epoch\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            test_loss.reset_states()\n",
    "            test_accuracy.reset_states()\n",
    "            \n",
    "            \n",
    "            for x, y in train_dataset:\n",
    "                # train_one_step(model_gt_stripped, mask_model, optimizer, x, y, grad_mask_mul)\n",
    "                train_one_step(model_gt_stripped, mask_model_stripped, optimizer, x, y)\n",
    "\n",
    "\n",
    "            for x_t, y_t in test_dataset:\n",
    "                # test_step(x_t, y_t)\n",
    "                test_step(model_gt_stripped, optimizer, x_t, y_t)\n",
    "        \n",
    "            # 'i' is the index for number of pruning rounds-\n",
    "            round: TrainingRound = TrainingRound(i, {}, {}, {}, {}, -1, -1, \n",
    "                                                train_accuracy.result(), \n",
    "                                                train_loss.result(), \n",
    "                                                test_accuracy.result(), \n",
    "                                                test_loss.result())\n",
    "            summary.add_pruning_round(round)\n",
    "        \n",
    "            # Count number of non-zero parameters in each layer and in total-\n",
    "            # print(\"layer-wise manner model, number of nonzero parameters in each layer are: \\n\")\n",
    "\n",
    "            model_sum_params = 0\n",
    "        \n",
    "            for layer in model_gt_stripped.trainable_weights:\n",
    "                # print(tf.math.count_nonzero(layer, axis = None).numpy())\n",
    "                model_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()\n",
    "        \n",
    "            print(\"Total number of trainable parameters = {0}\\n\".format(model_sum_params))\n",
    "\n",
    "            # Code for manual Early Stopping:\n",
    "            if np.abs(test_loss.result() < best_val_loss) >= minimum_delta:\n",
    "                # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "                best_val_loss = test_loss.result()\n",
    "            \n",
    "                # reset 'loc_patience' variable-\n",
    "                loc_patience = 0\n",
    "            \n",
    "            else:  # there is no improvement in monitored metric 'val_loss'\n",
    "                loc_patience += 1  # number of epochs without any improvement\n",
    "\n",
    "        \n",
    "        # Save trained model weights-\n",
    "        model_gt.save_weights(\"LeNet_MNIST_Trained_Weights.h5\", overwrite=True)\n",
    "\n",
    "        # Prune trained model:\n",
    "        \n",
    "        # Specify the parameters to be used for layer-wise pruning, Fully-Connected layer pruning-\n",
    "        pruning_params_fc = {\n",
    "            'pruning_schedule': sparsity.ConstantSparsity(\n",
    "                target_sparsity=dense1_pruning[i - 1], begin_step = 1000,\n",
    "                end_step = end_step, frequency=100\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # Instantiate a Nueal Network model to be pruned using parameters from above-\n",
    "        pruned_model = pruned_nn(pruning_params_fc)\n",
    "        \n",
    "        # Load weights from original trained and unpruned model-\n",
    "        # pruned_model.load_weights(\"LeNet_MNIST_Trained_Weights.h5\")\n",
    "        pruned_model.set_weights(model_gt.get_weights())\n",
    "        \n",
    "        # Train pruned NN-\n",
    "        history_pruned = pruned_model.fit(\n",
    "            x = X_train, y = y_train,\n",
    "            batch_size = batch_size,\n",
    "            epochs = epochs,\n",
    "            verbose = 1,\n",
    "            callbacks = callback,\n",
    "            validation_data = (X_test, y_test),\n",
    "            shuffle = True\n",
    "        )\n",
    "        \n",
    "        # Strip the pruning wrappers from pruned model-\n",
    "        pruned_model_stripped = sparsity.strip_pruning(pruned_model)\n",
    "        \n",
    "        # print(\"\\nIn pruned model, number of nonzero parameters in each layer are: \\n\")\n",
    "        pruned_sum_params = 0\n",
    "        \n",
    "        for layer in pruned_model_stripped.trainable_weights:\n",
    "            # print(tf.math.count_nonzero(layer, axis = None).numpy())\n",
    "            pruned_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()\n",
    "        \n",
    "\n",
    "        # Create a mask:\n",
    "        \n",
    "        # Instantiate a new neural network model for which, the mask is to be created,\n",
    "        mask_model = pruned_nn(pruning_params_unpruned)\n",
    "        \n",
    "        # Load weights of PRUNED model-\n",
    "        # mask_model.load_weights(\"LeNet_MNIST_Pruned_Weights.h5\")\n",
    "        mask_model.set_weights(pruned_model.get_weights())\n",
    "        \n",
    "        # Strip the model of its pruning parameters-\n",
    "        mask_model_stripped = sparsity.strip_pruning(mask_model)\n",
    "        \n",
    "        # For each layer, for each weight which is 0, leave it, as is.\n",
    "        # And for weights which survive the pruning,reinitialize it to ONE (1)-\n",
    "        for wts in mask_model_stripped.trainable_weights:\n",
    "            wts.assign(tf.where(tf.equal(wts, 0.), 0., 1.))\n",
    "\n",
    "        \n",
    "        # Extract Winning Ticket:\n",
    "        \n",
    "        # Instantiate a new neural network model for which, the weights are to be extracted-\n",
    "        winning_ticket_model = pruned_nn(pruning_params_unpruned)\n",
    "        \n",
    "        # Load weights of PRUNED model-\n",
    "        # winning_ticket_model.load_weights(\"LeNet_MNIST_Pruned_Weights.h5\")\n",
    "        winning_ticket_model.set_weights(pruned_model.get_weights())\n",
    "        \n",
    "        # Strip the model of its pruning parameters-\n",
    "        winning_ticket_model_stripped = sparsity.strip_pruning(winning_ticket_model)\n",
    "        \n",
    "        # For each layer, for each weight which is 0, leave it, as is. And for weights which survive the pruning,\n",
    "        # reinitialize it to the value, the model received BEFORE it was trained and pruned-\n",
    "        for orig_wts, pruned_wts in zip(orig_model_stripped.trainable_weights,\n",
    "                                        winning_ticket_model_stripped.trainable_weights):\n",
    "            pruned_wts.assign(tf.where(tf.equal(pruned_wts, 0), pruned_wts, orig_wts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
