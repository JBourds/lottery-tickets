{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae830d3-ea46-4835-949b-8cc6522b7b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from tensorflow import keras\n",
    "import numpy.typing as npt\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "print(\"Numpy Version:\", np.__version__)\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Generator, List, Tuple\n",
    "\n",
    "from src.harness import architecture as arch\n",
    "from src.harness import dataset as ds\n",
    "from src.harness import meta\n",
    "from src.harness import history as hist\n",
    "\n",
    "from src.metrics import features as f\n",
    "from src.metrics.synflow import compute_synflow_per_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137726ca-0ca7-4f8a-a6da-87a8c9b19b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reload(f)\n",
    "\n",
    "epath = \"/users/j/b/jbourde2/lottery-tickets/experiments/11-04-2024/lenet_mnist_0_seed_5_experiments_1_batches_0.025_default_sparsity_lm_pruning_20241102-111614\"\n",
    "experiments = list(hist.get_experiments(epath))\n",
    "e0 = experiments[0]\n",
    "t0 = next(e0)\n",
    "t0.seed_weights = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90743cae-a5f7-4f82-85b9-e25e6185dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"weightabase.pkl\"\n",
    "merged_df = pd.read_pickle(df_path)\n",
    "# corrected_wdf = correct_class_imbalance(wdf)\n",
    "# merged_df = merge_dfs(tdf, ldf, corrected_wdf)\n",
    "# merged_df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63232da-fe18-48d8-9105-ab3d5fc2adbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.to_pickle(\"weightabase.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdb949-840e-412e-8936-e43985c26f9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Feature importance ovservations:\n",
    "\n",
    "- Mask & sign directly tell the model what the outcome is (sign == 0 rather than +/- 1) which gets it perfectly\n",
    "- The final measures for weights all get >93% accuracy, magnitude gets 98.14%\n",
    "- Measures of current sparsity get pretty high (layer and overall sparsity both ~78%)\n",
    "- The initial percentile a weight falls in gets 75% (wi_std gets much worse even though they are the same measure- perhaps the scale being between 0 and 1 makes it easier to train on?)\n",
    "- All the OHE and initial weight magnitude measures get 57.85% accuracy\n",
    "    - What is special about this number?\n",
    "    - Why are the initial metrics (including magnitude) so uninformative?\n",
    "        - Could a normalization scheme help this?\n",
    "    - Why does \"wi_std\" do worse than random chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b042f99e-7dc0-48ee-a45c-4643e212ea99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import copy as shallowcopy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "def make_meta_mask(\n",
    "    meta: keras.Model,\n",
    "    make_x: Callable[[str, str, keras.Model, List[npt.NDArray]], npt.NDArray],\n",
    "    architecture: str,\n",
    "    dataset: str,\n",
    "    steps: int,\n",
    ") -> Tuple[List[npt.NDArray], List[float]]:\n",
    "    a = arch.Architecture(architecture, dataset)\n",
    "    _, val_X, _, val_Y = a.load_data()\n",
    "    model = a.get_model_constructor()()\n",
    "    original_weights = copy.deepcopy(model.get_weights())\n",
    "    model.compile(optimizer=\"Adam\", loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    masks = [np.ones_like(w) for w in model.get_weights()]\n",
    "    \n",
    "    def update_masks(mask_pred: npt.NDArray) -> List[npt.NDArray]:\n",
    "        start = 0\n",
    "        end = 0\n",
    "        new_masks = []\n",
    "        nonlocal masks\n",
    "        for m in masks:\n",
    "            end += m.size\n",
    "            new_m = np.reshape(mask_pred[start:end], m.shape)\n",
    "            new_masks.append(new_m)\n",
    "            start = end\n",
    "        return masks\n",
    "            \n",
    "    accuracies = []\n",
    "    for step in range(steps):\n",
    "        # Get validation accuracy\n",
    "        _, accuracy = model.evaluate(val_X, val_Y)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Step {step} accuracy: {accuracy:.2%}\")\n",
    "        # Extract features\n",
    "        X = make_x(architecture, model, masks)\n",
    "        # Predict and replace existing mask\n",
    "        mask_pred = meta.predict(X, batch_size=2**20)\n",
    "        masks = update_masks(mask_pred)\n",
    "        model.set_weights([w * m for w, m in zip(original_weights, masks)])\n",
    "        \n",
    "    return masks, accuracies\n",
    "\n",
    "\n",
    "def make_x(\n",
    "    architecture: str,\n",
    "    model: keras.Model,\n",
    "    masks: List[npt.NDArray],\n",
    "    train_steps: int = 0,\n",
    "    batch_size: int = 32,\n",
    ") -> npt.NDArray:\n",
    "    # Layer features:\n",
    "    nparams = sum(map(np.size, masks))\n",
    "    nfeatures = 11\n",
    "    features = np.zeros((nparams, nfeatures))\n",
    "    \n",
    "    # Helper functions to add the unrolled weight values and\n",
    "    # scalar layer values to the feature matrix\n",
    "    n = 0\n",
    "    def add_layer_features(layer_values: List[float]):\n",
    "        nonlocal n\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for v, size in zip(layer_values, map(np.size, masks)):\n",
    "            end += size\n",
    "            features[start:end, n] = v\n",
    "            start = end\n",
    "        n += 1\n",
    "        \n",
    "    def add_weight_features(weight_features: List[npt.NDArray]):\n",
    "        nonlocal n\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for v in weight_features:\n",
    "            end += v.size\n",
    "            features[start:end, n] = np.ravel(v)\n",
    "            start = end\n",
    "        n += 1\n",
    "    \n",
    "    # Make a separate copy to compute synflow for\n",
    "    masked_weights = [w * m for w, m in zip(model.get_weights(), masks)]\n",
    "    masked_model = shallowcopy(model)\n",
    "    masked_model.set_weights(masked_weights)\n",
    "    synflow_scores = [np.reshape(scores, -1) for scores in compute_synflow_per_weight(masked_model)]\n",
    "    \n",
    "    # Mask features\n",
    "    sparsities = [np.count_nonzero(m) / np.size(m) for m in masks]\n",
    "    rel_size = [np.size(m) / nparams for m in masks]\n",
    "    prop_pos = [np.count_nonzero(w >= 0) for w in masks]\n",
    "    \n",
    "    # Layer type\n",
    "    layer_ohe = arch.Architecture.ohe_layer_types(architecture)\n",
    "    for values in [sparsities, rel_size, prop_pos]:\n",
    "        add_layer_features(values)\n",
    "    \n",
    "    # Weight features\n",
    "    l_std = [np.std(w) for w in masked_weights]\n",
    "    l_mean = [np.mean(w) for w in masked_weights]\n",
    "    l_sorted = [np.sort(np.ravel(w)) for w in masked_weights]\n",
    "    \n",
    "    w_std = [(w - l_mean) / l_std for w, l_mean, l_std in zip(l_std, l_mean, masked_weights)]\n",
    "    w_sign = [np.sign(w) for w in masked_weights]\n",
    "    num_nonzero = sum(map(np.count_nonzero, masks))\n",
    "    num_zero = nparams - num_nonzero\n",
    "    w_perc = np.array([\n",
    "        np.argmax(np.ravel(v) < v_sorted) - num_zero \n",
    "        for v, v_sorted in zip(masked_weights, l_sorted)]\n",
    "    ) / num_nonzero\n",
    "    \n",
    "    flat_masks = [np.ravel(m) for m in masks]\n",
    "    for values in [w_std, w_perc, synflow_scores, w_sign]:\n",
    "        add_weight_features(values)\n",
    "    \n",
    "    for values in [layer_ohe[:, i] for i in range(layer_ohe.shape[1])]:\n",
    "        add_layer_features(values)\n",
    "        \n",
    "    return features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lt",
   "language": "python",
   "name": "lt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
