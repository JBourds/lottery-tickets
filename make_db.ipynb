{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3ae830d3-ea46-4835-949b-8cc6522b7b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Version: 1.26.4\n",
      "Tensorflow Version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "from tensorflow import keras\n",
    "import numpy.typing as npt\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "print(\"Numpy Version:\", np.__version__)\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Generator, List, Tuple\n",
    "\n",
    "from src.harness import architecture as arch\n",
    "from src.harness import dataset as ds\n",
    "from src.harness import meta\n",
    "from src.harness import history as hist\n",
    "\n",
    "from src.metrics.features import *\n",
    "from src.metrics.synflow import compute_synflow_per_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "027c4eb3-f08c-41dd-8efa-8d6cd24847e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(x: pd.Series) -> pd.Series:\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "\n",
    "def get_train_one_step() -> Callable:\n",
    "    @tf.function\n",
    "    def train_one_step(\n",
    "        model: tf.keras.Model,\n",
    "        masks: List[tf.Tensor],\n",
    "        inputs: tf.Tensor,\n",
    "        labels: tf.Tensor,\n",
    "        optimizer: tf.keras.optimizers.Optimizer,\n",
    "        loss_fn: tf.keras.metrics.Metric,\n",
    "    ) -> float:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = loss_fn(labels, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        grad_mask_mul = []\n",
    "        for grad_layer, mask in zip(gradients, masks):\n",
    "            grad_mask_mul.append(tf.math.multiply(grad_layer, mask))\n",
    "        optimizer.apply_gradients(\n",
    "            zip(grad_mask_mul, model.trainable_weights))\n",
    "        return loss\n",
    "\n",
    "    return train_one_step\n",
    "\n",
    "def build_weight_df_with_training(\n",
    "    layer_df: pd.DataFrame,\n",
    "    architecture: arch.Architecture,\n",
    "    weights: List[npt.NDArray[np.float32]],\n",
    "    masks: List[npt.NDArray[np.float32]],\n",
    "    n: int = 1, \n",
    "    batch_size: int = 32,\n",
    "    optimizer: tf.keras.optimizers.Optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss_fn: tf.keras.losses.Loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    ") -> pd.DataFrame:\n",
    "    # Training\n",
    "    model = architecture.get_model_constructor()()\n",
    "    model.set_weights([w * m for w, m in zip(weights, masks)])\n",
    "    X_train, _, Y_train, _ = architecture.load_data()\n",
    "    masks = list(map(lambda x: tf.convert_to_tensor(x, dtype=tf.float64), masks))\n",
    "    X_train = tf.random.shuffle(X_train, seed=0)\n",
    "    Y_train = tf.random.shuffle(Y_train, seed=0)\n",
    "    \n",
    "    @tf.function\n",
    "    def do_train_steps() -> List[float]:\n",
    "        losses = []\n",
    "        for i in tqdm(range(n)):\n",
    "            start = i * batch_size\n",
    "            stop = start + batch_size\n",
    "            inputs, labels = X_train[start:stop], Y_train[start:stop]\n",
    "            train_one_step = get_train_one_step()\n",
    "            losses.append(train_one_step(model, masks, inputs, labels, optimizer, loss_fn))\n",
    "        return losses\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    losses = do_train_steps()\n",
    "    trained = model.get_weights()\n",
    "    \n",
    "    # Compute features\n",
    "    weight_features_list = []\n",
    "    start_idx = 0\n",
    "    print(\"Making weight features with training\")\n",
    "    num_weights = sum(map(np.size, model.get_weights()))\n",
    "    shape = [1] + list(model.input_shape[1:])\n",
    "    t_synflow = compute_synflow_per_weight(model)\n",
    "    \n",
    "    # Duplicate features for initial vs. final weights\n",
    "    for layer, (tw, m) in tqdm(enumerate(zip(trained, masks))):\n",
    "        print(f\"Layer {layer}\")\n",
    "        mask = m.numpy().astype(bool).ravel()\n",
    "        num_params = len(mask)\n",
    "        num_nonzero = np.count_nonzero(mask)\n",
    "        num_zero = num_params - num_nonzero\n",
    "        # Precompute values for this layer\n",
    "        layer_num = np.full(num_params, layer, dtype=np.int8)\n",
    "        weight_nums = np.arange(num_params, dtype=np.int32)\n",
    "\n",
    "        t_flat = tw.flatten()\n",
    "        t_sorted = np.sort(t_flat)\n",
    "        t_sign = np.sign(t_flat)\n",
    "        t_mag = np.abs(t_flat, dtype=np.float32)\n",
    "        t_perc = np.array([np.argmax(v < t_sorted) - num_zero for v in t_flat]) / num_nonzero\n",
    "        # Use std from initial weights assuming we did small amounts of training\n",
    "        t_norm_std = (t_flat - layer_df[\"li_mean\"].iloc[layer]) / layer_df[\"li_std\"].iloc[layer]\n",
    "         \n",
    "        # Create a dictionary for weight features for this layer\n",
    "        layer_weight_features = {\n",
    "            \"l_num\": layer_num,\n",
    "            \"w_num\": weight_nums,\n",
    "            f\"wt{n}_sign\": t_sign,\n",
    "            f\"wt{n}_val\": t_flat,\n",
    "            f\"wt{n}_mag\": t_mag,\n",
    "            f\"wt{n}_perc\": t_perc.astype(np.float32),\n",
    "            f\"wt{n}_std\": t_norm_std.astype(np.float32),\n",
    "            f\"wt{n}_synflow\": t_synflow[layer].numpy().flatten(),\n",
    "            \"w_mask\": mask,\n",
    "        }\n",
    "        \n",
    "        weight_features_list.append(pd.DataFrame(layer_weight_features))\n",
    "\n",
    "    weight_df = pd.concat(weight_features_list, axis=0, ignore_index=True)\n",
    "    \n",
    "    keys = [\"e_num\", \"t_num\", \"l_num\"]\n",
    "    if (key := \"norm_wi_mag\") not in merged_df.columns:\n",
    "        weight_df[key] = weight_df.groupby(keys)[\"wi_mag\"].transform(normalize)\n",
    "    if (key := \"norm_synflow\") not in merged_df.columns:\n",
    "        weight_df[key] = weight_df.groupby(keys)[\"wi_synflow\"].transform(normalize)\n",
    "    \n",
    "    return weight_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "464a2499-4f29-4dde-b543-7b3e86d9ce5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_num</th>\n",
       "      <th>w_num</th>\n",
       "      <th>wt100_sign</th>\n",
       "      <th>wt100_val</th>\n",
       "      <th>wt100_mag</th>\n",
       "      <th>wt100_perc</th>\n",
       "      <th>wt100_std</th>\n",
       "      <th>wt100_synflow</th>\n",
       "      <th>w_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.063685</td>\n",
       "      <td>0.063685</td>\n",
       "      <td>0.064677</td>\n",
       "      <td>-1.484341</td>\n",
       "      <td>4.948736e-19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.497181</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>4.089137e-22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.036945</td>\n",
       "      <td>0.036945</td>\n",
       "      <td>0.241696</td>\n",
       "      <td>-0.862135</td>\n",
       "      <td>1.321771e-18</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.020769</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>0.349188</td>\n",
       "      <td>-0.485728</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.019048</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.360940</td>\n",
       "      <td>-0.445678</td>\n",
       "      <td>6.917020e-19</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   l_num  w_num  wt100_sign  wt100_val  wt100_mag  wt100_perc  wt100_std  \\\n",
       "0      0      0        -1.0  -0.063685   0.063685    0.064677  -1.484341   \n",
       "1      0      1         1.0   0.001072   0.001072    0.497181   0.022486   \n",
       "2      0      2        -1.0  -0.036945   0.036945    0.241696  -0.862135   \n",
       "3      0      3        -1.0  -0.020769   0.020769    0.349188  -0.485728   \n",
       "4      0      4        -1.0  -0.019048   0.019048    0.360940  -0.445678   \n",
       "\n",
       "   wt100_synflow  w_mask  \n",
       "0   4.948736e-19    True  \n",
       "1   4.089137e-22    True  \n",
       "2   1.321771e-18    True  \n",
       "3   0.000000e+00    True  \n",
       "4   6.917020e-19    True  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_wdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fbe6b61b-ca53-4292-84b2-7b04ab519472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_num</th>\n",
       "      <th>w_num</th>\n",
       "      <th>wf_sign</th>\n",
       "      <th>wi_sign</th>\n",
       "      <th>wf_val</th>\n",
       "      <th>wi_val</th>\n",
       "      <th>wf_mag</th>\n",
       "      <th>wi_mag</th>\n",
       "      <th>wf_perc</th>\n",
       "      <th>wi_perc</th>\n",
       "      <th>wf_std</th>\n",
       "      <th>wi_std</th>\n",
       "      <th>w_mask</th>\n",
       "      <th>wf_synflow</th>\n",
       "      <th>wi_synflow</th>\n",
       "      <th>e_num</th>\n",
       "      <th>t_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.035860</td>\n",
       "      <td>-0.063685</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.063685</td>\n",
       "      <td>0.368172</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>-0.275510</td>\n",
       "      <td>-0.836884</td>\n",
       "      <td>True</td>\n",
       "      <td>1.144758e-18</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.750361</td>\n",
       "      <td>0.506867</td>\n",
       "      <td>0.625495</td>\n",
       "      <td>1.252448</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.111195e-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.065232</td>\n",
       "      <td>-0.036945</td>\n",
       "      <td>0.065232</td>\n",
       "      <td>0.036945</td>\n",
       "      <td>0.792908</td>\n",
       "      <td>0.251063</td>\n",
       "      <td>0.738902</td>\n",
       "      <td>1.515425</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.013991e-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.023325</td>\n",
       "      <td>-0.020769</td>\n",
       "      <td>0.023325</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>0.422258</td>\n",
       "      <td>0.359009</td>\n",
       "      <td>-0.149734</td>\n",
       "      <td>-0.545222</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.069716</td>\n",
       "      <td>-0.019048</td>\n",
       "      <td>0.069716</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.232657</td>\n",
       "      <td>0.370429</td>\n",
       "      <td>-0.615245</td>\n",
       "      <td>-1.624690</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   l_num  w_num  wf_sign  wi_sign    wf_val    wi_val    wf_mag    wi_mag  \\\n",
       "0      0      0     -1.0     -1.0 -0.035860 -0.063685  0.035860  0.063685   \n",
       "1      0      1      1.0      1.0  0.053931  0.001072  0.053931  0.001072   \n",
       "2      0      2      1.0     -1.0  0.065232 -0.036945  0.065232  0.036945   \n",
       "3      0      3     -1.0     -1.0 -0.023325 -0.020769  0.023325  0.020769   \n",
       "4      0      4     -1.0     -1.0 -0.069716 -0.019048  0.069716  0.019048   \n",
       "\n",
       "    wf_perc   wi_perc    wf_std    wi_std  w_mask    wf_synflow    wi_synflow  \\\n",
       "0  0.368172  0.072266 -0.275510 -0.836884    True  1.144758e-18  0.000000e+00   \n",
       "1  0.750361  0.506867  0.625495  1.252448    True  0.000000e+00  5.111195e-22   \n",
       "2  0.792908  0.251063  0.738902  1.515425    True  0.000000e+00  6.013991e-20   \n",
       "3  0.422258  0.359009 -0.149734 -0.545222    True  0.000000e+00  0.000000e+00   \n",
       "4  0.232657  0.370429 -0.615245 -1.624690    True  0.000000e+00  0.000000e+00   \n",
       "\n",
       "   e_num  t_num  \n",
       "0      0      0  \n",
       "1      0      0  \n",
       "2      0      0  \n",
       "3      0      0  \n",
       "4      0      0  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137726ca-0ca7-4f8a-a6da-87a8c9b19b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epath = \"/users/j/b/jbourde2/lottery-tickets/experiments/11-04-2024/lenet_mnist_0_seed_5_experiments_1_batches_0.025_default_sparsity_lm_pruning_20241102-111614\"\n",
    "experiments = list(hist.get_experiments(epath))\n",
    "e0 = experiments[0]\n",
    "t0 = next(e0)\n",
    "t0.seed_weights = lambda x: x\n",
    "\n",
    "tdf, ldf, wdf = build_trial_dfs(t0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ac384430-8212-474c-9147-3d83df0c6f84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses:\n",
      "[2.230298603006082,\n",
      " 2.3053508395794537,\n",
      " 2.275178110098286,\n",
      " 2.208902956778287,\n",
      " 2.186161806067825,\n",
      " 2.0944166448669366,\n",
      " 2.0050227526189026,\n",
      " 2.075310553995859,\n",
      " 1.9836495647897308,\n",
      " 1.9801935397815325,\n",
      " 1.972452241723449,\n",
      " 1.9442885349934083,\n",
      " 1.887469338344589,\n",
      " 1.816022684273078,\n",
      " 1.8113223037462602,\n",
      " 1.8031308834765007,\n",
      " 1.7182051447774602,\n",
      " 1.6096276986199278,\n",
      " 1.6579361597120879,\n",
      " 1.6602321224784737,\n",
      " 1.479272697219519,\n",
      " 1.6248499472272693,\n",
      " 1.4124958408002937,\n",
      " 1.3347886576121908,\n",
      " 1.477865186036193,\n",
      " 1.304608082980669,\n",
      " 1.314242112637305,\n",
      " 1.5095975022683503,\n",
      " 1.2804871309528327,\n",
      " 1.288843145380243,\n",
      " 1.194197481541245,\n",
      " 1.1868096026456576,\n",
      " 1.260358833448425,\n",
      " 1.0716644589277218,\n",
      " 1.2438025900059848,\n",
      " 1.0820792051675792,\n",
      " 1.031371358947578,\n",
      " 1.0747625418955977,\n",
      " 0.957824997117374,\n",
      " 0.8359280157597959,\n",
      " 0.936098094692311,\n",
      " 1.0919245467021486,\n",
      " 0.8434738705767453,\n",
      " 0.8631648917820764,\n",
      " 0.9318111802309972,\n",
      " 0.9907606580469555,\n",
      " 0.7983439205969569,\n",
      " 0.7071748766598337,\n",
      " 1.0326735069941926,\n",
      " 0.884048657442586,\n",
      " 0.8237782001596865,\n",
      " 0.9240872099544246,\n",
      " 0.638630525881186,\n",
      " 0.8134084313781611,\n",
      " 0.6973211097009626,\n",
      " 0.6519593694334944,\n",
      " 0.8365557841694775,\n",
      " 0.7016159084517629,\n",
      " 0.6168564722954413,\n",
      " 0.6401227906716718,\n",
      " 0.7375058349093762,\n",
      " 0.6163302425014836,\n",
      " 0.7246696812615917,\n",
      " 0.5435159550076842,\n",
      " 0.7137638375341527,\n",
      " 0.5959041606736932,\n",
      " 0.6607540912627654,\n",
      " 0.7921880714610123,\n",
      " 0.6204335376039385,\n",
      " 0.6129837396833568,\n",
      " 0.6874608358079707,\n",
      " 0.5475013440422405,\n",
      " 0.7272861283532874,\n",
      " 0.5926154852425807,\n",
      " 0.6086356021063426,\n",
      " 0.6584268530517723,\n",
      " 0.6405119800681363,\n",
      " 0.6843406767127225,\n",
      " 0.5527930985731172,\n",
      " 0.5242069502088882,\n",
      " 0.48727525031926644,\n",
      " 0.5406060915978677,\n",
      " 0.5141590746353271,\n",
      " 0.48507584540634613,\n",
      " 0.6480699127470104,\n",
      " 0.5067884181272391,\n",
      " 0.431790226409143,\n",
      " 0.362287508590015,\n",
      " 0.4224255890295545,\n",
      " 0.502240366487031,\n",
      " 0.48742336421366195,\n",
      " 0.4090118716332175,\n",
      " 0.41623937065613137,\n",
      " 0.4129754086137205,\n",
      " 0.5039799764589048,\n",
      " 0.5737566593964878,\n",
      " 0.5039430335060432,\n",
      " 0.40526843465658136,\n",
      " 0.5985437458654075,\n",
      " 0.391476253041221]\n",
      "Making weight features with training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:31, 31.91s/it]/tmp/ipykernel_188286/2774183975.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  t_norm_std = (t_flat - layer_df[\"li_mean\"].iloc[layer]) / layer_df[\"li_std\"].iloc[layer]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "Layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:32,  8.35s/it]/tmp/ipykernel_188286/2774183975.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  t_norm_std = (t_flat - layer_df[\"li_mean\"].iloc[layer]) / layer_df[\"li_std\"].iloc[layer]\n",
      "6it [00:32,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3\n",
      "Layer 4\n",
      "Layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_wdf = build_weight_df_with_training(\n",
    "    ldf, \n",
    "    arch.Architecture(t0.architecture, t0.dataset), \n",
    "    t0.initial_weights,\n",
    "    t0.masks,\n",
    "    n=100,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90743cae-a5f7-4f82-85b9-e25e6185dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"mnist_weightabase.pkl\"\n",
    "merged_df = pd.read_pickle(df_path)\n",
    "# corrected_wdf = correct_class_imbalance(wdf)\n",
    "# merged_df = merge_dfs(tdf, ldf, corrected_wdf)\n",
    "# merged_df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a49815-f6d3-4235-a257-098b86485942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(x: pd.Series) -> pd.Series:\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "keys = [\"e_num\", \"t_num\", \"l_num\"]\n",
    "if (key := \"norm_wi_mag\") not in merged_df.columns:\n",
    "    merged_df[key] = merged_df.groupby(keys)[\"wi_mag\"].transform(normalize)\n",
    "if (key := \"norm_synflow\") not in merged_df.columns:\n",
    "    merged_df[key] = merged_df.groupby(keys)[\"wi_synflow\"].transform(normalize)\n",
    "# if (key := \"const_synflow\") not in merged_df.columns:\n",
    "#     merged_df[key] = merged_df[\"wi_synflow\"].map(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdb949-840e-412e-8936-e43985c26f9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Feature importance ovservations:\n",
    "\n",
    "- Mask & sign directly tell the model what the outcome is (sign == 0 rather than +/- 1) which gets it perfectly\n",
    "- The final measures for weights all get >93% accuracy, magnitude gets 98.14%\n",
    "- Measures of current sparsity get pretty high (layer and overall sparsity both ~78%)\n",
    "- The initial percentile a weight falls in gets 75% (wi_std gets much worse even though they are the same measure- perhaps the scale being between 0 and 1 makes it easier to train on?)\n",
    "- All the OHE and initial weight magnitude measures get 57.85% accuracy\n",
    "    - What is special about this number?\n",
    "    - Why are the initial metrics (including magnitude) so uninformative?\n",
    "        - Could a normalization scheme help this?\n",
    "    - Why does \"wi_std\" do worse than random chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b042f99e-7dc0-48ee-a45c-4643e212ea99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import copy as shallowcopy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "def create_meta(shape: Tuple[int, ...]) -> keras.Model:\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=shape),\n",
    "        keras.layers.Dense(8, \"relu\"),\n",
    "        keras.layers.Dense(8, \"relu\"),\n",
    "        keras.layers.Dense(8, \"relu\"),\n",
    "        keras.layers.Dense(8, \"relu\"),\n",
    "        keras.layers.Dense(8, \"relu\"),\n",
    "        keras.layers.Dense(8, \"relu\"),\n",
    "        keras.layers.Dense(8, \"relu\"),\n",
    "        keras.layers.Dense(8, \"relu\"),\n",
    "        \n",
    "        keras.layers.Dense(1, \"sigmoid\"),\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def make_meta_mask(\n",
    "    meta: keras.Model,\n",
    "    make_x: Callable[[str, str, keras.Model, List[npt.NDArray]], npt.NDArray],\n",
    "    architecture: str,\n",
    "    dataset: str,\n",
    "    steps: int,\n",
    ") -> Tuple[List[npt.NDArray], List[float]]:\n",
    "    a = arch.Architecture(architecture, dataset)\n",
    "    _, val_X, _, val_Y = a.load_data()\n",
    "    model = a.get_model_constructor()()\n",
    "    original_weights = copy.deepcopy(model.get_weights())\n",
    "    model.compile(optimizer=\"Adam\", loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    masks = [np.ones_like(w) for w in model.get_weights()]\n",
    "    \n",
    "    def update_masks(mask_pred: npt.NDArray) -> List[npt.NDArray]:\n",
    "        start = 0\n",
    "        end = 0\n",
    "        new_masks = []\n",
    "        nonlocal masks\n",
    "        for m in masks:\n",
    "            end += m.size\n",
    "            new_m = np.reshape(mask_pred[start:end], m.shape)\n",
    "            new_masks.append(new_m)\n",
    "            start = end\n",
    "        return masks\n",
    "            \n",
    "    accuracies = []\n",
    "    for step in range(steps):\n",
    "        # Get validation accuracy\n",
    "        _, accuracy = model.evaluate(val_X, val_Y)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Step {step} accuracy: {accuracy:.2%}\")\n",
    "        # Extract features\n",
    "        X = make_x(architecture, model, masks)\n",
    "        # Predict and replace existing mask\n",
    "        mask_pred = meta.predict(X, batch_size=2**20)\n",
    "        masks = update_masks(mask_pred)\n",
    "        model.set_weights([w * m for w, m in zip(original_weights, masks)])\n",
    "        \n",
    "    return masks, accuracies\n",
    "\n",
    "\n",
    "def make_x(\n",
    "    architecture: str,\n",
    "    model: keras.Model,\n",
    "    masks: List[npt.NDArray],\n",
    "    train_batches: int = 0,\n",
    "    batch_size: int = 32,\n",
    ") -> npt.NDArray:\n",
    "    # Layer features:\n",
    "    # i_features = [\"l_sparsity\", \"l_rel_size\", \"li_prop_positive\", \"wi_std\", \"wi_perc\", \"wi_synflow\", \"wi_sign\", \"dense\", \"bias\", \"conv\", \"output\"]\n",
    "    nparams = sum(map(np.size, masks))\n",
    "    nfeatures = 11\n",
    "    features = np.zeros((nparams, nfeatures))\n",
    "    \n",
    "    # Helper functions to add the unrolled weight values and\n",
    "    # scalar layer values to the feature matrix\n",
    "    n = 0\n",
    "    def add_layer_features(layer_values: List[float]):\n",
    "        nonlocal n\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for v, size in zip(layer_values, map(np.size, masks)):\n",
    "            end += size\n",
    "            features[start:end, n] = v\n",
    "            start = end\n",
    "        n += 1\n",
    "        \n",
    "    def add_weight_features(weight_features: List[npt.NDArray]):\n",
    "        nonlocal n\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for v in weight_features:\n",
    "            end += v.size\n",
    "            features[start:end, n] = np.ravel(v)\n",
    "            start = end\n",
    "        n += 1\n",
    "    \n",
    "    # Make a separate copy to compute synflow for\n",
    "    masked_weights = [w * m for w, m in zip(model.get_weights(), masks)]\n",
    "    masked_model = shallowcopy(model)\n",
    "    masked_model.set_weights(masked_weights)\n",
    "    synflow_scores = [np.reshape(scores, -1) for scores in compute_synflow_per_weight(masked_model)]\n",
    "    \n",
    "    # Mask features\n",
    "    sparsities = [np.count_nonzero(m) / np.size(m) for m in masks]\n",
    "    rel_size = [np.size(m) / nparams for m in masks]\n",
    "    prop_pos = [np.count_nonzero(w >= 0) for w in masks]\n",
    "    \n",
    "    # Layer type\n",
    "    layer_ohe = arch.Architecture.ohe_layer_types(architecture)\n",
    "    for values in [sparsities, rel_size, prop_pos]:\n",
    "        add_layer_features(values)\n",
    "    \n",
    "    # Weight features\n",
    "    l_std = [np.std(w) for w in masked_weights]\n",
    "    l_mean = [np.mean(w) for w in masked_weights]\n",
    "    l_sorted = [np.sort(np.ravel(w)) for w in masked_weights]\n",
    "    \n",
    "    w_std = [(w - l_mean) / l_std for w, l_mean, l_std in zip(l_std, l_mean, masked_weights)]\n",
    "    w_sign = [np.sign(w) for w in masked_weights]\n",
    "    num_nonzero = sum(map(np.count_nonzero, masks))\n",
    "    num_zero = nparams - num_nonzero\n",
    "    w_perc = np.array([\n",
    "        np.argmax(np.ravel(v) < v_sorted) - num_zero \n",
    "        for v, v_sorted in zip(masked_weights, l_sorted)]\n",
    "    ) / num_nonzero\n",
    "    \n",
    "    flat_masks = [np.ravel(m) for m in masks]\n",
    "    for values in [w_std, w_perc, synflow_scores, w_sign]:\n",
    "        add_weight_features(values)\n",
    "    \n",
    "    for values in [layer_ohe[:, i] for i in range(layer_ohe.shape[1])]:\n",
    "        add_layer_features(values)\n",
    "        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a9158-5538-454d-b09d-08badcea47ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(model: keras.Model):\n",
    "    summary_str = []\n",
    "    model.summary(print_fn=lambda x: summary_str.append(x))\n",
    "    return summary_str[0]\n",
    "\n",
    "def best_subset(subsets: Dict) -> Tuple[Tuple[str], float]:\n",
    "    best_key = None\n",
    "    best_accuracy = None\n",
    "    for key, value in subsets.items():\n",
    "        if best_key is None or max(value[\"accuracy\"]) > best_accuracy:\n",
    "            best_key = key\n",
    "            best_accuracy = max(value[\"accuracy\"])\n",
    "    return best_key, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d2c94-3263-4835-83bc-cf675cf8d8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# relu_1layer_8wide = relu_1layer\n",
    "# relu_1layer_12wide = {}\n",
    "# relu_1layer_16wide = {}\n",
    "# relu_1layer_128wide = {}\n",
    "# relu_8layer_8wide = {}\n",
    "# relu_8layer_128wide = {}\n",
    "variants = [\n",
    "       (\"1 layer, 8 wide\", relu_1layer_8wide,),\n",
    "       (\"1 layer, 12 wide\", relu_1layer_12wide,),\n",
    "       (\"1 layer, 16 wide\", relu_1layer_16wide,),\n",
    "       (\"1 layer, 128 wide\", relu_1layer_128wide,),\n",
    "       (\"8 layer, 8 wide\", relu_8layer_8wide,),\n",
    "       (\"8 layer, 128 wide\", relu_8layer_128wide,),\n",
    "]\n",
    "for variant, data in variants:\n",
    "    print(variant)\n",
    "    print(best_subset(data))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58340e3-a403-4fba-b842-7b8d1d851854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Takeaways (no training steps):\n",
    "#     - Large diminishing marginal returns beyond a 1 layer 16 neuron ReLU network\n",
    "def plot_feature_arch_subsets(variants: List[Tuple[str, Dict[Tuple[str], Dict]]]):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(\"Architecture Sweep over Feature Subsets\")\n",
    "    plt.xlabel(\"Feature Subset\")\n",
    "    plt.ylabel(\"Max Accuracy (%)\")\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "    for variant, data_dict in variants:\n",
    "        categories = []\n",
    "        accuracies = []\n",
    "        for features, data in data_dict.items():\n",
    "            categories.append(\",\\n\".join(features))\n",
    "            accuracies.append(max(data[\"accuracy\"]))\n",
    "        plt.scatter(categories, accuracies, label=variant)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.ylim(bottom=0.75)\n",
    "    plt.savefig(\"arch_feature_subsets_no_training.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_arch_subsets(variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c4746-3261-4dd7-aee6-80553020da48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for variant, feature_subsets in variants:\n",
    "#     filename = \"_\".join([v.strip(\",\") for v in variant.split()]) + \".json\"\n",
    "#     with open(filename, \"w\") as outfile:\n",
    "#         to_save = {\", \".join(key): value for key, value in feature_subsets.items()}\n",
    "#         json.dump(to_save, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6f754-4d95-4881-bd34-a5daaa5b8b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature subsets\n",
    "import json\n",
    "\n",
    "# Feature scale is very important - normalized synaptic scores alone aren't better than random chance \n",
    "# but when added with a more informative feature (e.g., wi_perc) they add ~2% accuracy.\n",
    "# Straight up synaptic flow scores are too low to make a meaningful difference\n",
    "for variant, feature_subsets in variants:\n",
    "    print(\"VARIANT:\", variant)\n",
    "    if feature_subsets:\n",
    "        continue\n",
    "    for features in relu_1layer_8wide:\n",
    "        print(features)\n",
    "        # features = [\"norm_synflow\", \"wi_perc\", \"sparsity\", \"l_sparsity\"]\n",
    "        X, Y = featurize_db(merged_df, list(features))\n",
    "\n",
    "        model = create_meta(X[0].shape)\n",
    "        summary = summarize(model)\n",
    "        epochs = 3\n",
    "        batch_size = 256\n",
    "        history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=0.2, shuffle=True)\n",
    "        feature_subsets[tuple(features)] = {\"epochs\": epochs, \"batch_size\": batch_size, \"accuracy\": history.history[\"accuracy\"], \"summary\": summary}\n",
    "\n",
    "        filename = \"_\".join([v.strip(\",\") for v in variant.split()]) + \".json\"\n",
    "        with open(filename, \"w\") as outfile:\n",
    "            to_save = {\", \".join(key): value for key, value in feature_subsets.items()}\n",
    "            json.dump(to_save, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbab91d-59d4-45da-9a16-e881bc3f7118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy of 57.57% is what can be achieved using random noise as a feature\n",
    "features = [\"norm_synflow\"]\n",
    "X, Y = featurize_db(merged_df, features)\n",
    "\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "X = rng.standard_normal(size=X.shape)\n",
    "model = create_meta(X[0].shape)\n",
    "model.fit(X, Y, epochs=1, batch_size=256, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd16936-8423-4513-99ed-b4487cc395d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lt",
   "language": "python",
   "name": "lt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
