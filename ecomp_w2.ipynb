{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6278b811-d488-4ff7-8a1d-23fd73d96c97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 13:58:43.722220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-08 13:58:43.799369: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-08 13:58:43.829267: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 13:58:43.965306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-08 13:58:49.012740: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.harness import architecture as arch\n",
    "from src.harness import utils\n",
    "\n",
    "import copy\n",
    "from enum import Enum\n",
    "import functools\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Any, Callable, Dict, Iterable, List, Literal, Set, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "feccc459-f158-4723-bd37-d6751533ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typedefs\n",
    "Mutation = Callable[[Literal['Individual']], None]\n",
    "Crossover = Callable[[Literal['Individual'], Literal['Individual']], Literal['Individual']]\n",
    "FitnessFunction = Callable[[Literal['Individual']], float]\n",
    "# Must return a list matching dimensionality of input model since the input\n",
    "# to the model is a single value for every synapse\n",
    "FeatureSelector = Callable[[keras.Model], List[np.ndarray]]\n",
    "\n",
    "def layer_sparsity_features(model: keras.Model) -> List[np.ndarray]:\n",
    "    sparsities = [\n",
    "        nonzero / total \n",
    "        for total, nonzero \n",
    "        in utils.count_total_and_nonzero_params_per_layer(model)\n",
    "    ]\n",
    "    return [np.ones_like(w).flatten() * s for s, w in zip(sparsities, model.get_weights())]\n",
    "\n",
    "def magnitude_features(model: keras.Model) -> List[np.ndarray]:\n",
    "    return [np.abs(w).flatten() for w in model.get_weights()]\n",
    "\n",
    "# Needed to break symmetry so an entire layer is not masked from the beginning\n",
    "def random_features(model: keras.Model) -> List[np.ndarray]:\n",
    "    return [np.random.normal(size=w.shape).flatten() for w in model.get_weights()]\n",
    "\n",
    "def layer_num_features(model: keras.Model) -> List[np.ndarray]:\n",
    "    return [np.ones_like(w).flatten() * i for i, w in enumerate(model.get_weights())]\n",
    "\n",
    "# Individual class representing a NN which maps features about a model and synapses to binary decision \n",
    "# for if it will be masked or not\n",
    "class Individual:\n",
    "    # One shared copy throughout the class (all individuals in population share same original weights)\n",
    "    ARCHITECTURE = None\n",
    "    MODEL = None\n",
    "    DATA = None\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        architecture: arch.Architecture, \n",
    "        features: List[FeatureSelector],\n",
    "        layers: Iterable[Tuple[int, str]],\n",
    "    ):\n",
    "        # If this is the first instance of the class, initialize it with read only copies of data\n",
    "        if self.ARCHITECTURE is None:\n",
    "            self.ARCHITECTURE = architecture\n",
    "            self.MODEL = self.ARCHITECTURE.get_model_constructor()()\n",
    "            self.MODEL.compile(\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "            )\n",
    "            self.DATA = self.ARCHITECTURE.load_data()\n",
    "        \n",
    "        self.features = features\n",
    "        layers = [keras.layers.Input(shape=(len(features),))] \\\n",
    "            + [keras.layers.Dense(size, activation) for size, activation in layers] \\\n",
    "            + [keras.layers.Dense(1, activation='sigmoid')]\n",
    "        self.genome = keras.Sequential(layers)\n",
    "        # Dummy loss- we don't train this with gradient descent\n",
    "        # but use it to map synapse features to probabilities of being masked\n",
    "        self.genome.compile(loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "        self._phenotype = None\n",
    "        self._fitness = None\n",
    "        self.metrics = {}\n",
    "        self.rng = np.random.default_rng()\n",
    "        \n",
    "    @staticmethod\n",
    "    def copy_from(individual: Literal['Individual']) -> Literal['Individual']:\n",
    "        copied = copy.deepcopy(individual)\n",
    "        copied.metrics.clear()\n",
    "        copied._phenotype = None\n",
    "        copied._fitness = None\n",
    "        copied.rng = np.random.default_rng()\n",
    "        return copied\n",
    "    \n",
    "    @property\n",
    "    def phenotype(self) -> List[np.ndarray[bool]]:\n",
    "        \"\"\"\n",
    "        Function which produces a list of boolean Numpy arrays matching the dimensionality\n",
    "        of the architecture it is trained on based on the output of the NN genotype encoding\n",
    "        from the computed features for each synapse.\n",
    "        \"\"\"\n",
    "        if self._phenotype is None:\n",
    "            computed_features = [compute_feature(self.model) for compute_feature in self.features]\n",
    "            masks = []\n",
    "            for layer_features, shape in zip(zip(*computed_features), map(np.shape, self.model.get_weights())):\n",
    "                X = np.array(list(zip(*layer_features)))\n",
    "                mask = (self.genome(X).numpy().reshape(shape) > .5).astype(np.int8)\n",
    "                masks.append(mask)\n",
    "            self._phenotype = masks\n",
    "        return self._phenotype\n",
    "\n",
    "    @property\n",
    "    def architecture(self) -> arch.Architecture | None:\n",
    "        return self.ARCHITECTURE\n",
    "    \n",
    "    @property\n",
    "    def model(self) -> keras.Model | None:\n",
    "        return self.MODEL\n",
    "    \n",
    "    @property\n",
    "    def data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray] | None:\n",
    "        return self.DATA\n",
    "    \n",
    "    @property\n",
    "    def training_data(self) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        if self.data is not None:\n",
    "            X_train, _, Y_train, _ = self.data\n",
    "            return X_train, Y_train\n",
    "        \n",
    "    @property\n",
    "    def test_data(self) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        if self.data is not None:\n",
    "            _, X_test, _, Y_test = self.data\n",
    "            # For now, use a smaller portion for proof of concept\n",
    "            return X_test[:100], Y_test[:100]\n",
    "        \n",
    "    @property\n",
    "    def fitness(self) -> Any:\n",
    "        return self._fitness\n",
    "        \n",
    "    def copy_model(self) -> keras.Model | None:\n",
    "        if self.model is not None:\n",
    "            return copy.deepcopy(self.model)\n",
    "        \n",
    "    @staticmethod\n",
    "    def sparsity(individual: Literal['Individual']) -> float:\n",
    "        total, nonzero = utils.count_total_and_nonzero_params_from_weights(individual.phenotype)\n",
    "        return nonzero / total\n",
    "        \n",
    "    @staticmethod\n",
    "    def eval_accuracy(individual: Literal['Individual'], verbose: int = 0) -> float:\n",
    "        if individual._fitness is None:\n",
    "            model = individual.copy_model()\n",
    "            weights = [w * m for w, m in zip(individual.model.get_weights(), individual.phenotype)]\n",
    "            model.set_weights(weights)\n",
    "            X_test, Y_test = individual.test_data\n",
    "            loss, accuracy = model.evaluate(X_test, Y_test, batch_size=len(X_test), verbose=verbose)\n",
    "            individual._fitness = accuracy\n",
    "        \n",
    "        return individual._fitness\n",
    "    \n",
    "    # Mutation Methods\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_annealing_mutate():\n",
    "        def f(\n",
    "            individual: Literal['Individual'], \n",
    "            rate: Callable[[int], float], \n",
    "            scale: Callable[[int], float]\n",
    "        ):\n",
    "            f.n += 1\n",
    "            Individual.mutate(individual, rate(f.n), scale(f.n))\n",
    "        f.n = 0\n",
    "        return f\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutate(individual: Literal['Individual'], rate: float, scale: float):\n",
    "        weights = individual.genome.get_weights()\n",
    "        for layer_index, layer in enumerate(weights):\n",
    "            perturb_mask = (np.random.uniform(\n",
    "                low=0, \n",
    "                high=1, \n",
    "                size=layer.shape,\n",
    "            ) < rate).astype(np.int8)\n",
    "            perturbations = -np.abs(individual.rng.normal(\n",
    "                loc=0,\n",
    "                scale=scale,\n",
    "                size=layer.shape,\n",
    "            )) * perturb_mask\n",
    "            weights[layer_index] = layer + perturbations\n",
    "        individual.genome.set_weights(weights)\n",
    "        individual._phenotype = None\n",
    "    \n",
    "    # Crossover Methods\n",
    "    \n",
    "    @staticmethod\n",
    "    def crossover(p1: Literal['Individual'], p2: Literal['Individual']) -> Iterable[Literal['Individual']]:\n",
    "        child1, child2 = list(map(Individual.copy_from, (p1, p2)))\n",
    "        p1_weights = p1.genome.get_weights()\n",
    "        p2_weights = p2.genome.get_weights()\n",
    "        c1_weights = child1.genome.get_weights()\n",
    "        c2_weights = child2.genome.get_weights()\n",
    "        for layer_index, weights in enumerate(p1_weights):\n",
    "            # Generate a 0/1 for each row, then extend it across all outgoing synapses\n",
    "            parents = np.repeat(\n",
    "                np.random.randint(low=0, high=2, size=weights.shape[0]),\n",
    "                1 if weights.ndim == 1 else weights.shape[1],\n",
    "                axis=0,\n",
    "            ).reshape((weights.shape))\n",
    "            inverse_parents = np.logical_not(parents).astype(np.int8)\n",
    "            \n",
    "            # This multiplication uses masks to perform selection\n",
    "            c1_weights[layer_index] = p1_weights[layer_index] * parents \\\n",
    "                + p2_weights[layer_index] * inverse_parents\n",
    "            c2_weights[layer_index] = p2_weights[layer_index] * parents \\\n",
    "                + p1_weights[layer_index] * inverse_parents\n",
    "        child1.genome.set_weights(c1_weights)\n",
    "        child2.genome.set_weights(c2_weights)\n",
    "        return child1, child2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1730e491-1016-4074-b406-d886f245c43c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m [layer_sparsity_features, magnitude_features, random_features]\n\u001b[1;32m      3\u001b[0m layers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m individuals \u001b[38;5;241m=\u001b[39m [Individual(a, features, layers) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[86], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m [layer_sparsity_features, magnitude_features, random_features]\n\u001b[1;32m      3\u001b[0m layers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m individuals \u001b[38;5;241m=\u001b[39m [Individual(a, features, layers) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[85], line 49\u001b[0m, in \u001b[0;36mIndividual.__init__\u001b[0;34m(self, architecture, features, layers)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mARCHITECTURE\u001b[38;5;241m.\u001b[39mget_model_constructor()()\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     46\u001b[0m         loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(),\n\u001b[1;32m     47\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mCategoricalAccuracy()],\n\u001b[1;32m     48\u001b[0m     )\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDATA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mARCHITECTURE\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m     52\u001b[0m layers \u001b[38;5;241m=\u001b[39m [keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features),))] \\\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;241m+\u001b[39m [keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(size, activation) \u001b[38;5;28;01mfor\u001b[39;00m size, activation \u001b[38;5;129;01min\u001b[39;00m layers] \\\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;241m+\u001b[39m [keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[0;32m/gpfs1/home/j/b/jbourde2/lottery-tickets/src/harness/architecture.py:299\u001b[0m, in \u001b[0;36mArchitecture.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m/gpfs1/home/j/b/jbourde2/lottery-tickets/src/harness/dataset.py:103\u001b[0m, in \u001b[0;36mDataset.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Method to load the data for a given dataset.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m        Numpy data array extracted from the loader function.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_function()\n",
      "File \u001b[0;32m/gpfs1/home/j/b/jbourde2/lottery-tickets/src/harness/dataset.py:120\u001b[0m, in \u001b[0;36mload_and_process_mnist\u001b[0;34m(flatten)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_process_mnist\u001b[39m(flatten: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39marray]:\n\u001b[1;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    Function to load and preprocess the MNIST dataset.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Source: https://colab.research.google.com/github/maticvl/dataHacker/blob/master/CNN/LeNet_5_TensorFlow_2_0_datahacker.ipynb#scrollTo=UA2ehjxgF7bY\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m        tuple[np.array, np.array, np.array, np.array]: X and Y training and test sets after preprocessing.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     (X_train, Y_train), (X_test, Y_test) \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mmnist\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flatten:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;66;03m# Add a new axis for use in training the model\u001b[39;00m\n\u001b[1;32m    124\u001b[0m         X_train: np\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m X_train[:, :, :, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "File \u001b[0;32m~/.conda/envs/lt/lib/python3.11/site-packages/keras/src/datasets/mnist.py:68\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     60\u001b[0m path \u001b[38;5;241m=\u001b[39m get_file(\n\u001b[1;32m     61\u001b[0m     fname\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m     62\u001b[0m     origin\u001b[38;5;241m=\u001b[39morigin_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     ),\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39mload(path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 68\u001b[0m     x_train, y_train \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_train\u001b[39m\u001b[38;5;124m\"\u001b[39m], f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     69\u001b[0m     x_test, y_test \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_test\u001b[39m\u001b[38;5;124m\"\u001b[39m], f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x_train, y_train), (x_test, y_test)\n",
      "File \u001b[0;32m~/.conda/envs/lt/lib/python3.11/site-packages/numpy/lib/npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mread_array(\u001b[38;5;28mbytes\u001b[39m,\n\u001b[1;32m    257\u001b[0m                              allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_pickle,\n\u001b[1;32m    258\u001b[0m                              pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpickle_kwargs,\n\u001b[1;32m    259\u001b[0m                              max_header_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_header_size)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/.conda/envs/lt/lib/python3.11/site-packages/numpy/lib/format.py:831\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m    830\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 831\u001b[0m             data \u001b[38;5;241m=\u001b[39m _read_bytes(fp, read_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    832\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    833\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/.conda/envs/lt/lib/python3.11/site-packages/numpy/lib/format.py:966\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m         r \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m    967\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[0;32m~/.conda/envs/lt/lib/python3.11/zipfile.py:965\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 965\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read1(n)\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    967\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/.conda/envs/lt/lib/python3.11/zipfile.py:1041\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m   1040\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m-> 1041\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(data, n)\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = arch.Architecture(\"lenet\", \"mnist\")\n",
    "features = [layer_sparsity_features, magnitude_features, random_features]\n",
    "layers = []\n",
    "individuals = [Individual(a, features, layers) for _ in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb59ea93-6736-4bff-83dd-2ee921752bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objectives = [\n",
    "    (Target.MAXIMIZE, 1, Individual.eval_accuracy),\n",
    "    (Target.MINIMIZE, 1, Individual.sparsity),\n",
    "]\n",
    "\n",
    "ranked_fronts = ranked_pareto_fronts(individuals, objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dadc1dba-62a8-4b9e-bc53-dbb2b07b54d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(<__main__.Individual at 0x2b5efea6bc10>, 0.14820749647668047),\n",
       "  (<__main__.Individual at 0x2b5ee510a050>, inf),\n",
       "  (<__main__.Individual at 0x2b5ee4f9b710>, 0.16557405233508377),\n",
       "  (<__main__.Individual at 0x2b5efed7c990>, inf)],\n",
       " [(<__main__.Individual at 0x2b5efec31e10>, inf),\n",
       "  (<__main__.Individual at 0x2b5efee520d0>, inf),\n",
       "  (<__main__.Individual at 0x2b5efebe7910>, 0.26687521336651954),\n",
       "  (<__main__.Individual at 0x2b5efec62390>, 0.22775815099076469)],\n",
       " [(<__main__.Individual at 0x2b5efeaa0610>, inf),\n",
       "  (<__main__.Individual at 0x2b5efecf7ad0>, inf)],\n",
       " [(<__main__.Individual at 0x2b5ee5057610>, 0.19216533096139077),\n",
       "  (<__main__.Individual at 0x2b5ee4c334d0>, 0.49742807965876706),\n",
       "  (<__main__.Individual at 0x2b5efeb47110>, 0.5267518109516668),\n",
       "  (<__main__.Individual at 0x2b5efec88350>, inf),\n",
       "  (<__main__.Individual at 0x2b5efee68350>, inf)],\n",
       " [(<__main__.Individual at 0x2b5ee4d83090>, 0.6670398679923495),\n",
       "  (<__main__.Individual at 0x2b5efeef2f10>, inf),\n",
       "  (<__main__.Individual at 0x2b5ee5020350>, 0.5221087009295666),\n",
       "  (<__main__.Individual at 0x2b5efee1db50>, inf),\n",
       "  (<__main__.Individual at 0x2b5ee5057750>, 0.5151993486044206)],\n",
       " [(<__main__.Individual at 0x2b5ee5007810>, inf),\n",
       "  (<__main__.Individual at 0x2b5efeb166d0>, 0.6124387694303435),\n",
       "  (<__main__.Individual at 0x2b5efeb7bc90>, inf),\n",
       "  (<__main__.Individual at 0x2b5efef2b3d0>, 0.5143179224748642)],\n",
       " [(<__main__.Individual at 0x2b5efedd7cd0>, 0.20268332303676428),\n",
       "  (<__main__.Individual at 0x2b5efeda9150>, inf),\n",
       "  (<__main__.Individual at 0x2b5efeccb390>, 0.3747398850561935),\n",
       "  (<__main__.Individual at 0x2b5efebaed90>, inf)],\n",
       " [(<__main__.Individual at 0x2b5efed48190>, inf),\n",
       "  (<__main__.Individual at 0x2b5efeebef50>, inf)]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    pareto_front_sparsity(front, objectives)\n",
    "    for front in ranked_pareto_fronts(individuals, objectives)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "63e98fe4-3a5a-46e2-a529-b6b903fdd7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GenomeMetricCallback = Callable[[Dict, List[Individual]], Any]\n",
    "ObjectiveRangeFunction = Callable[[List[Individual]], float]\n",
    "\n",
    "class Target(Enum):\n",
    "    MINIMIZE = 0\n",
    "    MAXIMIZE = 1\n",
    "    \n",
    "ObjectiveFunc = Tuple[Target, ObjectiveRangeFunction, FitnessFunction]\n",
    "Objective = Tuple[Target, float, FitnessFunction]\n",
    "    \n",
    "def pareto_dominates(\n",
    "    a: Individual,\n",
    "    b: Individual,\n",
    "    objectives: List[Objective],\n",
    ") -> List[bool]:\n",
    "    for objective, _, fitness in objectives:\n",
    "        a_fitness, b_fitness = map(\n",
    "            lambda x: fitness(x) if objective == Target.MAXIMIZE else -fitness(x), \n",
    "            (a, b)\n",
    "        )\n",
    "        if a_fitness < b_fitness:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def ranked_pareto_fronts(\n",
    "    population: List[Individual],\n",
    "    objectives: List[Objective],\n",
    ") -> List[List[Individual]]:\n",
    "    pop = set(population)\n",
    "    fronts = []\n",
    "    while len(pop) > 0:\n",
    "        next_front = pareto_front(pop, objectives)\n",
    "        fronts.append(next_front)\n",
    "        pop -= set(next_front)\n",
    "    return fronts\n",
    "    \n",
    "\n",
    "def pareto_front(\n",
    "    population: List[Individual],\n",
    "    objectives: List[Objective],\n",
    ") -> List[Individual]:\n",
    "    front = set()\n",
    "    for individual in population:\n",
    "        front.add(individual)\n",
    "        for opponent in front - {individual}:\n",
    "            if pareto_dominates(opponent, individual, objectives):\n",
    "                front.remove(individual)\n",
    "                break\n",
    "            elif pareto_dominates(individual, opponent, objectives):\n",
    "                front.remove(opponent)\n",
    "    return list(front)\n",
    "\n",
    "# Returns front with sparsities assigned\n",
    "# Objectives has objective, range, and fitness function\n",
    "def pareto_front_sparsity(\n",
    "    front: Iterable[Individual],\n",
    "    objectives: List[Objective],\n",
    ") -> Tuple[List[List[Individual]], List[List[float]]]:\n",
    "    individuals = list(front)\n",
    "    indices = list(range(len(individuals)))\n",
    "    sparsities = [0] * len(front)\n",
    "    \n",
    "    for obj, obj_range, fitness in objectives:\n",
    "        reverse = False if obj == Target.MAXIMIZE else False\n",
    "        key = lambda pop_index: fitness(individuals[pop_index])\n",
    "        sorted_indices = list(sorted(indices, key=key, reverse=reverse))\n",
    "        \n",
    "        sparsities[sorted_indices[0]] = np.inf\n",
    "        sparsities[sorted_indices[-1]] = np.inf\n",
    "        for sorted_index, pop_index in enumerate(sorted_indices[1:-1], start=1):\n",
    "            before = fitness(individuals[sorted_indices[sorted_index - 1]])\n",
    "            after = fitness(individuals[sorted_indices[sorted_index + 1]])\n",
    "            sparsities[pop_index] += (after - before) / obj_range\n",
    "            \n",
    "    return individuals, sparsities  \n",
    "\n",
    "# Modified to be able to return multiple winners in each iteration\n",
    "def nondominated_lexicographic_tournament_selection(\n",
    "    ranked_fronts: List[List[Individual]],\n",
    "    sparsities: List[List[float]],\n",
    "    tournament_size: int,\n",
    "    num_winners: int,\n",
    ") -> List[Individual]:\n",
    "    print(f\"Selection with {len(ranked_fronts)} fronts and {len(sparsities)} sparsities\") \n",
    "    # Returns (front, Individual, sparsity)\n",
    "    def sample_individual() -> Tuple[int, Individual, float]:\n",
    "        front_index = np.random.randint(0, len(ranked_fronts))\n",
    "        individual_index = np.random.randint(0, len(ranked_fronts[front_index]))\n",
    "        return front_index, ranked_fronts[front_index][individual_index], sparsities[front_index][individual_index]\n",
    "    \n",
    "    # Minimize front and maximize sparsity/uniqueness- sorting will put these at start of list\n",
    "    def key(tup: Tuple[int, Individual, float]) -> Tuple[int, float]:\n",
    "        front, individual, sparsity = tup\n",
    "        return front, -sparsity\n",
    "    \n",
    "    n_best = sorted(\n",
    "        [sample_individual() for _ in range(tournament_size)], \n",
    "        key=key,\n",
    "    )\n",
    "    \n",
    "    return [individual for _, individual, _ in n_best[:num_winners]]\n",
    "    \n",
    "def nsga2(\n",
    "    num_generations: int,\n",
    "    archive_size: int,\n",
    "    population_size: int,\n",
    "    fronts_to_consider: int,\n",
    "    tournament_size: int,\n",
    "    num_tournament_winners: int,\n",
    "    individual_constructor: Callable[[], Individual],\n",
    "    objectives: List[ObjectiveFunc],\n",
    "    crossover: Crossover | None = None,\n",
    "    mutations: List[Mutation] = [],\n",
    "    genome_metric_callbacks: List[GenomeMetricCallback] = [],\n",
    ") -> Tuple[Dict, Dict, List[Individual]]:\n",
    "    if num_tournament_winners > tournament_size:\n",
    "        raise ValueError(\"Cannot have more tournament winners than participants\")\n",
    "        \n",
    "    # Save data about whole genome and specific objectives over time\n",
    "    genome_metrics = {}\n",
    "    objective_metrics = {\n",
    "        f\"objective_{i}_value\": np.zeros((population_size, num_generations))\n",
    "        for i in range(len(objectives))\n",
    "    }\n",
    "    objective_metrics.update({\n",
    "        f\"objective_{i}_range\": np.zeros(num_generations)\n",
    "        for i in range(len(objectives))\n",
    "    })\n",
    "    \n",
    "    # Create and evaluate the initial population\n",
    "    population = [individual_constructor() for _ in range(population_size)]\n",
    "    archive = []\n",
    "    \n",
    "    for generation_index in range(num_generations):\n",
    "        print(f\"Generation {generation_index + 1}\")\n",
    "        \n",
    "        # Elitist (µ + λ) style strategy\n",
    "        for individual in population:\n",
    "            for _, _, fitness in objectives:\n",
    "                fitness(individual)\n",
    "        population.extend(archive)\n",
    "        \n",
    "        # Create ranked Pareto fronts with their sparsities up to the \n",
    "        # number of fronts specified\n",
    "        concrete_objectives = [(o, r(population), f) for o, r, f in objectives]\n",
    "        ranked_fronts_with_sparsities = [\n",
    "            pareto_front_sparsity(front, concrete_objectives)\n",
    "            for front in ranked_pareto_fronts(population, objectives)[:fronts_to_consider]\n",
    "        ]\n",
    "        # Rebuild the archive from the best, most sparse individuals in lower fronts\n",
    "        archive = []\n",
    "        ranked_fronts = [tup[0] for tup in ranked_fronts_with_sparsities]\n",
    "        ranked_sparsities = [tup[1] for tup in ranked_fronts_with_sparsities]\n",
    "        for front, sparsities in zip(ranked_fronts, ranked_sparsities):\n",
    "            remaining_spots = archive_size - len(archive)\n",
    "            if remaining_spots < len(front):\n",
    "                indices = sorted(\n",
    "                    list(range(len(sparsities))), \n",
    "                    key=lambda i: sparsities[i], \n",
    "                    reverse=True,\n",
    "                )[:remaining_spots]\n",
    "                archive.extend([front[i] for i in indices])\n",
    "                break\n",
    "            else:\n",
    "                archive.extend(front)\n",
    "        \n",
    "        children = []\n",
    "        # Selection, breeding, and mutation\n",
    "        selected = nondominated_lexicographic_tournament_selection(\n",
    "            ranked_fronts, \n",
    "            ranked_sparsities,\n",
    "            tournament_size,\n",
    "            num_tournament_winners,\n",
    "        )\n",
    "        while len(children) < population_size:\n",
    "            parents = np.random.choice(selected, 2)\n",
    "            new_children = crossover(*parents) if crossover else list(map(Individual.copy_from, parents))\n",
    "            \n",
    "            for child in new_children:\n",
    "                for mutation in mutations:\n",
    "                    mutation(child)\n",
    "            children.extend(new_children)\n",
    "        population = children[:population_size]\n",
    "        \n",
    "        # Callbacks to gather data during training process\n",
    "        for callback in genome_metric_callbacks:\n",
    "            callback(genome_metrics, population)\n",
    "        for obj_index, (_, range_func, fitness_func) in enumerate(objectives):\n",
    "            objective_metrics[f\"objective_{obj_index}_range\"][generation_index] = range_func(population)\n",
    "            for pop_index, individual in enumerate(population):\n",
    "                objective_metrics[f\"objective_{obj_index}_value\"][pop_index, generation_index] = fitness_func(individual)\n",
    "        \n",
    "    return genome_metrics, objective_metrics, archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a8b30d28-45e5-461a-ae3e-4c715d8fb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sparsity(data: Dict, population: List[Individual]):\n",
    "    overall_key = \"average_global_sparsity\"\n",
    "    layer_key = \"average_layer_sparsity\"\n",
    "    for key in [overall_key, layer_key]:\n",
    "        if data.get(key) is None:\n",
    "            data[key] = []\n",
    "    \n",
    "    global_counts = list(map(lambda i: utils.count_total_and_nonzero_params_from_weights(i.phenotype), population))\n",
    "    overall_sparsities = [nonzero / total for total, nonzero in global_counts]\n",
    "    layer_counts = list(map(lambda i: utils.count_total_and_nonzero_params_per_layer_from_weights(i.phenotype), population))\n",
    "    layer_sparsities = [[nonzero / total for total, nonzero in layer] for layer in layer_counts]\n",
    "    \n",
    "    data[overall_key].append(np.mean(overall_sparsities))\n",
    "    data[layer_key].append(np.mean(layer_sparsities, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7f9c5a48-06c6-4c70-a5fc-1e285c39b820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Generation 1\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 2\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Run 2\n",
      "Generation 1\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 2\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Run 3\n",
      "Generation 1\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 2\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Run 4\n",
      "Generation 1\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 2\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Run 5\n",
      "Generation 1\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 2\n",
      "Selection with 2 fronts and 2 sparsities\n"
     ]
    }
   ],
   "source": [
    "num_runs = 5\n",
    "\n",
    "architecture = arch.Architecture('lenet', 'mnist')\n",
    "features = [layer_sparsity_features, magnitude_features]\n",
    "layers = [(4, 'relu')]\n",
    "\n",
    "individual_constructor = functools.partial(\n",
    "    Individual, \n",
    "    architecture=architecture,\n",
    "    features=features,\n",
    "    layers=layers,\n",
    ")\n",
    "\n",
    "objectives = [\n",
    "    (Target.MAXIMIZE, lambda x: 1, Individual.eval_accuracy),\n",
    "    (Target.MINIMIZE, lambda x: 1, Individual.sparsity),\n",
    "]\n",
    "\n",
    "rate_func = lambda n: 0.5\n",
    "scale_func = lambda n: 1 / np.sqrt(n + 1)\n",
    "mutations = [\n",
    "    functools.partial(Individual.get_annealing_mutate(), rate=rate_func, scale=scale_func)\n",
    "]\n",
    "    \n",
    "genome_metric_callbacks = [\n",
    "    average_sparsity\n",
    "]\n",
    "kwargs = {\n",
    "    \"num_generations\": 2,\n",
    "    \"archive_size\": 10,\n",
    "    \"population_size\": 10,\n",
    "    \"fronts_to_consider\": 2,\n",
    "    \"tournament_size\": 4,\n",
    "    \"num_tournament_winners\": 2,\n",
    "    \"individual_constructor\": individual_constructor,\n",
    "    \"objectives\": objectives,\n",
    "    \"mutations\": mutations,\n",
    "    \"crossover\": Individual.crossover,\n",
    "    \"genome_metric_callbacks\": genome_metric_callbacks,\n",
    "}\n",
    "\n",
    "all_genome_metrics = []\n",
    "all_objective_metrics = []\n",
    "all_archives = []\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}\")\n",
    "    genome_metrics, objective_metrics, archive = nsga2(**kwargs)\n",
    "    all_genome_metrics.append(genome_metrics)\n",
    "    all_objective_metrics.append(objective_metrics)\n",
    "    all_archives.append(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6c4f1674-4cc6-4158-b5ff-d9a2c3c037eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'objective_0_value': array([[0.16, 0.08],\n",
      "       [0.08, 0.16],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.08],\n",
      "       [0.14, 0.16],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.08],\n",
      "       [0.07, 0.08],\n",
      "       [0.16, 0.08],\n",
      "       [0.16, 0.08]]), 'objective_1_value': array([[0.99846217, 0.        ],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.98553318, 0.99846217],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.99846217, 0.        ],\n",
      "       [0.99846217, 0.        ],\n",
      "       [0.99846217, 0.        ]]), 'objective_0_range': array([1., 1.]), 'objective_1_range': array([1., 1.])}, {'objective_0_value': array([[0.08, 0.12],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.14]]), 'objective_1_value': array([[0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.9983384 ]]), 'objective_0_range': array([1., 1.]), 'objective_1_range': array([1., 1.])}, {'objective_0_value': array([[0.08      , 0.11      ],\n",
      "       [0.08      , 0.15000001],\n",
      "       [0.08      , 0.11      ],\n",
      "       [0.08      , 0.08      ],\n",
      "       [0.08      , 0.08      ],\n",
      "       [0.08      , 0.08      ],\n",
      "       [0.08      , 0.11      ],\n",
      "       [0.08      , 0.08      ],\n",
      "       [0.08      , 0.08      ],\n",
      "       [0.08      , 0.08      ]]), 'objective_1_value': array([[0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.0213908 ]]), 'objective_0_range': array([1., 1.]), 'objective_1_range': array([1., 1.])}, {'objective_0_value': array([[0.08      , 0.08      ],\n",
      "       [0.12      , 0.08      ],\n",
      "       [0.11      , 0.08      ],\n",
      "       [0.11      , 0.1       ],\n",
      "       [0.11      , 0.08      ],\n",
      "       [0.08      , 0.08      ],\n",
      "       [0.08      , 0.08      ],\n",
      "       [0.08      , 0.18000001],\n",
      "       [0.08      , 0.08      ],\n",
      "       [0.11      , 0.1       ]]), 'objective_1_value': array([[0.        , 0.        ],\n",
      "       [0.99846217, 0.        ],\n",
      "       [0.99846217, 0.        ],\n",
      "       [0.99846217, 0.99846217],\n",
      "       [0.99846217, 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.61583211],\n",
      "       [0.        , 0.        ],\n",
      "       [0.99846217, 0.99846217]]), 'objective_0_range': array([1., 1.]), 'objective_1_range': array([1., 1.])}, {'objective_0_value': array([[0.12, 0.08],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.12],\n",
      "       [0.08, 0.08],\n",
      "       [0.08, 0.08]]), 'objective_1_value': array([[0.99846217, 0.        ],\n",
      "       [0.00816173, 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.04302914],\n",
      "       [0.        , 0.99846217],\n",
      "       [0.        , 0.        ],\n",
      "       [0.        , 0.        ]]), 'objective_0_range': array([1., 1.]), 'objective_1_range': array([1., 1.])}]\n"
     ]
    }
   ],
   "source": [
    "print(all_objective_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd3d4b-2272-4b2a-8a11-fb003a88d9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Generation 1\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 2\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 3\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 4\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 5\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 6\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 7\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 8\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 9\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 10\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Run 2\n",
      "Generation 1\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 2\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 3\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 4\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 5\n",
      "Selection with 2 fronts and 2 sparsities\n",
      "Generation 6\n",
      "Selection with 2 fronts and 2 sparsities\n"
     ]
    }
   ],
   "source": [
    "num_runs = 5\n",
    "\n",
    "architecture = arch.Architecture('lenet', 'mnist')\n",
    "features = [layer_sparsity_features, magnitude_features]\n",
    "layers = [(4, 'relu')]\n",
    "layers = []\n",
    "\n",
    "individual_constructor = functools.partial(\n",
    "    Individual, \n",
    "    architecture=architecture,\n",
    "    features=features,\n",
    "    layers=layers,\n",
    ")\n",
    "\n",
    "objectives = [\n",
    "    (Target.MAXIMIZE, lambda x: 1, Individual.eval_accuracy),\n",
    "    (Target.MINIMIZE, lambda x: 1, Individual.sparsity),\n",
    "]\n",
    "\n",
    "rate_func = lambda n: 0.5\n",
    "scale_func = lambda n: 1 / np.sqrt(n + 1)\n",
    "mutations = [\n",
    "    functools.partial(Individual.get_annealing_mutate(), rate=rate_func, scale=scale_func)\n",
    "]\n",
    "    \n",
    "genome_metric_callbacks = [\n",
    "    average_sparsity\n",
    "]\n",
    "kwargs = {\n",
    "    \"num_generations\": 10,\n",
    "    \"archive_size\": 10,\n",
    "    \"population_size\": 10,\n",
    "    \"fronts_to_consider\": 2,\n",
    "    \"tournament_size\": 4,\n",
    "    \"num_tournament_winners\": 2,\n",
    "    \"individual_constructor\": individual_constructor,\n",
    "    \"objectives\": objectives,\n",
    "    \"mutations\": mutations,\n",
    "    \"crossover\": Individual.crossover,\n",
    "    \"genome_metric_callbacks\": genome_metric_callbacks,\n",
    "}\n",
    "\n",
    "all_genome_metrics2 = []\n",
    "all_objective_metrics2 = []\n",
    "all_archives2 = []\n",
    "for run in range(num_runs):\n",
    "    print(f\"Run {run + 1}\")\n",
    "    genome_metrics, objective_metrics, archive = nsga2(**kwargs)\n",
    "    all_genome_metrics2.append(genome_metrics)\n",
    "    all_objective_metrics2.append(objective_metrics)\n",
    "    all_archives2.append(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544a571-2169-4884-b63b-d053093f1d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lt",
   "language": "python",
   "name": "lt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
