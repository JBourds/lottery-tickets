{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import functools\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.sparsity.keras import ConstantSparsity, PolynomialDecay, prune_low_magnitude\n",
    "\n",
    "from src.harness.constants import Constants as C\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.model import save_model, load_model\n",
    "from src.harness.pruning import create_pruning_callbacks, create_pruning_parameters\n",
    "from src.harness.training import train, TrainingRound\n",
    "from src.lottery_ticket.foundations import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_and_process_mnist()\n",
    "\n",
    "num_epochs: int = 10\n",
    "batch_size: int = 60\n",
    "input_shape: tuple = X_train[0].shape\n",
    "num_classes: int = 10\n",
    "\n",
    "num_train_samples: int = X_train.shape[0]\n",
    "\n",
    "end_step: int = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * num_epochs\n",
    "\n",
    "pruning_parameters: dict = create_pruning_parameters(0.01, 0, end_step, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed: int):\n",
    "    # Set seeds for reproducability\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "# Create a model with the same architecture using all Keras components to check its accuracy with the same parameters\n",
    "def create_lenet_300_100(input_shape: tuple[int, ...], num_classes: int, optimizer = C.OPTIMIZER) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Function for creating LeNet-300-100 model.\n",
    "\n",
    "    :param input_shape: Expected input shape for images.\n",
    "    :param num_classes: Number of potential classes to predict.\n",
    "    :param optimizer:   Optimizer to use for training.\n",
    "\n",
    "    :returns: Compiled LeNet-300-100 architecture.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(input_shape),\n",
    "        Dense(300, activation='relu', kernel_initializer=tf.initializers.GlorotUniform()),\n",
    "        Dense(100, activation='relu', kernel_initializer=tf.initializers.GlorotUniform()),\n",
    "        Dense(num_classes, activation='softmax', kernel_initializer=tf.initializers.GlorotUniform()),\n",
    "    ], name=\"LeNet-300-100\")\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.CategoricalCrossentropy(), \n",
    "        optimizer=optimizer(), \n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def global_pruning_nn(input_shape: tuple[int, ...], num_classes: int, pruning_params: dict) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Function to define the architecture of a neural network model\n",
    "    following 300 100 architecture for MNIST dataset and using\n",
    "    provided parameter which are used to prune the model.\n",
    "    \n",
    "    :param\n",
    "    \"\"\"\n",
    "    \n",
    "    model = sparsity.prune_low_magnitude(Sequential([\n",
    "        Input(input_shape),\n",
    "        Dense(300, activation='relu', kernel_initializer=tf.initializers.GlorotUniform()),\n",
    "        Dense(100, activation='relu', kernel_initializer=tf.initializers.GlorotUniform()),\n",
    "        Dense(num_classes, activation='softmax', kernel_initializer=tf.initializers.GlorotUniform())\n",
    "    ], name=\"Pruned_LeNet-300-100\"), **pruning_parameters)\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.CategoricalCrossentropy(), \n",
    "        optimizer=C.OPTIMIZER(), \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "set_seed(0)\n",
    "model = global_pruning_nn(input_shape, num_classes, pruning_parameters)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.harness.pruning import get_layer_weight_counts, get_pruning_percents\n",
    "\n",
    "def count_params(model: keras.Model) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Helper function to count the total number of parameters and number of nonzero parameters.\n",
    "    \"\"\"\n",
    "    weights = model.get_weights()\n",
    "    total_weights = sum(tf.size(w).numpy() for w in weights)  # Calculate total weights\n",
    "    nonzero_weights = sum(tf.math.count_nonzero(w).numpy() for w in weights)  # Calculate non-zero weights\n",
    "    return total_weights, nonzero_weights\n",
    "\n",
    "def initialize_mask_model(model: keras.Model):\n",
    "    \"\"\"\n",
    "    Function which performs layerwise initialization of a keras model's weights to all 1s\n",
    "    as an initialization step for masking.\n",
    "\n",
    "    :param model: Keras model being set to all 1s.\n",
    "    \"\"\"\n",
    "    for weights in model.trainable_weights:\n",
    "        weights.assign(\n",
    "            tf.ones_like(\n",
    "                input=weights,\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Figure out the pruning percents for each round depending on target sparsity and number of pruning rounds\n",
    "target_sparsity: float = 0.01\n",
    "layer_weight_counts: list = get_layer_weight_counts(model)\n",
    "pruning_percents: list[np.array] = get_pruning_percents(layer_weight_counts, .2 ,target_sparsity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.harness.experiment import ExperimentData\n",
    "from src.harness.training import test_step, train, train_one_step, training_loop, TrainingRound\n",
    "\n",
    "# Save and load model\n",
    "mask_model = global_pruning_nn(input_shape, num_classes, pruning_parameters)\n",
    "model: keras.Model = sparsity.strip_pruning(model)\n",
    "\n",
    "print(f'Nonzero parameters before training:')\n",
    "print(count_params(model)[1])\n",
    "\n",
    "experiment_data: ExperimentData = ExperimentData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train one step\n",
    "# Define your loss function outside of the function\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Define your optimizer outside of the function\n",
    "optimizer = C.OPTIMIZER()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the function\n",
    "train_one_step(model, mask_model, X_train, Y_train, loss_fn, optimizer)\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nonzero parameters after training:')\n",
    "print(count_params(model)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
