{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from src.harness import constants as C\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.experiment import experiment\n",
    "from src.harness.model import create_model, LeNet300, load_model\n",
    "from src.harness.pruning import prune_by_percent\n",
    "from src.harness.training import train, TrainingRound\n",
    "from src.lottery_ticket.foundations import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4215 - accuracy: 0.8827 - val_loss: 0.2239 - val_accuracy: 0.9339\n",
      "Epoch 2/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1969 - accuracy: 0.9433 - val_loss: 0.1752 - val_accuracy: 0.9487\n",
      "Epoch 3/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1433 - accuracy: 0.9585 - val_loss: 0.1372 - val_accuracy: 0.9603\n",
      "Epoch 4/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.9670 - val_loss: 0.1170 - val_accuracy: 0.9640\n",
      "Epoch 5/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0930 - accuracy: 0.9732 - val_loss: 0.1026 - val_accuracy: 0.9694\n",
      "Epoch 6/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0773 - accuracy: 0.9772 - val_loss: 0.0964 - val_accuracy: 0.9693\n",
      "Epoch 7/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0660 - accuracy: 0.9813 - val_loss: 0.0867 - val_accuracy: 0.9724\n",
      "Epoch 8/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0568 - accuracy: 0.9844 - val_loss: 0.0796 - val_accuracy: 0.9750\n",
      "Epoch 9/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0494 - accuracy: 0.9862 - val_loss: 0.0738 - val_accuracy: 0.9766\n",
      "Epoch 10/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0433 - accuracy: 0.9883 - val_loss: 0.0705 - val_accuracy: 0.9783\n",
      "Epoch 11/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9900 - val_loss: 0.0689 - val_accuracy: 0.9785\n",
      "Epoch 12/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0331 - accuracy: 0.9911 - val_loss: 0.0680 - val_accuracy: 0.9778\n",
      "Epoch 13/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0293 - accuracy: 0.9924 - val_loss: 0.0654 - val_accuracy: 0.9788\n",
      "Epoch 14/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0255 - accuracy: 0.9937 - val_loss: 0.0772 - val_accuracy: 0.9767\n",
      "Epoch 15/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0227 - accuracy: 0.9947 - val_loss: 0.0667 - val_accuracy: 0.9781\n",
      "Epoch 16/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0199 - accuracy: 0.9957 - val_loss: 0.0659 - val_accuracy: 0.9793\n",
      "Epoch 17/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9965 - val_loss: 0.0666 - val_accuracy: 0.9789\n",
      "Epoch 18/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0154 - accuracy: 0.9972 - val_loss: 0.0642 - val_accuracy: 0.9806\n",
      "Epoch 19/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0136 - accuracy: 0.9978 - val_loss: 0.0660 - val_accuracy: 0.9795\n",
      "Epoch 20/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0117 - accuracy: 0.9982 - val_loss: 0.0693 - val_accuracy: 0.9789\n",
      "Epoch 21/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0629 - val_accuracy: 0.9808\n",
      "Epoch 22/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0093 - accuracy: 0.9987 - val_loss: 0.0636 - val_accuracy: 0.9815\n",
      "Epoch 23/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0083 - accuracy: 0.9991 - val_loss: 0.0666 - val_accuracy: 0.9811\n",
      "Epoch 24/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0073 - accuracy: 0.9993 - val_loss: 0.0657 - val_accuracy: 0.9814\n",
      "Epoch 25/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9995 - val_loss: 0.0645 - val_accuracy: 0.9817\n",
      "Epoch 26/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.0629 - val_accuracy: 0.9821\n",
      "Epoch 27/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 0.0656 - val_accuracy: 0.9819\n",
      "Epoch 28/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 0.0670 - val_accuracy: 0.9819\n",
      "Epoch 29/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9997 - val_loss: 0.0654 - val_accuracy: 0.9818\n",
      "Epoch 30/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9998 - val_loss: 0.0668 - val_accuracy: 0.9813\n",
      "Epoch 31/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0037 - accuracy: 0.9998 - val_loss: 0.0660 - val_accuracy: 0.9821\n",
      "Epoch 32/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0033 - accuracy: 0.9999 - val_loss: 0.0671 - val_accuracy: 0.9815\n",
      "Epoch 33/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0031 - accuracy: 0.9998 - val_loss: 0.0673 - val_accuracy: 0.9817\n",
      "Epoch 34/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9820\n",
      "Epoch 35/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.0683 - val_accuracy: 0.9818\n",
      "Epoch 36/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 0.9819\n",
      "Epoch 37/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9821\n",
      "Epoch 38/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9820\n",
      "Epoch 39/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9820\n",
      "Epoch 40/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9825\n",
      "Epoch 41/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0692 - val_accuracy: 0.9819\n",
      "Epoch 42/60\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0695 - val_accuracy: 0.9823\n",
      "Epoch 43/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0695 - val_accuracy: 0.9822\n",
      "Epoch 44/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9826\n",
      "Epoch 45/60\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 0.9827\n",
      "Epoch 46/60\n",
      "211/469 [============>.................] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m keras_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(), optimizer\u001b[38;5;241m=\u001b[39mC\u001b[38;5;241m.\u001b[39mOPTIMIZER(), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m keras_model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train,\n\u001b[1;32m     21\u001b[0m                              batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     22\u001b[0m                              epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[1;32m     23\u001b[0m                              verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     24\u001b[0m                              validation_data\u001b[38;5;241m=\u001b[39m(X_test, Y_test))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     27\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m keras_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, Y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_and_process_mnist()\n",
    "\n",
    "# Create a model with the same architecture using all Keras components to check its accuracy with the same parameters\n",
    "def create_lenet_300_100(input_shape: tuple[int, ...], num_classes: int):\n",
    "    \"\"\"\n",
    "    Simple hardcoded class definition for creating the sequential Keras equivalent to LeNet-300-100.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential(name=\"LeNet-300-100\")\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "    model.add(keras.layers.Dense(300, activation='relu'))\n",
    "    model.add(keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "num_classes: int = 10\n",
    "input_shape: tuple[int, ...] = X_train[0].shape\n",
    "keras_model: keras.Model = create_lenet_300_100(input_shape, num_classes)\n",
    "keras_model.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=C.OPTIMIZER(), metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = keras_model.fit(X_train, Y_train,\n",
    "                             batch_size=128,\n",
    "                             epochs=60,\n",
    "                             verbose=1,\n",
    "                             validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = keras_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING_RANDOM_SEED: int = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = create_model(TESTING_RANDOM_SEED, X_train, Y_train)\n",
    "initial_weights1: dict[str: np.array] = model.get_current_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Loss: 2.3420140743255615\n",
      "Iteration 2/10, Loss: 2.257554769515991\n",
      "Iteration 3/10, Loss: 2.1908860206604004\n",
      "Iteration 4/10, Loss: 2.1296677589416504\n",
      "Iteration 5/10, Loss: 2.0702004432678223\n",
      "Iteration 6/10, Loss: 2.0107617378234863\n",
      "Iteration 7/10, Loss: 1.950530767440796\n",
      "Iteration 8/10, Loss: 1.8891801834106445\n",
      "Iteration 9/10, Loss: 1.8266687393188477\n",
      "Iteration 10/10, Loss: 1.763145089149475\n"
     ]
    }
   ],
   "source": [
    "# Test Training a model\n",
    "optimizer = C.OPTIMIZER()\n",
    "make_dataset: callable = load_and_process_mnist\n",
    "round: TrainingRound = train(make_dataset, model, TESTING_RANDOM_SEED, optimizer, C.TEST_TRAINING_ITERATIONS)\n",
    "\n",
    "for i in range(3):\n",
    "    key: str = f'layer{i}'\n",
    "    # Sanity check that the initial weights are correct\n",
    "    assert np.array_equal(initial_weights1[key], round.initial_weights[key])\n",
    "    # Verify the final weights are different from initial weights\n",
    "    assert not np.array_equal(round.initial_weights[key], round.final_weights[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Loss: 2.280717134475708\n",
      "Iteration 2/10, Loss: 2.204838991165161\n",
      "Iteration 3/10, Loss: 2.1418585777282715\n",
      "Iteration 4/10, Loss: 2.0832467079162598\n",
      "Iteration 5/10, Loss: 2.026040554046631\n",
      "Iteration 6/10, Loss: 1.9689557552337646\n",
      "Iteration 7/10, Loss: 1.9113845825195312\n",
      "Iteration 8/10, Loss: 1.8531246185302734\n",
      "Iteration 9/10, Loss: 1.794109582901001\n",
      "Iteration 10/10, Loss: 1.7345093488693237\n"
     ]
    }
   ],
   "source": [
    "# Try creating a model from the initial weights as a preset\n",
    "make_model: callable = functools.partial(LeNet300, TESTING_RANDOM_SEED)\n",
    "percents: dict[str: float] = {key: 0.5 for key in round.final_weights}\n",
    "starting_masks: dict[str: np.ndarray] = {f'layer{i}': np.ones(round.initial_weights[f'layer{i}'].shape) for i in range(3)}\n",
    "\n",
    "# Create pruned masks\n",
    "masks = prune_by_percent(C.TEST_PRUNING_PERCENTS, starting_masks, round.final_weights)\n",
    "\n",
    "# This is transforming initial weights into a tensor when it should be Numpy?\n",
    "# Passing in masks is the issue\n",
    "model = make_model(X_train, Y_train, presets=round.initial_weights, masks=masks)\n",
    "\n",
    "# Sanity check that the weights are correctly loaded and are masked off accordingly\n",
    "for i in range(3):\n",
    "    key = f'layer{i}'\n",
    "    layer_weights = model.get_current_weights()[key]\n",
    "    layer_mask = model.masks[key]\n",
    "    expected_weights: np.ndarray = round.initial_weights[key] * layer_mask\n",
    "    assert np.array_equal(expected_weights, layer_weights), f'Expected {expected_weights} but received {layer_weights}'\n",
    "    assert np.array_equal(model.masks[key], masks[key])\n",
    "\n",
    "# Save the tensors storing the actual weight values (these include the masked off weights)\n",
    "pretrained_weights = model.weights.copy()\n",
    "\n",
    "# Try doing a a simulated round of pruning\n",
    "round2: TrainingRound = train(make_dataset, model, 1, optimizer, C.TEST_TRAINING_ITERATIONS)\n",
    "\n",
    "# Make sure the masked off weights don't receive any updates in the actual tensorflow tensor\n",
    "trained_weights = model.weights\n",
    "\n",
    "# Compare the masked weights before and after training\n",
    "for i in range(3):\n",
    "    key = f'layer{i}'\n",
    "    pretrained_layer_weights = pretrained_weights[key]\n",
    "    trained_layer_weights = trained_weights[key]\n",
    "    \n",
    "    # Invert the mask, to only look at the weights which WERE masked off\n",
    "    inverted_mask: np.ndarray = 1 - masks[key]\n",
    "    masked_pretrained_weights = pretrained_layer_weights * inverted_mask\n",
    "    masked_trained_weights = trained_layer_weights * inverted_mask\n",
    "    \n",
    "    # Assert that the masked weights remain unchanged after training\n",
    "    assert np.array_equal(masked_pretrained_weights, masked_trained_weights), f'Weights changed after training for layer {key}'\n",
    "    assert not np.array_equal(round2.initial_weights, round.final_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INDEX: int = 0\n",
    "PRUNING_STEP: int = 5\n",
    "\n",
    "# Get initial weights\n",
    "dir: str = f'models/model_{MODEL_INDEX}/initial/'\n",
    "weight_files = [paths.weights(dir) + f'/layer{i}.npy' for i in range(3)]\n",
    "mask_files = [paths.masks(dir) + f'/layer{i}.npy' for i in range(3)]\n",
    "\n",
    "layer_weights = {f'layer{i}': np.load(layer) for i, layer in enumerate(weight_files)}\n",
    "masks = {f'layer{i}': np.load(layer) for i, layer in enumerate(mask_files)}\n",
    "# Test loading a model\n",
    "model: LeNet300 = load_model(MODEL_INDEX, PRUNING_STEP, True)\n",
    "\n",
    "for i in range(3):\n",
    "    key: str = f'layer{i}'\n",
    "    # Verify all the layer weights match\n",
    "    assert np.array_equal(model.weights[key], layer_weights[key])\n",
    "    # Verify all masks are 1s\n",
    "    assert np.sum(masks[key]) == masks[key].size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layer0', (784, 300)), ('layer1', (300, 100)), ('layer2', (100, 10))]\n"
     ]
    }
   ],
   "source": [
    "# Test pruning\n",
    "print([(key, layer.shape) for key, layer in layer_weights.items()])\n",
    "percents: dict[str: float] = {key: 0.5 for key in layer_weights}\n",
    "new_masks: dict[str, np.array] = prune_by_percent(percents, masks, layer_weights)\n",
    "for key in new_masks:\n",
    "    new_mask: np.array = new_masks[key]\n",
    "    old_mask: np.array = masks[key]\n",
    "    assert (old_mask.sum() / 2 - new_mask.sum()) <= 1, f'Doesn\\'t match for key {key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Step 0\n",
      "Iteration 1/10, Loss: 2.3612027168273926\n",
      "Iteration 2/10, Loss: 2.258298635482788\n",
      "Iteration 3/10, Loss: 2.1781187057495117\n",
      "Iteration 4/10, Loss: 2.1064586639404297\n",
      "Iteration 5/10, Loss: 2.037797689437866\n",
      "Iteration 6/10, Loss: 1.9696766138076782\n",
      "Iteration 7/10, Loss: 1.901034951210022\n",
      "Iteration 8/10, Loss: 1.8315355777740479\n",
      "Iteration 9/10, Loss: 1.7611823081970215\n",
      "Iteration 10/10, Loss: 1.6903091669082642\n",
      "Pruning Step 1\n",
      "Iteration 1/10, Loss: 2.2866768836975098\n",
      "Iteration 2/10, Loss: 2.1959917545318604\n",
      "Iteration 3/10, Loss: 2.1210360527038574\n",
      "Iteration 4/10, Loss: 2.052361011505127\n",
      "Iteration 5/10, Loss: 1.9861609935760498\n",
      "Iteration 6/10, Loss: 1.920701026916504\n",
      "Iteration 7/10, Loss: 1.855150580406189\n",
      "Iteration 8/10, Loss: 1.7892708778381348\n",
      "Iteration 9/10, Loss: 1.7231773138046265\n",
      "Iteration 10/10, Loss: 1.6571670770645142\n",
      "Pruning Step 2\n",
      "Iteration 1/10, Loss: 2.2150192260742188\n",
      "Iteration 2/10, Loss: 2.132519245147705\n",
      "Iteration 3/10, Loss: 2.0611836910247803\n",
      "Iteration 4/10, Loss: 1.9947313070297241\n",
      "Iteration 5/10, Loss: 1.9305822849273682\n",
      "Iteration 6/10, Loss: 1.8674631118774414\n",
      "Iteration 7/10, Loss: 1.804727554321289\n",
      "Iteration 8/10, Loss: 1.742276906967163\n",
      "Iteration 9/10, Loss: 1.680162787437439\n",
      "Iteration 10/10, Loss: 1.6185871362686157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<src.harness.experiment.ExperimentData at 0x1563892d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test experiment\n",
    "make_dataset: callable = load_and_process_mnist\n",
    "# Make partial function application giving the model its random seed\n",
    "make_model: callable = functools.partial(LeNet300, 999)\n",
    "train_model: callable = functools.partial(train, iterations=C.TEST_TRAINING_ITERATIONS)\n",
    "prune_masks: callable = functools.partial(prune_by_percent, C.PRUNING_PERCENTS)\n",
    "experiment(make_dataset, make_model, train_model, prune_masks, C.TEST_PRUNING_STEPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
