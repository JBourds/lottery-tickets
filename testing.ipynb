{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.model import create_model, LeNet300, load_model\n",
    "from src.harness.pruning import prune_by_percent\n",
    "from src.harness.training import train\n",
    "from src.lottery_ticket.foundations import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/3, Loss: 2.3580663204193115\n",
      "Iteration 2/3, Loss: 2.349560499191284\n",
      "Iteration 3/3, Loss: 2.341191291809082\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_and_process_mnist()\n",
    "model = create_model(0, X_train, Y_train)\n",
    "initial_weights1: dict[str: np.array] = model.get_current_weights()\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(.01)\n",
    "initial_weights2, final_weights = train(load_and_process_mnist, model, optimizer, 3)\n",
    "\n",
    "# Sanity check that the initial weights are correct\n",
    "for i in range(3):\n",
    "    key: str = f'layer{i}'\n",
    "    assert np.array_equal(initial_weights1[key], initial_weights2[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INDEX: int = 0\n",
    "# Get initial weights\n",
    "dir: str = f'models/model_{MODEL_INDEX}/initial/'\n",
    "weight_files = [paths.weights(dir) + f'/layer{i}.npy' for i in range(3)]\n",
    "mask_files = [paths.masks(dir) + f'/layer{i}.npy' for i in range(3)]\n",
    "\n",
    "layer_weights = {f'layer{i}': np.load(layer) for i, layer in enumerate(weight_files)}\n",
    "masks = {f'layer{i}': np.load(layer) for i, layer in enumerate(mask_files)}\n",
    "# Test loading a model\n",
    "model: LeNet300 = load_model(MODEL_INDEX, 0, True)\n",
    "\n",
    "for i in range(3):\n",
    "    key: str = f'layer{i}'\n",
    "    # Verify all the layer weights match\n",
    "    assert np.array_equal(model.weights[key], layer_weights[key])\n",
    "    # Verify all masks are 1s\n",
    "    assert np.sum(masks[key]) == masks[key].size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layer0', (784, 300)), ('layer1', (300, 100)), ('layer2', (100, 10))]\n"
     ]
    }
   ],
   "source": [
    "# Test pruning\n",
    "print([(key, layer.shape) for key, layer in layer_weights.items()])\n",
    "percents: dict[str: float] = {key: 0.5 for key in layer_weights}\n",
    "new_masks: dict[str, np.array] = prune_by_percent(percents, masks, layer_weights)\n",
    "for key in new_masks:\n",
    "    new_mask: np.array = new_masks[key]\n",
    "    old_mask: np.array = masks[key]\n",
    "    assert (old_mask.sum() / 2 - new_mask.sum()) <= 1, f'Doesn\\'t match for key {key}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
