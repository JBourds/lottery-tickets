{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import copy\n",
    "import functools\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from src.harness import constants as C\n",
    "from src.harness import dataset as ds\n",
    "from src.harness import history\n",
    "from src.harness import experiment\n",
    "from src.harness import mixins\n",
    "from src.harness import model as mod\n",
    "from src.harness import paths\n",
    "from src.harness import pruning\n",
    "from src.harness import rewind\n",
    "from src.harness import training as train\n",
    "from src.harness import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ds)\n",
    "reload(mod)\n",
    "reload(pruning)\n",
    "\n",
    "# Select the dataset\n",
    "mnist_dataset: ds.Dataset = ds.Dataset(ds.Datasets.MNIST)\n",
    "X_train, X_test, Y_train, Y_test = mnist_dataset.load()\n",
    "input_shape: tuple = mnist_dataset.input_shape\n",
    "num_classes: int = mnist_dataset.num_classes\n",
    "\n",
    "print(f'Input Shape: {input_shape}')\n",
    "print(f'Num Classes: {num_classes}')\n",
    "print(f'X_train Shape: {X_train.shape}, Y_train Shape: {Y_train.shape}')\n",
    "print(f'X_test Shape: {X_test.shape}, Y_test Shape: {Y_test.shape}')\n",
    "\n",
    "num_epochs: int = 10\n",
    "batch_size: int = len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ds)\n",
    "reload(mod)\n",
    "reload(utils)\n",
    "\n",
    "# Create a model with the same architecture using all Keras components to check its accuracy with the same parameters\n",
    "utils.set_seed(0)\n",
    "make_lenet: callable = functools.partial(mod.create_lenet_300_100, input_shape, num_classes)\n",
    "\n",
    "original_model: keras.Model = make_lenet()\n",
    "# original_model.summary()\n",
    "# original_model.trainable_variables\n",
    "\n",
    "original_mask_model: keras.Model = mod.create_masked_nn(make_lenet)\n",
    "original_mask_model.summary()\n",
    "# original_mask_model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Loss Function and Accuracy Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(C)\n",
    "reload(train)\n",
    "\n",
    "# Use the original model as a reference\n",
    "loss_fn: tf.keras.losses.Loss = C.LOSS_FUNCTION()\n",
    "accuracy_metric: tf.keras.metrics.Metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "test_loss, test_accuracy = train.test_step(original_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(C)\n",
    "reload(train)\n",
    "\n",
    "# Test single step of training\n",
    "\n",
    "# Define the optimizer outside of the function\n",
    "optimizer = C.OPTIMIZER()\n",
    "train_one_step: callable = train.get_train_one_step()\n",
    "accuracy_metric.reset_state()\n",
    "\n",
    "# Copy originals\n",
    "model: keras.Model = mod.load_model(0, 1)\n",
    "mask_model: keras.Model = mod.load_model(0, 1, masks=True)\n",
    "\n",
    "# Sanity Check\n",
    "for _ in range(100):\n",
    "    test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "    print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "epochs: int = 1\n",
    "batch_size: int = len(X_train)\n",
    "\n",
    "train_accuracies: np.array = np.zeros(epochs)\n",
    "test_accuracies: np.array = np.zeros(epochs)\n",
    "\n",
    "original_weights: list[np.ndarray] = copy.deepcopy(model.get_weights())\n",
    "masks: list[np.ndarray] = copy.deepcopy(mask_model.get_weights())\n",
    "\n",
    "# Single training steps\n",
    "for _ in range(10):\n",
    "    train_loss, train_accuracy = train_one_step(model, mask_model, X_train, Y_train, optimizer)\n",
    "    test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "\n",
    "ending_weights: list[np.ndarray] = copy.deepcopy(model.get_weights())\n",
    "\n",
    "# Compare the masks with the starting/ending training weights to make sure the ones masked off haven't changed\n",
    "# Picked a layer near the end so it would have been affected by backpropagation\n",
    "for idx, (mask, start_weight, end_weight) in enumerate(zip(masks[0][4], original_weights[0][4], ending_weights[0][4])):\n",
    "    if mask == 0:\n",
    "        assert start_weight == end_weight, f'Weights not equal at index {idx}'\n",
    "    # This could technically fail, but is unable to since even a small update would trigger it\n",
    "    elif mask == 0:\n",
    "        assert start_weight != end_weight, f'Weights not equal at index {idx}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(C)\n",
    "reload(ds)\n",
    "reload(train)\n",
    "\n",
    "# Testing `training_loop` function\n",
    "epochs: int = C.TRAINING_EPOCHS\n",
    "batch_size: int = 60\n",
    "X_train, _, _, _ = mnist_dataset.load()\n",
    "num_datapoints: int = X_train.shape[0]\n",
    "\n",
    "# Copy originals\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "\n",
    "# Sanity Check\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "trial_data: history.TrialData = train.training_loop(0, model, mask_model, mnist_dataset, epochs, batch_size=batch_size)\n",
    "\n",
    "iteration_count: int = np.sum(trial_data.train_accuracies != 0)\n",
    "print(f'Took {iteration_count} iterations')\n",
    "print(f'Ended on epoch {np.ceil(iteration_count * batch_size / num_datapoints)} out of {epochs}')\n",
    "print(f'Ended with a best training accuracy of {np.max(trial_data.train_accuracies) * 100:.2f}% and test accuracy of {np.max(trial_data.test_accuracies) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(train)\n",
    "\n",
    "# Testing `train` function\n",
    "\n",
    "# Copy originals\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "\n",
    "# Sanity Check\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "trial_data = train.train(0, 0, model, mask_model, mnist_dataset, batch_size=C.BATCH_SIZE)\n",
    "\n",
    "print(f'\\nTook {np.sum(trial_data.test_accuracies != 0)} / {C.TRAINING_EPOCHS} epochs')\n",
    "print(f'Ended with a best training accuracy of {np.max(trial_data.train_accuracies) * 100:.2f}% and test accuracy of training accuracy of {np.max(trial_data.test_accuracies) * 100:.2f}%')\n",
    "\n",
    "print(f'Test Accuracies:')\n",
    "print(trial_data.test_accuracies)\n",
    "print(f'Training Accuracies:')\n",
    "print(trial_data.train_accuracies)\n",
    "\n",
    "# Get test parameters\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the model back\n",
    "loaded_model: keras.Model = mod.load_model(0, 0)\n",
    "\n",
    "# Get test parameters\n",
    "test_loss, test_accuracy = train.test_step(loaded_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layerwise Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pruning)\n",
    "reload(utils)\n",
    "\n",
    "# Test loading the model back\n",
    "loaded_model: keras.Model = mod.load_model(0, 0)\n",
    "mask_model: keras.Model = mod.load_model(0, 0, masks=True)\n",
    "target_sparsity = 0.1\n",
    "\n",
    "def test_pruning_sparsity(model: keras.Model, mask_model: keras.Model, target_sparsity: float):\n",
    "    \"\"\"\n",
    "    Test function to verify pruning correctness.\n",
    "    NOTE: Sensitive to boundary conditions and rounding.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Keras model to copy and test pruning on.\n",
    "        target_sparsity (float): Target sparsity to test with.\n",
    "    \"\"\"\n",
    "    copy_model: keras.Model = copy.deepcopy(model)\n",
    "    mask_model: keras.Model = copy.deepcopy(mask_model)\n",
    "    \n",
    "    print(f'Test Pruning Sparsity: Target Sparsity = {target_sparsity}')\n",
    "    sparse_model = copy.deepcopy(copy_model)\n",
    "\n",
    "    total, nonzero = utils.count_total_and_nonzero_params(copy_model)\n",
    "    print(f'Before Pruning: Total Params: {total}, Nonzero Params: {nonzero}')\n",
    "    pruning.prune(sparse_model, mask_model, pruning.low_magnitude_pruning, target_sparsity)\n",
    "    \n",
    "    # Add some small wiggle room for rounding- even with output being pruned at half the rate this is correct\n",
    "    error_tolerance: int = int(target_sparsity * total / 20)\n",
    "\n",
    "    pruned_total, pruned_nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    print(f'After Pruning:  Total Params: {pruned_total}, Nonzero Params: {pruned_nonzero}')\n",
    "    \n",
    "    assert pruned_total == total\n",
    "    assert np.abs(pruned_nonzero - total * target_sparsity) < error_tolerance\n",
    "\n",
    "    sparse_layer_weight_counts: list[int] = utils.count_total_and_nonzero_params_per_layer(sparse_model)\n",
    "    print(f'Layer total and nonzero weight counts: {sparse_layer_weight_counts}')\n",
    "\n",
    "    # Test that pruning worked as expected\n",
    "    for idx in range(len(sparse_layer_weight_counts))[::2]:\n",
    "        total_synapses, nonzero_synapses = sparse_layer_weight_counts[idx]\n",
    "        total_biases, nonzero_biases = sparse_layer_weight_counts[idx + 1]\n",
    "        assert np.abs((total_synapses + total_biases) * target_sparsity - nonzero_synapses + nonzero_biases) < error_tolerance\n",
    "        \n",
    "    # Test that we can prune the model to half of what it is currently at as well\n",
    "    target_sparsity /= 2\n",
    "    total, nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    pruning.prune(sparse_model, mask_model, pruning.low_magnitude_pruning, target_sparsity)\n",
    "    pruned_total, pruned_nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    \n",
    "    assert np.abs(pruned_nonzero - int(total * target_sparsity)) < error_tolerance\n",
    "    \n",
    "def test_global_pruning(model: keras.Model, mask_model: keras.Model, target_sparsity: float):\n",
    "    \"\"\"\n",
    "    Testing function to demonstrate correctness of global pruning.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Keras model to copy and test pruning on.\n",
    "        target_sparsity (float): Target sparsity to test with.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Test Global Pruning: Target Sparsity = {target_sparsity}')\n",
    "    \n",
    "    # Global pruning will not necessarily have equal pruning in each layer, but overall will be correct\n",
    "    sparse_model = copy.deepcopy(model)\n",
    "    mask_model: keras.Model = copy.deepcopy(mask_model)\n",
    "    \n",
    "    print(f'Mask model: {mask_model.trainable_variables[0][:10]}')\n",
    "    print(f'Sparse model: {sparse_model.trainable_variables[0][:10]}')\n",
    "    \n",
    "    pruning.prune(sparse_model, mask_model, pruning.low_magnitude_pruning, target_sparsity, global_pruning=True)\n",
    "    total, nonzero = utils.count_total_and_nonzero_params(model)\n",
    "    print(f'Before Pruning: Total Params: {total}, Nonzero Params: {nonzero}')\n",
    "    \n",
    "    pruned_total, pruned_nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    print(f'After Pruning:  Total Params: {pruned_total}, Nonzero Params: {pruned_nonzero}')\n",
    "    \n",
    "    # Add some small wiggle room for rounding- even with output being pruned at half the rate this is correct\n",
    "    error_tolerance: int = int(target_sparsity * total / 20)\n",
    "    \n",
    "    pruned_total, pruned_nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    assert np.abs(pruned_nonzero - total * target_sparsity) < error_tolerance\n",
    "    \n",
    "    sparse_layer_weight_counts: list[int] = utils.count_total_and_nonzero_params_per_layer(sparse_model)\n",
    "    print(f'Layer total and nonzero weight counts: {sparse_layer_weight_counts}')\n",
    "    \n",
    "    print(f'Mask model: {mask_model.trainable_variables[4][0][:10]}')\n",
    "    print(f'Sparse model: {sparse_model.trainable_variables[4][0][:10]}')\n",
    "    \n",
    "print()\n",
    "test_pruning_sparsity(loaded_model, mask_model, 0.5)\n",
    "print()\n",
    "test_global_pruning(loaded_model, mask_model, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mod)\n",
    "reload(rewind)\n",
    "    \n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "original_weights = model.get_weights()\n",
    "\n",
    "rewind_to_original_weights: callable = functools.partial(rewind.rewind_to_original_init, 0)\n",
    "rewind_to_random_weights: callable = functools.partial(rewind.rewind_to_random_init, 0, tf.initializers.GlorotUniform())\n",
    "rewind.rewind_model_weights(model, mask_model, rewind_to_random_weights)\n",
    "\n",
    "print(original_weights[0][0][:10])\n",
    "print(model.get_weights()[0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune Low Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mod)\n",
    "reload(pruning)\n",
    "\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "\n",
    "# Asserting that every array in the mask model's weights are 1s\n",
    "for layer in mask_model.layers:\n",
    "    for weights in layer.get_weights():\n",
    "        assert np.all(weights == 1), \"Error: Not all elements in mask model's weights are 1s after updating masks\"\n",
    "\n",
    "# Asserting that every array in the mask model's weights are still 1s\n",
    "for layer in mask_model.layers:\n",
    "    for weights in layer.get_weights():\n",
    "        assert np.all(weights == 1), \"Error: Not all elements in mask model's weights are 1s after updating masks\"\n",
    "        \n",
    "pruning.prune(model, mask_model, pruning.low_magnitude_pruning, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(experiment)\n",
    "reload(pruning)\n",
    "reload(rewind)\n",
    "reload(train)\n",
    "\n",
    "# Pruning Parameters\n",
    "first_step_pruning: float = 0.2\n",
    "target_sparsity: float = 0.10\n",
    "make_lenet: callable = functools.partial(mod.create_lenet_300_100, input_shape, num_classes)\n",
    "\n",
    "global_pruning: bool = False\n",
    "sparsities: list[float] = pruning.get_sparsity_percents(model, first_step_pruning, target_sparsity)\n",
    "experiment_data: history.ExperimentData = experiment.run_iterative_pruning_experiment(\n",
    "    0, \n",
    "    make_lenet, \n",
    "    mnist_dataset,\n",
    "    sparsities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model: keras.Model = mod.load_model(0, 0, initial=True)\n",
    "print(loaded_model.get_weights()[0][0])\n",
    "\n",
    "for round in experiment_data.pruning_rounds:\n",
    "    print(round)\n",
    "    print(round.masks[0][0])\n",
    "    print(round.initial_weights[0][0])\n",
    "    print(round.final_weights[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(experiment)\n",
    "reload(paths)\n",
    "reload(pruning)\n",
    "reload(rewind)\n",
    "reload(train)\n",
    "\n",
    "# Test high level experiment API\n",
    "experiment_directory: str = 'testing_experiment'\n",
    "experiment_summary: history.ExperimentSummary = experiment.run_experiments(\n",
    "    1, \n",
    "    experiment_directory,\n",
    "    functools.partial(experiment.get_lenet_300_100_experiment_parameters, ds.Datasets.MNIST, 0.2, 0.65),\n",
    "    experiment.run_iterative_pruning_experiment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(experiment_summary.experiments[0].pruning_rounds))\n",
    "\n",
    "# TODO: Check out why the final weights aren't still masked off\n",
    "for seed, experiment_data in experiment_summary.experiments.items():\n",
    "    original_model: keras.Model = mod.load_model(seed, 0, initial=True)\n",
    "    for round in experiment_data.pruning_rounds:\n",
    "        print(f'Pruning Round {round.pruning_step}')\n",
    "        # print(f'Original Model:')\n",
    "        # print(original_model.get_weights()[4][0][:20])\n",
    "        # print('Masks:')\n",
    "        # print(mask_model.get_weights()[4][0][:20])\n",
    "        print('Initial Weights:')\n",
    "        print(round.initial_weights[4][0][:20])\n",
    "        print('Final Weights')\n",
    "        print(round.final_weights[4][0][:20])\n",
    "        # print()\n",
    "        # break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(experiment_summary.experiments[0].pruning_rounds))\n",
    "\n",
    "# TODO: Check out why the final weights aren't still masked off\n",
    "for seed, experiment_data in experiment_summary.experiments.items():\n",
    "    original_model: keras.Model = mod.load_model(seed, 0, initial=True)\n",
    "    for round in experiment_data.pruning_rounds:\n",
    "        print(f'Pruning Round {round.pruning_step}')\n",
    "        # print(f'Original Model:')\n",
    "        # print(original_model.get_weights()[4][0][:20])\n",
    "        # print('Masks:')\n",
    "        # print(mask_model.get_weights()[4][0][:20])\n",
    "        print('Initial Weights:')\n",
    "        print(round.initial_weights[4][0][:20])\n",
    "        print('Final Weights')\n",
    "        print(round.final_weights[4][0][:20])\n",
    "        # print()\n",
    "        # break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check out why the final weights aren't still masked off\n",
    "loaded_experiment_summary = history.ExperimentData.load_from(os.path.join('testing_experiment', 'experiment_summary.pkl'))\n",
    "for seed, experiment_data in loaded_experiment_summary.experiments.items():\n",
    "    original_model: keras.Model = mod.load_model(seed, 0, initial=True)\n",
    "    for round in experiment_data.pruning_rounds:\n",
    "        print(f'Pruning Round {round.pruning_step}')\n",
    "        mask_model: keras.Model = mod.load_model(seed, round.pruning_step, masks=True)\n",
    "        print(f'Original Model:')\n",
    "        print(original_model.get_weights()[4][0][:20])\n",
    "        print('Masks:')\n",
    "        print(mask_model.get_weights()[4][0][:20])\n",
    "        print('Initial Weights:')\n",
    "        print(round.initial_weights[4][0][:20])\n",
    "        print('Final Weights')\n",
    "        print(round.final_weights[4][0][:20])\n",
    "        print()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
