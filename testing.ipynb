{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.model import create_model, LeNet300, load_model\n",
    "from src.harness.pruning import prune_by_percent\n",
    "from src.lottery_ticket.foundations.trainer import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'models/model_0/initial' already exists.\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_and_process_mnist()\n",
    "model = create_model(0, X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INDEX: int = 0\n",
    "# Get initial weights\n",
    "dir: str = f'models/model_{MODEL_INDEX}/initial/trial0/'\n",
    "layers = [\n",
    "    dir + 'layer0.npy',\n",
    "    dir + 'layer1.npy',\n",
    "    dir + 'layer2.npy'\n",
    "]\n",
    "\n",
    "layer_weights = {f'layer{i}': np.load(layer) for i, layer in enumerate(layers)}\n",
    "# Test loading a model\n",
    "model: LeNet300 = load_model(MODEL_INDEX, 0, True)\n",
    "\n",
    "# Verify all the layer weights match\n",
    "for i in range(3):\n",
    "    key: str = f'layer{i}'\n",
    "    assert np.array_equal(model.weights[key], layer_weights[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layer0', (784, 300)), ('layer1', (300, 100)), ('layer2', (100, 10))]\n",
      "117599.0 117600.0\n",
      "14999.0 15000.0\n",
      "499.0 500.0\n"
     ]
    }
   ],
   "source": [
    "# Test pruning\n",
    "print([(key, layer.shape) for key, layer in layer_weights.items()])\n",
    "percents: dict[str: float] = {key: 0.5 for key in layer_weights}\n",
    "masks: dict[str, np.array] = {key: np.ones(layer.shape) for key, layer in layer_weights.items()}\n",
    "new_masks: dict[str, np.array] = prune_by_percent(percents, masks, layer_weights)\n",
    "for key in new_masks:\n",
    "    new_mask: np.array = new_masks[key]\n",
    "    old_mask: np.array = masks[key]\n",
    "    assert (old_mask.sum() / 2 - new_mask.sum()) <= 1, f'Doesn\\'t match for key {key}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
