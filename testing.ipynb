{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.harness import constants as C\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.experiment import experiment\n",
    "from src.harness.model import create_model, LeNet300, load_model\n",
    "from src.harness.pruning import prune_by_percent\n",
    "from src.harness.training import train\n",
    "from src.lottery_ticket.foundations import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "X_train, Y_train, X_test, Y_test = load_and_process_mnist()\n",
    "model = create_model(0, X_train, Y_train)\n",
    "initial_weights1: dict[str: np.array] = model.get_current_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Loss: 2.3580663204193115\n",
      "Iteration 2/10, Loss: 2.2764296531677246\n",
      "Iteration 3/10, Loss: 2.2057595252990723\n",
      "Iteration 4/10, Loss: 2.1410763263702393\n",
      "Iteration 5/10, Loss: 2.079235076904297\n",
      "Iteration 6/10, Loss: 2.018141984939575\n",
      "Iteration 7/10, Loss: 1.9564679861068726\n",
      "Iteration 8/10, Loss: 1.8935106992721558\n",
      "Iteration 9/10, Loss: 1.829115867614746\n",
      "Iteration 10/10, Loss: 1.763397455215454\n"
     ]
    }
   ],
   "source": [
    "# Test Training a model\n",
    "optimizer = C.OPTIMIZER()\n",
    "make_dataset: callable = load_and_process_mnist\n",
    "initial_weights2, final_weights = train(make_dataset, model, 0, optimizer, C.TEST_TRAINING_ITERATIONS)\n",
    "\n",
    "for i in range(3):\n",
    "    key: str = f'layer{i}'\n",
    "    # Sanity check that the initial weights are correct\n",
    "    assert np.array_equal(initial_weights1[key], initial_weights2[key])\n",
    "    # Verify the final weights are different from initial weights\n",
    "    assert not np.array_equal(initial_weights2[key], final_weights[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10, Loss: 2.2923543453216553\n",
      "Iteration 2/10, Loss: 2.219968795776367\n",
      "Iteration 3/10, Loss: 2.154776096343994\n",
      "Iteration 4/10, Loss: 2.0936989784240723\n",
      "Iteration 5/10, Loss: 2.0346004962921143\n",
      "Iteration 6/10, Loss: 1.9759339094161987\n",
      "Iteration 7/10, Loss: 1.9168710708618164\n",
      "Iteration 8/10, Loss: 1.8570656776428223\n",
      "Iteration 9/10, Loss: 1.7964187860488892\n",
      "Iteration 10/10, Loss: 1.7350682020187378\n"
     ]
    }
   ],
   "source": [
    "# Try creating a model from the initial weights as a preset\n",
    "make_model: callable = functools.partial(LeNet300, 0)\n",
    "percents: dict[str: float] = {key: 0.5 for key in final_weights}\n",
    "starting_masks: dict[str: np.ndarray] = {f'layer{i}': np.ones(initial_weights2[f'layer{i}'].shape) for i in range(3)}\n",
    "\n",
    "# Create pruned masks\n",
    "masks = prune_by_percent(C.PRUNING_PERCENTS, starting_masks, final_weights)\n",
    "\n",
    "# This is transforming initial weights into a tensor when it should be Numpy?\n",
    "# Passing in masks is the issue\n",
    "model = make_model(X_train, Y_train, presets=initial_weights2, masks=masks)\n",
    "\n",
    "# Sanity check that the weights are correctly loaded and are masked off accordingly\n",
    "for i in range(3):\n",
    "    key = f'layer{i}'\n",
    "    layer_weights = model.get_current_weights()[key]\n",
    "    layer_mask = model.masks[key]\n",
    "    expected_weights: np.ndarray = initial_weights2[key] * layer_mask\n",
    "    assert np.array_equal(expected_weights, layer_weights), f'Expected {expected_weights} but received {layer_weights}'\n",
    "    assert np.array_equal(model.masks[key], masks[key])\n",
    "\n",
    "# Save the tensors storing the actual weight values (these include the masked off weights)\n",
    "pretrained_weights = model.weights.copy()\n",
    "\n",
    "# Try doing a a simulated round of pruning\n",
    "initial_weights3, final_weights2 = train(make_dataset, model, 1, optimizer, C.TEST_TRAINING_ITERATIONS)\n",
    "\n",
    "# Make sure the masked off weights don't receive any updates in the actual tensorflow tensor\n",
    "trained_weights = model.weights\n",
    "\n",
    "# Compare the masked weights before and after training\n",
    "for i in range(3):\n",
    "    key = f'layer{i}'\n",
    "    pretrained_layer_weights = pretrained_weights[key]\n",
    "    trained_layer_weights = trained_weights[key]\n",
    "    \n",
    "    # Invert the mask, to only look at the weights which WERE masked off\n",
    "    inverted_mask: np.ndarray = 1 - masks[key]\n",
    "    masked_pretrained_weights = pretrained_layer_weights * inverted_mask\n",
    "    masked_trained_weights = trained_layer_weights * inverted_mask\n",
    "    \n",
    "    # Assert that the masked weights remain unchanged after training\n",
    "    assert np.array_equal(masked_pretrained_weights, masked_trained_weights), f'Weights changed after training for layer {key}'\n",
    "    assert not np.array_equal(initial_weights3, final_weights2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_INDEX: int = 0\n",
    "# Get initial weights\n",
    "dir: str = f'models/model_{MODEL_INDEX}/initial/'\n",
    "weight_files = [paths.weights(dir) + f'/layer{i}.npy' for i in range(3)]\n",
    "mask_files = [paths.masks(dir) + f'/layer{i}.npy' for i in range(3)]\n",
    "\n",
    "layer_weights = {f'layer{i}': np.load(layer) for i, layer in enumerate(weight_files)}\n",
    "masks = {f'layer{i}': np.load(layer) for i, layer in enumerate(mask_files)}\n",
    "# Test loading a model\n",
    "model: LeNet300 = load_model(MODEL_INDEX, 0, True)\n",
    "\n",
    "for i in range(3):\n",
    "    key: str = f'layer{i}'\n",
    "    # Verify all the layer weights match\n",
    "    assert np.array_equal(model.weights[key], layer_weights[key])\n",
    "    # Verify all masks are 1s\n",
    "    assert np.sum(masks[key]) == masks[key].size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layer0', (784, 300)), ('layer1', (300, 100)), ('layer2', (100, 10))]\n"
     ]
    }
   ],
   "source": [
    "# Test pruning\n",
    "print([(key, layer.shape) for key, layer in layer_weights.items()])\n",
    "percents: dict[str: float] = {key: 0.5 for key in layer_weights}\n",
    "new_masks: dict[str, np.array] = prune_by_percent(percents, masks, layer_weights)\n",
    "for key in new_masks:\n",
    "    new_mask: np.array = new_masks[key]\n",
    "    old_mask: np.array = masks[key]\n",
    "    assert (old_mask.sum() / 2 - new_mask.sum()) <= 1, f'Doesn\\'t match for key {key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Step 0\n",
      "Iteration 1/10, Loss: 2.3972244262695312\n",
      "Iteration 2/10, Loss: 2.298975706100464\n",
      "Iteration 3/10, Loss: 2.2233521938323975\n",
      "Iteration 4/10, Loss: 2.155620574951172\n",
      "Iteration 5/10, Loss: 2.090500593185425\n",
      "Iteration 6/10, Loss: 2.025655746459961\n",
      "Iteration 7/10, Loss: 1.9601891040802002\n",
      "Iteration 8/10, Loss: 1.8937861919403076\n",
      "Iteration 9/10, Loss: 1.8262752294540405\n",
      "Iteration 10/10, Loss: 1.7576806545257568\n",
      "Pruning Step 1\n",
      "Iteration 1/10, Loss: 2.328902006149292\n",
      "Iteration 2/10, Loss: 2.2418630123138428\n",
      "Iteration 3/10, Loss: 2.170030117034912\n",
      "Iteration 4/10, Loss: 2.1039531230926514\n",
      "Iteration 5/10, Loss: 2.0400655269622803\n",
      "Iteration 6/10, Loss: 1.9768481254577637\n",
      "Iteration 7/10, Loss: 1.9134869575500488\n",
      "Iteration 8/10, Loss: 1.8495373725891113\n",
      "Iteration 9/10, Loss: 1.784929633140564\n",
      "Iteration 10/10, Loss: 1.7198681831359863\n",
      "Pruning Step 2\n",
      "Iteration 1/10, Loss: 2.2701938152313232\n",
      "Iteration 2/10, Loss: 2.1930620670318604\n",
      "Iteration 3/10, Loss: 2.1257569789886475\n",
      "Iteration 4/10, Loss: 2.0626347064971924\n",
      "Iteration 5/10, Loss: 2.0012004375457764\n",
      "Iteration 6/10, Loss: 1.9402564764022827\n",
      "Iteration 7/10, Loss: 1.879219889640808\n",
      "Iteration 8/10, Loss: 1.8179731369018555\n",
      "Iteration 9/10, Loss: 1.7565895318984985\n",
      "Iteration 10/10, Loss: 1.695383071899414\n",
      "Pruning Step 3\n",
      "Iteration 1/10, Loss: 2.2267236709594727\n",
      "Iteration 2/10, Loss: 2.1522293090820312\n",
      "Iteration 3/10, Loss: 2.0870141983032227\n",
      "Iteration 4/10, Loss: 2.025876998901367\n",
      "Iteration 5/10, Loss: 1.966504454612732\n",
      "Iteration 6/10, Loss: 1.9078816175460815\n",
      "Iteration 7/10, Loss: 1.8497564792633057\n",
      "Iteration 8/10, Loss: 1.7920423746109009\n",
      "Iteration 9/10, Loss: 1.734838843345642\n",
      "Iteration 10/10, Loss: 1.6782695055007935\n",
      "Pruning Step 4\n",
      "Iteration 1/10, Loss: 2.156140089035034\n",
      "Iteration 2/10, Loss: 2.088264226913452\n",
      "Iteration 3/10, Loss: 2.028071641921997\n",
      "Iteration 4/10, Loss: 1.9713298082351685\n",
      "Iteration 5/10, Loss: 1.916255235671997\n",
      "Iteration 6/10, Loss: 1.8621515035629272\n",
      "Iteration 7/10, Loss: 1.8087762594223022\n",
      "Iteration 8/10, Loss: 1.7560657262802124\n",
      "Iteration 9/10, Loss: 1.7040073871612549\n",
      "Iteration 10/10, Loss: 1.652691125869751\n",
      "Pruning Step 5\n",
      "Iteration 1/10, Loss: 2.1050970554351807\n",
      "Iteration 2/10, Loss: 2.045022487640381\n",
      "Iteration 3/10, Loss: 1.9915200471878052\n",
      "Iteration 4/10, Loss: 1.9410127401351929\n",
      "Iteration 5/10, Loss: 1.8920917510986328\n",
      "Iteration 6/10, Loss: 1.844166874885559\n",
      "Iteration 7/10, Loss: 1.7969623804092407\n",
      "Iteration 8/10, Loss: 1.7504136562347412\n",
      "Iteration 9/10, Loss: 1.7045137882232666\n",
      "Iteration 10/10, Loss: 1.6592603921890259\n"
     ]
    }
   ],
   "source": [
    "# Test experiment\n",
    "make_dataset: callable = load_and_process_mnist\n",
    "# Make partial function application giving the model its random seed\n",
    "make_model: callable = functools.partial(LeNet300, 0)\n",
    "train_model: callable = functools.partial(train, iterations=C.TEST_TRAINING_ITERATIONS)\n",
    "prune_masks: callable = functools.partial(prune_by_percent, C.PRUNING_PERCENTS)\n",
    "experiment(make_dataset, make_model, train_model, prune_masks, C.TEST_PRUNING_STEPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
