{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import functools\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.sparsity.keras import ConstantSparsity, PolynomialDecay, prune_low_magnitude\n",
    "\n",
    "from src.harness.constants import Constants as C\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.experiment import ExperimentData\n",
    "from src.harness.model import create_lenet_300_100, create_masked_nn, create_pruned_lenet, save_model, load_model\n",
    "from src.harness.pruning import create_pruning_callbacks, create_pruning_parameters\n",
    "from src.harness.training import test_step, train, train_one_step, training_loop, TrainingRound\n",
    "from src.harness.utils import count_params, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_and_process_mnist()\n",
    "\n",
    "num_epochs: int = 10\n",
    "batch_size: int = 60\n",
    "input_shape: tuple = X_train[0].shape\n",
    "num_classes: int = 10\n",
    "\n",
    "num_train_samples: int = X_train.shape[0]\n",
    "\n",
    "end_step: int = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * num_epochs\n",
    "\n",
    "pruning_parameters: dict = create_pruning_parameters(0.01, 0, end_step, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeNet-300-100\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 300)            235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 100)            30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 10)             1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(784, 300) dtype=float32, numpy=\n",
       " array([[ 0.00233377, -0.02841079,  0.05042759, ...,  0.04471111,\n",
       "          0.0469005 , -0.00633286],\n",
       "        [ 0.04599051,  0.07108735, -0.06869254, ...,  0.0193451 ,\n",
       "         -0.03234061,  0.03169315],\n",
       "        [ 0.04332277,  0.00188898, -0.05098354, ...,  0.05931461,\n",
       "          0.0038614 , -0.06489589],\n",
       "        ...,\n",
       "        [ 0.06133612, -0.00425664, -0.03556142, ...,  0.05923072,\n",
       "         -0.04446548,  0.03342723],\n",
       "        [ 0.02262612, -0.01283204,  0.03786094, ..., -0.03319833,\n",
       "          0.0259892 , -0.02313284],\n",
       "        [ 0.00880335,  0.00193495, -0.07007851, ..., -0.05422467,\n",
       "         -0.0134997 , -0.03317983]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(300,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(300, 100) dtype=float32, numpy=\n",
       " array([[-0.0434095 , -0.07433628, -0.05388562, ...,  0.0876581 ,\n",
       "         -0.10433097,  0.03121664],\n",
       "        [-0.00328721, -0.02546117,  0.04901647, ..., -0.10859712,\n",
       "         -0.06570645,  0.02486881],\n",
       "        [-0.03667895, -0.03799579,  0.01590053, ..., -0.0737709 ,\n",
       "         -0.08908979, -0.04353051],\n",
       "        ...,\n",
       "        [-0.00807663,  0.05725048, -0.01186235, ..., -0.09258783,\n",
       "          0.01092064,  0.10543489],\n",
       "        [-0.09088705,  0.0916472 , -0.06650726, ...,  0.08839033,\n",
       "          0.01187759,  0.10380534],\n",
       "        [ 0.09920131,  0.09549812, -0.03529024, ..., -0.11858274,\n",
       "          0.06918063,  0.01733922]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(100, 10) dtype=float32, numpy=\n",
       " array([[ 0.07437277, -0.19531535,  0.19520438,  0.22488275,  0.07258725,\n",
       "         -0.08138183,  0.14727226, -0.22700475, -0.06249095,  0.00931731],\n",
       "        [ 0.03195813, -0.210477  ,  0.21443737,  0.17592233, -0.01365571,\n",
       "         -0.22713138,  0.20700791,  0.14148203, -0.00154041,  0.20497328],\n",
       "        [ 0.06000116, -0.10535178, -0.00137274,  0.14773947,  0.0375104 ,\n",
       "          0.14155778, -0.00968431,  0.13058028,  0.02592465,  0.16830257],\n",
       "        [-0.22678703,  0.04024464,  0.05280712,  0.10807505, -0.04410902,\n",
       "         -0.02121568,  0.09835726,  0.11579394, -0.01995897,  0.07619014],\n",
       "        [-0.0771828 , -0.19200909, -0.15237138,  0.14020786, -0.02927884,\n",
       "         -0.1622223 ,  0.12302566, -0.11761715, -0.09103478, -0.20733644],\n",
       "        [ 0.09467754, -0.21544455,  0.10824448,  0.02606419,  0.08428535,\n",
       "          0.05895248, -0.06754348,  0.11372283,  0.18307653, -0.04822306],\n",
       "        [-0.18179394, -0.18711403,  0.11880636,  0.16339147,  0.04486513,\n",
       "         -0.03560355,  0.0299603 ,  0.17413408,  0.13596413,  0.01232244],\n",
       "        [ 0.20696887,  0.22754493, -0.00598615, -0.107125  ,  0.0724853 ,\n",
       "         -0.19336936,  0.01948756,  0.13674456,  0.21405783, -0.01800491],\n",
       "        [ 0.10978767, -0.13517638,  0.02026695,  0.175192  , -0.03428805,\n",
       "          0.12859887,  0.051211  , -0.07638609, -0.11449642,  0.1178816 ],\n",
       "        [ 0.17002076,  0.03300235,  0.17908466,  0.03590971,  0.01078059,\n",
       "          0.05808678,  0.15832719, -0.07843471,  0.04697883, -0.17275444],\n",
       "        [ 0.20155755, -0.05159409,  0.22990698,  0.02419248,  0.12447134,\n",
       "         -0.05193898,  0.19473141, -0.1513941 , -0.00193909,  0.04098037],\n",
       "        [ 0.08564532, -0.18681791, -0.11989307, -0.00327329, -0.22671103,\n",
       "          0.0318867 , -0.21735324, -0.17495061,  0.01295701, -0.20183511],\n",
       "        [ 0.11441067, -0.00190373, -0.17783225, -0.10858288, -0.05104712,\n",
       "          0.0563257 , -0.17800364, -0.14065984, -0.0161186 , -0.00076647],\n",
       "        [-0.0831385 ,  0.11301771,  0.07404727,  0.0227063 , -0.13399291,\n",
       "          0.05033243,  0.13891551,  0.16939098, -0.1300475 , -0.20702645],\n",
       "        [ 0.1243799 , -0.15255225, -0.23262435, -0.0320161 , -0.16430633,\n",
       "         -0.03101905,  0.20316201, -0.03965613,  0.08379298,  0.1661712 ],\n",
       "        [ 0.22735283, -0.20099531,  0.07238808, -0.09850588,  0.0972909 ,\n",
       "          0.02390048,  0.22203323,  0.20606625,  0.13437438,  0.08339897],\n",
       "        [ 0.11083052, -0.17712429,  0.04338869,  0.13343751, -0.22240236,\n",
       "         -0.04087107, -0.1363908 ,  0.09703943,  0.04367018, -0.15531793],\n",
       "        [-0.04354373, -0.02244876,  0.16326708, -0.14865851, -0.08990015,\n",
       "          0.232737  , -0.14570974,  0.03102762,  0.22603181, -0.06135826],\n",
       "        [ 0.0207251 , -0.13566598, -0.04044749,  0.07904968,  0.14188546,\n",
       "         -0.01158498, -0.09716226, -0.22020897,  0.05711818, -0.1950233 ],\n",
       "        [ 0.20571113, -0.09969889,  0.23145914, -0.20387833,  0.1399833 ,\n",
       "          0.14703622, -0.19785838,  0.14372453,  0.06241348,  0.11620906],\n",
       "        [-0.16857395, -0.12257474, -0.12252112, -0.20310111,  0.14529508,\n",
       "          0.1618202 , -0.18670237,  0.16667122,  0.03563228, -0.16059914],\n",
       "        [ 0.11865023,  0.0490382 , -0.16379717,  0.16378933,  0.10253724,\n",
       "          0.1703901 ,  0.05667701,  0.21083581, -0.18016206, -0.03358334],\n",
       "        [-0.03814457,  0.09115562,  0.23028207, -0.02331574,  0.22827539,\n",
       "         -0.09607902, -0.11588821, -0.01053961,  0.21738821, -0.06335403],\n",
       "        [-0.1607731 ,  0.1240069 , -0.05871215,  0.21935713,  0.01271774,\n",
       "          0.02198204, -0.21309808, -0.16666088, -0.22825482,  0.00998177],\n",
       "        [-0.08749773,  0.03715917,  0.09712601, -0.15318245,  0.10453114,\n",
       "          0.1797716 ,  0.20015717, -0.0409165 , -0.04474641, -0.06373017],\n",
       "        [ 0.02140924, -0.21821058, -0.03374225,  0.11604935, -0.20540977,\n",
       "          0.13135388, -0.1630581 , -0.16021478, -0.02779408,  0.10714689],\n",
       "        [ 0.1224767 ,  0.1862638 , -0.22716095, -0.03333889, -0.12485533,\n",
       "          0.09648332, -0.01876219,  0.05806664, -0.19328777, -0.10795851],\n",
       "        [-0.1217537 ,  0.06642175, -0.04682381,  0.0041856 ,  0.1416513 ,\n",
       "          0.02577636,  0.14266708,  0.22919652,  0.20965254, -0.17710859],\n",
       "        [ 0.09238142,  0.00104088, -0.11382049,  0.09264463,  0.02412575,\n",
       "         -0.09824792, -0.21084896,  0.1906997 , -0.1283071 , -0.13440606],\n",
       "        [-0.04333386, -0.1933057 ,  0.15211034,  0.18587649, -0.11670569,\n",
       "          0.07370636,  0.04673663,  0.1814987 ,  0.21546438, -0.1001052 ],\n",
       "        [-0.12267775, -0.10142176,  0.18010893,  0.11606529, -0.04325189,\n",
       "         -0.14347458, -0.17595385,  0.01821712,  0.21481362,  0.1300855 ],\n",
       "        [ 0.13331851, -0.13449594, -0.13259076, -0.16806513, -0.06783046,\n",
       "          0.2033656 ,  0.04436511,  0.09425819,  0.11640546, -0.03225152],\n",
       "        [ 0.07504207, -0.08801936, -0.10138841,  0.03781834, -0.21611574,\n",
       "          0.16882402,  0.08507609,  0.02619332,  0.21710667, -0.20969065],\n",
       "        [ 0.11123288, -0.03092749, -0.12239806, -0.1096448 , -0.22742927,\n",
       "         -0.11586415,  0.14414695, -0.1893617 ,  0.20194352,  0.1108987 ],\n",
       "        [-0.07426442, -0.09191987, -0.07390817,  0.05889028,  0.1151616 ,\n",
       "         -0.0801366 , -0.19583675,  0.21171203,  0.10564351, -0.1081975 ],\n",
       "        [-0.11979195,  0.16639209,  0.17950433,  0.18920675, -0.10045166,\n",
       "         -0.09360299,  0.16431904, -0.1426134 , -0.13607326, -0.18408373],\n",
       "        [-0.19488019, -0.08939694,  0.14774388, -0.04718882,  0.21982798,\n",
       "          0.1642094 ,  0.07860404, -0.19324663,  0.2126961 ,  0.11136222],\n",
       "        [-0.03754599,  0.19632009,  0.00173301,  0.08940151, -0.07332812,\n",
       "         -0.09927754, -0.03025079, -0.07182468, -0.03620726, -0.08167282],\n",
       "        [ 0.22665909,  0.11498633,  0.16578642, -0.22277778, -0.08355372,\n",
       "         -0.00822882, -0.04881743,  0.18147233, -0.15180293,  0.14800775],\n",
       "        [-0.00945914, -0.17694522,  0.05078563, -0.18059716,  0.0767746 ,\n",
       "          0.03148293, -0.09294698,  0.03600135, -0.02225238, -0.03246991],\n",
       "        [-0.2156097 , -0.13437165,  0.02445853,  0.0272181 , -0.14805454,\n",
       "          0.20907626,  0.1652129 , -0.0852669 ,  0.11883026,  0.18098375],\n",
       "        [ 0.1356282 ,  0.13932657,  0.22543874, -0.18966313, -0.20344362,\n",
       "         -0.10399897, -0.05847903, -0.0816119 , -0.06489047,  0.04966491],\n",
       "        [-0.00863576,  0.08222786, -0.07347958, -0.1806221 ,  0.09591052,\n",
       "         -0.22049212, -0.14360493,  0.09134755,  0.11548394,  0.06604478],\n",
       "        [ 0.08592039, -0.10266159, -0.2187591 , -0.05533569, -0.16850808,\n",
       "         -0.17280188, -0.10716893, -0.00598265, -0.16256374,  0.01960078],\n",
       "        [-0.2111821 ,  0.23113143,  0.19000056,  0.01032767,  0.21387222,\n",
       "         -0.07748783, -0.14222373, -0.12899733, -0.22353272,  0.10301572],\n",
       "        [ 0.20301056, -0.20313841,  0.01584448,  0.16780844, -0.10571985,\n",
       "          0.0660508 , -0.15899771, -0.01488401,  0.17934936, -0.18682837],\n",
       "        [-0.03196664, -0.15330184,  0.17641574, -0.14730276, -0.09238765,\n",
       "         -0.21214546,  0.16305125,  0.03865457,  0.02671733,  0.05421969],\n",
       "        [ 0.1367288 ,  0.20202515,  0.11577958,  0.10276306, -0.03066306,\n",
       "          0.20188272,  0.02721909,  0.06456634,  0.16993701, -0.04215032],\n",
       "        [ 0.1714499 ,  0.12950003, -0.06478691, -0.04766852, -0.03714179,\n",
       "          0.2083483 ,  0.16376075,  0.1145159 , -0.18981196, -0.07558565],\n",
       "        [-0.20182286,  0.01327501,  0.19450444, -0.11686087, -0.04446238,\n",
       "         -0.16404712,  0.07472563, -0.2232184 ,  0.18501559,  0.20829347],\n",
       "        [ 0.17328185,  0.17401981, -0.04909723, -0.10012314,  0.20148477,\n",
       "          0.22232312, -0.1619877 ,  0.11584589,  0.11106092,  0.03449252],\n",
       "        [-0.22847483, -0.15861045,  0.1238662 ,  0.14049768, -0.131679  ,\n",
       "         -0.11624385,  0.13552088, -0.14454263,  0.18840948, -0.04766902],\n",
       "        [-0.10467352, -0.1345926 , -0.18517333, -0.21576115,  0.19668809,\n",
       "         -0.00155205,  0.11060411, -0.03873798,  0.10277283,  0.10494491],\n",
       "        [ 0.05773661,  0.23086444,  0.00989775,  0.00445555, -0.16099834,\n",
       "         -0.06107683, -0.01660226,  0.08458942,  0.0273999 ,  0.23191285],\n",
       "        [ 0.01001456, -0.03603865,  0.14806312,  0.221998  , -0.06491865,\n",
       "         -0.14504638, -0.10427287, -0.04228808,  0.1491403 , -0.02199763],\n",
       "        [ 0.01715079, -0.00092077, -0.14857605, -0.1852873 , -0.07486066,\n",
       "         -0.12406225, -0.14799127,  0.13412282, -0.00134073, -0.05837584],\n",
       "        [ 0.1385909 ,  0.20783296,  0.10161102, -0.22423093, -0.12854654,\n",
       "          0.07616019, -0.08342165,  0.18362015, -0.11045659, -0.03329541],\n",
       "        [ 0.05492309, -0.0787918 , -0.20521744,  0.2099877 , -0.14940122,\n",
       "          0.22644326,  0.1683195 ,  0.11723605, -0.04760315,  0.07776392],\n",
       "        [-0.03815849, -0.22707531,  0.00737438,  0.13414085,  0.03042892,\n",
       "         -0.06841736,  0.11574745, -0.08327715,  0.05779496,  0.10158396],\n",
       "        [ 0.10613608,  0.01511899, -0.159924  , -0.03016587,  0.21957752,\n",
       "         -0.0346712 ,  0.13812527,  0.01019208,  0.09570867, -0.21984977],\n",
       "        [-0.19435343,  0.1446873 ,  0.04484758,  0.07412142,  0.14812112,\n",
       "          0.10264918,  0.22336975,  0.14380768,  0.09413025, -0.02245572],\n",
       "        [ 0.05418193, -0.14353317,  0.00060761,  0.03930923,  0.08958048,\n",
       "         -0.1611169 , -0.18783295,  0.01043041,  0.16487107,  0.2057743 ],\n",
       "        [-0.02253301, -0.17866637, -0.00376648,  0.1494304 ,  0.15802154,\n",
       "         -0.04060508, -0.05228065, -0.2188614 , -0.19741175, -0.18611592],\n",
       "        [-0.02564952, -0.19810721,  0.12936243, -0.06592484, -0.22077699,\n",
       "         -0.17198262,  0.119762  , -0.01570578,  0.08830667,  0.20395389],\n",
       "        [ 0.09105349, -0.21673086,  0.01960078, -0.04324031, -0.1428304 ,\n",
       "         -0.21024947, -0.05328901, -0.12584381, -0.20264785, -0.17758507],\n",
       "        [-0.02764501, -0.1931376 , -0.0353983 ,  0.11802602,  0.02166408,\n",
       "         -0.12846473,  0.07995272,  0.22912732, -0.19364637,  0.18835837],\n",
       "        [-0.08167042,  0.0349043 , -0.03165799,  0.1274792 , -0.05704531,\n",
       "          0.13123411,  0.05699128,  0.10696384,  0.1399172 , -0.0675599 ],\n",
       "        [ 0.09952509, -0.14889067, -0.08049007,  0.11967525,  0.07770064,\n",
       "          0.19277924, -0.01880495,  0.12164956,  0.20262888,  0.09587622],\n",
       "        [-0.04476552, -0.06883007, -0.02792977,  0.06252307,  0.20810181,\n",
       "         -0.03091297,  0.1402838 ,  0.12188566, -0.02968065,  0.2128165 ],\n",
       "        [ 0.12555978,  0.09879044,  0.17402706,  0.04101506,  0.06163576,\n",
       "          0.23020256,  0.16545892, -0.06318782, -0.15442584, -0.0341008 ],\n",
       "        [ 0.09028029, -0.1303032 ,  0.03328732, -0.17465265,  0.01168071,\n",
       "          0.23240456,  0.21315026,  0.01140936, -0.13865866,  0.22570357],\n",
       "        [-0.10074589, -0.13487479, -0.15382814, -0.00291103, -0.08195034,\n",
       "          0.05400181, -0.02318846, -0.22742131,  0.09871939, -0.05458692],\n",
       "        [ 0.00944321, -0.03679433, -0.1267373 ,  0.03288424,  0.02170083,\n",
       "          0.12407789,  0.10839087,  0.05524358,  0.17966926, -0.06994283],\n",
       "        [ 0.04324999,  0.13371918, -0.22227357,  0.14468712, -0.01813103,\n",
       "         -0.17552003, -0.17262514, -0.1791291 ,  0.11927214, -0.1498343 ],\n",
       "        [ 0.0358558 ,  0.00562583, -0.04512985,  0.04323635, -0.14016525,\n",
       "          0.06407869,  0.12870869, -0.00644632,  0.09249195, -0.16318917],\n",
       "        [ 0.14251116, -0.15320957, -0.20815581, -0.1843215 , -0.10658142,\n",
       "         -0.15029114, -0.08185764, -0.22064552, -0.05508187, -0.08960786],\n",
       "        [-0.14789678,  0.17657599, -0.21947959, -0.15346688,  0.0688093 ,\n",
       "          0.12791201,  0.21494102, -0.17953575, -0.02915245, -0.06085522],\n",
       "        [-0.03624447, -0.18363515, -0.03410292,  0.00488965,  0.2273874 ,\n",
       "          0.19600359,  0.1665552 ,  0.10012886, -0.08704118, -0.18366204],\n",
       "        [ 0.09742519, -0.17583525, -0.13017285,  0.21295938, -0.04691976,\n",
       "          0.15038201,  0.22162989,  0.2042714 , -0.04927084, -0.01186356],\n",
       "        [-0.1462825 , -0.059709  , -0.07017225, -0.07827641, -0.04722056,\n",
       "          0.17972022,  0.22785658, -0.05038166,  0.05083212,  0.12627885],\n",
       "        [-0.02166174,  0.19252494,  0.0467115 ,  0.15446153,  0.00804602,\n",
       "         -0.17417552, -0.201345  , -0.03758764,  0.2311098 , -0.22647299],\n",
       "        [-0.19532631, -0.16177922, -0.00290029,  0.10719943,  0.05596775,\n",
       "          0.14004794, -0.13941365,  0.05662507,  0.12920582, -0.19089377],\n",
       "        [ 0.21226647,  0.00501055,  0.20870265, -0.15449439, -0.12143319,\n",
       "         -0.02872854,  0.09986231,  0.16185951,  0.04028133, -0.12459824],\n",
       "        [-0.11573569,  0.08267879,  0.04468971,  0.0828447 ,  0.05907398,\n",
       "         -0.07448497,  0.221288  , -0.05543184, -0.1689713 , -0.22872373],\n",
       "        [ 0.0898841 , -0.13319954, -0.12445252, -0.19116177, -0.10497448,\n",
       "          0.16211951,  0.15403712, -0.1857713 , -0.22231773,  0.1634872 ],\n",
       "        [-0.16262488, -0.18122092,  0.01103462,  0.16635   , -0.08665374,\n",
       "         -0.10360725,  0.11641985, -0.07043451,  0.01756507,  0.13580003],\n",
       "        [ 0.01805156,  0.15678182,  0.1562571 ,  0.1447849 ,  0.20375517,\n",
       "          0.11576498,  0.05681935,  0.03695536, -0.03934553, -0.18074338],\n",
       "        [ 0.11777356, -0.09798793, -0.04228953, -0.11400291,  0.09225857,\n",
       "         -0.23334183, -0.21462964, -0.02042878,  0.08567917,  0.13642067],\n",
       "        [ 0.153007  ,  0.03993905, -0.13950169, -0.19577718,  0.04150245,\n",
       "         -0.18824911, -0.12648472,  0.2202684 ,  0.00617687,  0.15343392],\n",
       "        [ 0.08252934, -0.00721987,  0.07968077, -0.00453323,  0.20692551,\n",
       "         -0.1158632 , -0.10775092, -0.20938778, -0.06877172, -0.17540237],\n",
       "        [-0.12863673,  0.11057514,  0.22477081, -0.2024548 , -0.12902218,\n",
       "         -0.12377565,  0.14532652, -0.18390949, -0.14404544, -0.00313354],\n",
       "        [ 0.22637671, -0.08322224, -0.22369231,  0.05058411, -0.1443418 ,\n",
       "          0.0240038 ,  0.22436878, -0.10103498,  0.11910278, -0.13915941],\n",
       "        [-0.00577962,  0.21152326, -0.12672365, -0.21916431, -0.06240129,\n",
       "         -0.18810724, -0.07769714, -0.19326077, -0.04125673,  0.01806983],\n",
       "        [ 0.09093171,  0.02713007, -0.06337352,  0.08517036, -0.2010594 ,\n",
       "          0.07357857, -0.08238739, -0.13938788,  0.08810532,  0.1633439 ],\n",
       "        [-0.16376108,  0.07930958,  0.13437164,  0.10094991, -0.07642213,\n",
       "         -0.19181693, -0.01762192, -0.05039035, -0.13352528, -0.04042695],\n",
       "        [ 0.01309387,  0.03217193,  0.12288401,  0.1497396 , -0.05306718,\n",
       "         -0.20409355, -0.1890828 ,  0.07659569, -0.1229766 ,  0.21490425],\n",
       "        [ 0.14403018, -0.02150495, -0.08679506,  0.1736052 , -0.03265205,\n",
       "          0.15798351, -0.10601853, -0.09349746, -0.03610125,  0.21902105],\n",
       "        [ 0.2144368 , -0.04987946,  0.02528664, -0.22651313,  0.08717921,\n",
       "         -0.03107606, -0.08455099,  0.16531685, -0.16060778, -0.18017504],\n",
       "        [ 0.09681243,  0.18553811,  0.03908077, -0.22740188,  0.0671317 ,\n",
       "         -0.04995017,  0.09577188,  0.17299533,  0.03645059, -0.17026854],\n",
       "        [-0.1354793 ,  0.17977756,  0.10198855,  0.12408859, -0.21768104,\n",
       "          0.06909776,  0.068694  , -0.0831636 ,  0.14932048,  0.05989715]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model with the same architecture using all Keras components to check its accuracy with the same parameters\n",
    "set_seed(0)\n",
    "\n",
    "model: keras.Model = create_lenet_300_100(input_shape, num_classes)\n",
    "model.summary()\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Pruned_LeNet-300-100\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_dense_3  (None, 1, 300)           470702    \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_4  (None, 1, 100)           60102     \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_5  (None, 1, 10)            2012      \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 532,816\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 266,206\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(784, 300) dtype=float32, numpy=\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(300,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(300, 100) dtype=float32, numpy=\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(100, 10) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(10,) dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_model: keras.Model = create_masked_nn(create_pruned_lenet, input_shape, num_classes, pruning_parameters)\n",
    "mask_model.summary()\n",
    "mask_model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.397225, Accuracy: 0.0663\n"
     ]
    }
   ],
   "source": [
    "# Test single step of training\n",
    "\n",
    "# Define the optimizer outside of the function\n",
    "optimizer = C.OPTIMIZER()\n",
    "\n",
    "# Create model and masked model\n",
    "model: keras.Model = create_lenet_300_100(input_shape, num_classes)\n",
    "mask_model: keras.Model = create_masked_nn(create_pruned_lenet, input_shape, num_classes, pruning_parameters)\n",
    "\n",
    "loss, accuracy = train_one_step(model, mask_model, X_train, Y_train, optimizer)\n",
    "print(f'Loss: {loss:.6f}, Accuracy: {accuracy:.6g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 95.3614501953125\n",
      "Test Accuracy: 0.16609999537467957\n"
     ]
    }
   ],
   "source": [
    "# Try getting test accuracies with `test_step`\n",
    "test_loss: tf.keras.metrics.Metric = C.LOSS_FUNCTION()\n",
    "test_accuracy: tf.keras.metrics.Metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "loss, accuracy = test_step(model, X_test, Y_test, test_loss, test_accuracy)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2 / 2 epochs\n",
      "Ended with a best training accuracy of 35.66% and test accuracy of training accuracy of 54.87%\n"
     ]
    }
   ],
   "source": [
    "# Testing `training_loop` function\n",
    "\n",
    "# Define the optimizer outside of the function\n",
    "optimizer = C.OPTIMIZER()\n",
    "\n",
    "epochs: int = 2\n",
    "\n",
    "# Create model and masked model\n",
    "model: keras.Model = create_lenet_300_100(input_shape, num_classes)\n",
    "mask_model: keras.Model = create_masked_nn(create_pruned_lenet, input_shape, num_classes, pruning_parameters)\n",
    "\n",
    "trained_model, training_round = training_loop(0, model, mask_model, load_and_process_mnist, epochs, batch_size, optimizer=optimizer)\n",
    "print(f'Took {np.sum(training_round.test_accuracies != 0)} / {epochs} epochs')\n",
    "print(f'Ended with a best training accuracy of {np.max(training_round.train_accuracies) * 100:.2f}% and test accuracy of {np.max(training_round.test_accuracies) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping initiated\n",
      "Directory 'models/model_0/trial0/weights' already exists.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Directory 'models/model_0/trial0/masks' already exists.\n",
      "Took 11 / 30 epochs\n",
      "Ended with a best training accuracy of 55.14% and test accuracy of training accuracy of 63.33%\n",
      "Test Loss: 1.2082486152648926\n",
      "Test Accuracy: 0.4763000011444092\n"
     ]
    }
   ],
   "source": [
    "# Testing `train` function\n",
    "\n",
    "# Create model and masked model\n",
    "model: keras.Model = create_lenet_300_100(input_shape, num_classes)\n",
    "mask_model: keras.Model = create_masked_nn(create_pruned_lenet, input_shape, num_classes, pruning_parameters)\n",
    "\n",
    "trained_model, pruned_mask_model, training_round = train(0, 0, model, mask_model, load_and_process_mnist)\n",
    "\n",
    "test_accuracy: tf.keras.metrics.Metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "print(f'Took {np.sum(training_round.test_accuracies != 0)} / {C.TRAINING_EPOCHS} epochs')\n",
    "print(f'Ended with a best training accuracy of {np.max(training_round.train_accuracies) * 100:.2f}% and test accuracy of training accuracy of {np.max(training_round.test_accuracies) * 100:.2f}%')\n",
    "\n",
    "loss, accuracy = test_step(trained_model, X_test, Y_test, test_loss, test_accuracy)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Test Loss: 1.2082486152648926\n",
      "Test Accuracy: 0.4763000011444092\n"
     ]
    }
   ],
   "source": [
    "# Test loading the model back\n",
    "loaded_model: keras.Model = load_model(0, 0)\n",
    "\n",
    "loss, accuracy = test_step(loaded_model, X_test, Y_test, test_loss, test_accuracy)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero parameters after training:\n",
      "266610\n"
     ]
    }
   ],
   "source": [
    "print(f'Nonzero parameters after training:')\n",
    "print(count_params(model)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
