{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import copy\n",
    "import functools\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from src.harness import constants as C\n",
    "from src.harness import dataset as ds\n",
    "from src.harness import history\n",
    "from src.harness import experiment\n",
    "from src.harness import mixins\n",
    "from src.harness import model as mod\n",
    "from src.harness import paths\n",
    "from src.harness import pruning\n",
    "from src.harness import rewind\n",
    "from src.harness import training as train\n",
    "from src.harness import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ds)\n",
    "reload(mod)\n",
    "reload(pruning)\n",
    "\n",
    "# Select the dataset\n",
    "mnist_dataset: ds.Dataset = ds.Dataset(ds.Datasets.MNIST)\n",
    "X_train, X_test, Y_train, Y_test = mnist_dataset.load()\n",
    "input_shape: tuple = mnist_dataset.input_shape\n",
    "num_classes: int = mnist_dataset.num_classes\n",
    "\n",
    "print(f'Input Shape: {input_shape}')\n",
    "print(f'Num Classes: {num_classes}')\n",
    "print(f'X_train Shape: {X_train.shape}, Y_train Shape: {Y_train.shape}')\n",
    "print(f'X_test Shape: {X_test.shape}, Y_test Shape: {Y_test.shape}')\n",
    "\n",
    "num_epochs: int = 10\n",
    "batch_size: int = len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(ds)\n",
    "reload(mod)\n",
    "reload(utils)\n",
    "\n",
    "# Create a model with the same architecture using all Keras components to check its accuracy with the same parameters\n",
    "utils.set_seed(0)\n",
    "make_lenet: callable = functools.partial(mod.create_lenet_300_100, input_shape, num_classes)\n",
    "\n",
    "original_model: keras.Model = make_lenet()\n",
    "# original_model.summary()\n",
    "# original_model.trainable_variables\n",
    "\n",
    "original_mask_model: keras.Model = mod.create_masked_nn(make_lenet)\n",
    "original_mask_model.summary()\n",
    "# original_mask_model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Loss Function and Accuracy Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(C)\n",
    "reload(train)\n",
    "\n",
    "# Use the original model as a reference\n",
    "loss_fn: tf.keras.losses.Loss = C.LOSS_FUNCTION()\n",
    "accuracy_metric: tf.keras.metrics.Metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "test_loss, test_accuracy = train.test_step(original_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Step of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(C)\n",
    "reload(train)\n",
    "\n",
    "# Test single step of training\n",
    "\n",
    "# Define the optimizer outside of the function\n",
    "optimizer = C.OPTIMIZER()\n",
    "train_one_step: callable = train.get_train_one_step()\n",
    "accuracy_metric.reset_state()\n",
    "\n",
    "# Copy originals\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "\n",
    "# Sanity Check\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "epochs: int = 1\n",
    "batch_size: int = len(X_train)\n",
    "\n",
    "train_accuracies: np.array = np.zeros(epochs)\n",
    "test_accuracies: np.array = np.zeros(epochs)\n",
    "\n",
    "original_weights: list[np.ndarray] = [np.copy(weights) for weights in model.get_weights()]\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f'Starting Epoch {i + 1}')\n",
    "    for n in range(int(np.ceil(len(X_train) / batch_size))):\n",
    "        train_loss, train_accuracy = train_one_step(model, mask_model, X_train[n * batch_size:(n + 1) * batch_size], Y_train[n * batch_size:(n + 1) * batch_size], optimizer)\n",
    "        train_accuracies[i] = train_accuracy\n",
    "\n",
    "        test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "        test_accuracies[i] = test_accuracy\n",
    "\n",
    "        print(f'Batch {n + 1} Train Loss: {train_loss:.6f}, Train Accuracy: {train_accuracy:.6f}, Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')\n",
    "\n",
    "ending_weights: list[np.ndarray] = [np.copy(weights) for weights in model.get_weights()]\n",
    "\n",
    "print(f'Test Accuracies:')\n",
    "print(test_accuracies)\n",
    "print(f'Training Accuracies:')\n",
    "print(train_accuracies)\n",
    "\n",
    "# Get test parameters\n",
    "accuracy_metric.reset_state()\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(C)\n",
    "reload(ds)\n",
    "reload(train)\n",
    "\n",
    "# Testing `training_loop` function\n",
    "epochs: int = C.TRAINING_EPOCHS\n",
    "\n",
    "# Copy originals\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "\n",
    "# Sanity Check\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "trial_data = train.training_loop(0, model, mask_model, mnist_dataset, epochs)\n",
    "\n",
    "print(f'Took {np.sum(trial_data.test_accuracies != 0)} / {epochs} epochs')\n",
    "print(f'Ended with a best training accuracy of {np.max(trial_data.train_accuracies) * 100:.2f}% and test accuracy of {np.max(trial_data.test_accuracies) * 100:.2f}%')\n",
    "\n",
    "print(f'Test Accuracies:')\n",
    "print(trial_data.test_accuracies)\n",
    "print(f'Training Accuracies:')\n",
    "print(trial_data.train_accuracies)\n",
    "\n",
    "# Get test parameters\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing `train` function\n",
    "\n",
    "# Copy originals\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "\n",
    "# Sanity Check\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "trial_data = train.train(0, 0, model, mask_model, mnist_dataset, batch_size=C.BATCH_SIZE)\n",
    "\n",
    "print(f'\\nTook {np.sum(trial_data.test_accuracies != 0)} / {C.TRAINING_EPOCHS} epochs')\n",
    "print(f'Ended with a best training accuracy of {np.max(trial_data.train_accuracies) * 100:.2f}% and test accuracy of training accuracy of {np.max(trial_data.test_accuracies) * 100:.2f}%')\n",
    "\n",
    "print(f'Test Accuracies:')\n",
    "print(trial_data.test_accuracies)\n",
    "print(f'Training Accuracies:')\n",
    "print(trial_data.train_accuracies)\n",
    "\n",
    "# Get test parameters\n",
    "test_loss, test_accuracy = train.test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the model back\n",
    "loaded_model: keras.Model = mod.load_model(0, 0)\n",
    "\n",
    "# Get test parameters\n",
    "test_loss, test_accuracy = train.test_step(loaded_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layerwise Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pruning)\n",
    "reload(utils)\n",
    "\n",
    "# Test loading the model back\n",
    "loaded_model: keras.Model = mod.load_model(0, 0)\n",
    "target_sparsity = 0.1\n",
    "\n",
    "def test_pruning_sparsity(model: keras.Model, target_sparsity: float):\n",
    "    \"\"\"\n",
    "    Test function to verify pruning correctness.\n",
    "    NOTE: Sensitive to boundary conditions and rounding.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Keras model to copy and test pruning on.\n",
    "        target_sparsity (float): Target sparsity to test with.\n",
    "    \"\"\"\n",
    "    copy_model: keras.Model = copy.deepcopy(model)\n",
    "    \n",
    "    print(f'Test Pruning Sparsity: Target Sparsity = {target_sparsity}')\n",
    "    sparse_model = copy.deepcopy(copy_model)\n",
    "\n",
    "    total, nonzero = utils.count_total_and_nonzero_params(copy_model)\n",
    "    print(f'Before Pruning: Total Params: {total}, Nonzero Params: {nonzero}')\n",
    "    pruning.prune(sparse_model, pruning.low_magnitude_pruning, target_sparsity)\n",
    "    \n",
    "    # Add some small wiggle room for rounding- even with output being pruned at half the rate this is correct\n",
    "    error_tolerance: int = int(target_sparsity * total / 20)\n",
    "\n",
    "    pruned_total, pruned_nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    print(f'After Pruning:  Total Params: {pruned_total}, Nonzero Params: {pruned_nonzero}')\n",
    "    \n",
    "    assert pruned_total == total\n",
    "    assert np.abs(pruned_nonzero - total * target_sparsity) < error_tolerance\n",
    "\n",
    "    sparse_layer_weight_counts: list[int] = utils.count_total_and_nonzero_params_per_layer(sparse_model)\n",
    "    print(f'Layer total and nonzero weight counts: {sparse_layer_weight_counts}')\n",
    "\n",
    "    # Test that pruning worked as expected\n",
    "    for idx in range(len(sparse_layer_weight_counts))[::2]:\n",
    "        total_synapses, nonzero_synapses = sparse_layer_weight_counts[idx]\n",
    "        total_biases, nonzero_biases = sparse_layer_weight_counts[idx + 1]\n",
    "        assert np.abs((total_synapses + total_biases) * target_sparsity - nonzero_synapses + nonzero_biases) < error_tolerance\n",
    "        \n",
    "    # Test that we can prune the model to half of what it is currently at as well\n",
    "    target_sparsity /= 2\n",
    "    total, nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    pruning.prune(sparse_model, pruning.low_magnitude_pruning, target_sparsity)\n",
    "    pruned_total, pruned_nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    \n",
    "    assert np.abs(pruned_nonzero - int(total * target_sparsity)) < error_tolerance\n",
    "    \n",
    "def test_global_pruning(model: keras.Model, target_sparsity: float):\n",
    "    \"\"\"\n",
    "    Testing function to demonstrate correctness of global pruning.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Keras model to copy and test pruning on.\n",
    "        target_sparsity (float): Target sparsity to test with.\n",
    "    \"\"\"\n",
    "    target_sparsity: float = 0.1\n",
    "    \n",
    "    print(f'Test Global Pruning: Target Sparsity = {target_sparsity}')\n",
    "    \n",
    "    # Global pruning will not necessarily have equal pruning in each layer, but overall will be correct\n",
    "    sparse_model = copy.deepcopy(model)\n",
    "    pruning.prune(sparse_model, pruning.low_magnitude_pruning, target_sparsity, global_pruning=True)\n",
    "    total, nonzero = utils.count_total_and_nonzero_params(model)\n",
    "    print(f'Before Pruning: Total Params: {total}, Nonzero Params: {nonzero}')\n",
    "    \n",
    "    pruned_total, pruned_nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    print(f'After Pruning:  Total Params: {pruned_total}, Nonzero Params: {pruned_nonzero}')\n",
    "    \n",
    "    # Add some small wiggle room for rounding- even with output being pruned at half the rate this is correct\n",
    "    error_tolerance: int = int(target_sparsity * total / 20)\n",
    "    \n",
    "    pruned_total, pruned_nonzero = utils.count_total_and_nonzero_params(sparse_model)\n",
    "    assert np.abs(pruned_nonzero - total * target_sparsity) < error_tolerance\n",
    "    \n",
    "    sparse_layer_weight_counts: list[int] = utils.count_total_and_nonzero_params_per_layer(sparse_model)\n",
    "    print(f'Layer total and nonzero weight counts: {sparse_layer_weight_counts}')\n",
    "    \n",
    "print()\n",
    "test_pruning_sparsity(loaded_model, target_sparsity)\n",
    "print()\n",
    "test_global_pruning(loaded_model, target_sparsity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mod)\n",
    "reload(rewind)\n",
    "    \n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "original_weights = model.get_weights()\n",
    "\n",
    "rewind_to_original_weights: callable = functools.partial(rewind.rewind_to_original_init, 0)\n",
    "rewind_to_random_weights: callable = functools.partial(rewind.rewind_to_random_init, 0, tf.initializers.GlorotUniform())\n",
    "rewind.rewind_model_weights(model, mask_model, rewind_to_random_weights)\n",
    "\n",
    "print(original_weights[0][0][:10])\n",
    "print(model.get_weights()[0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune Low Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mod)\n",
    "reload(pruning)\n",
    "\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(original_mask_model)\n",
    "\n",
    "# Asserting that every array in the mask model's weights are 1s\n",
    "for layer in mask_model.layers:\n",
    "    for weights in layer.get_weights():\n",
    "        assert np.all(weights == 1), \"Error: Not all elements in mask model's weights are 1s after updating masks\"\n",
    "\n",
    "pruning.update_masks(model, mask_model)\n",
    "\n",
    "# Asserting that every array in the mask model's weights are still 1s\n",
    "for layer in mask_model.layers:\n",
    "    for weights in layer.get_weights():\n",
    "        assert np.all(weights == 1), \"Error: Not all elements in mask model's weights are 1s after updating masks\"\n",
    "        \n",
    "pruning.prune(model, pruning.low_magnitude_pruning, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(experiment)\n",
    "reload(pruning)\n",
    "reload(rewind)\n",
    "reload(train)\n",
    "\n",
    "# Pruning Parameters\n",
    "first_step_pruning: float = 0.2\n",
    "target_sparsity: float = 0.65\n",
    "make_lenet: callable = functools.partial(mod.create_lenet_300_100, input_shape, num_classes)\n",
    "\n",
    "global_pruning: bool = False\n",
    "sparsities: list[float] = pruning.get_sparsity_percents(model, first_step_pruning, target_sparsity)\n",
    "experiment_data: history.ExperimentData = experiment.run_iterative_pruning_experiment(\n",
    "    0, \n",
    "    make_lenet, \n",
    "    mnist_dataset,\n",
    "    sparsities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(experiment)\n",
    "reload(train)\n",
    "reload(utils)\n",
    "\n",
    "for step_index in range(len(sparsities)):\n",
    "    round = experiment_data.pruning_rounds[step_index]\n",
    "    print(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(experiment)\n",
    "reload(paths)\n",
    "reload(pruning)\n",
    "reload(rewind)\n",
    "reload(train)\n",
    "\n",
    "# Test high level experiment API\n",
    "experiment_directory: str = 'testing_experiment'\n",
    "experiment_summary: history.ExperimentSummary = experiment.run_experiments(\n",
    "    2, \n",
    "    experiment_directory,\n",
    "    functools.partial(experiment.get_lenet_300_100_experiment_parameters, ds.Datasets.MNIST, 0.2, 0.65),\n",
    "    experiment.run_iterative_pruning_experiment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained: keras.Model = mod.load_model(0, 0, initial=True)\n",
    "trained: keras.Model = mod.load_model(0, 0)\n",
    "\n",
    "print(untrained.get_weights()[0][0][:10])\n",
    "print(trained.get_weights()[0][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed, experiment_data in experiment_summary.experiments.items():\n",
    "    print(f'Model {seed}')\n",
    "    for round in experiment_data.pruning_rounds:\n",
    "        print(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_experiment_summary = history.ExperimentData.load_from(os.path.join('testing_experiment', 'experiment_summary.pkl'))\n",
    "for seed, experiment_data in loaded_experiment_summary.experiments.items():\n",
    "    print(f'Model {seed}')\n",
    "    for round in experiment_data.pruning_rounds:\n",
    "        print(round)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
