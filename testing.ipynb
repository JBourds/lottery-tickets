{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "testing.ipynb\n",
    "\n",
    "File for performing testing to implement lottery ticket experiments.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import copy\n",
    "import functools\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.sparsity.keras import ConstantSparsity, PolynomialDecay, prune_low_magnitude\n",
    "\n",
    "from src.harness.constants import Constants as C\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.experiment import ExperimentData\n",
    "from src.harness.model import create_lenet_300_100, create_masked_nn, create_pruned_lenet, save_model, load_model\n",
    "from src.harness.pruning import create_pruning_callbacks, create_pruning_parameters\n",
    "from src.harness.training import test_step, train, get_train_one_step, training_loop, TrainingRound\n",
    "from src.harness.utils import count_params, get_layer_weight_counts, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_and_process_mnist()\n",
    "\n",
    "num_epochs: int = 10\n",
    "input_shape: tuple = X_train[0].shape\n",
    "num_classes: int = 10\n",
    "batch_size: int = len(X_train)\n",
    "\n",
    "num_train_samples: int = X_train.shape[0]\n",
    "\n",
    "# Make pruning parameters\n",
    "target_sparsity: float = 0.01\n",
    "step_pruning_percent: float = 0.2\n",
    "end_step: int = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * num_epochs\n",
    "frequency: int = 1\n",
    "pruning_parameters: dict = create_pruning_parameters(target_sparsity, 0, end_step, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with the same architecture using all Keras components to check its accuracy with the same parameters\n",
    "set_seed(0)\n",
    "\n",
    "original_model: keras.Model = create_lenet_300_100(input_shape, num_classes)\n",
    "# original_model.summary()\n",
    "# original_model.trainable_variables\n",
    "\n",
    "original_mask_model: keras.Model = create_masked_nn(create_pruned_lenet, input_shape, num_classes, pruning_parameters)\n",
    "# original_mask_model.summary()\n",
    "# original_mask_model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the original model as a reference\n",
    "loss_fn: tf.keras.losses.Loss = C.LOSS_FUNCTION()\n",
    "accuracy_metric: tf.keras.metrics.Metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "test_loss, test_accuracy = test_step(original_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that deepcopy is exactly the same as the original model\n",
    "# Copy originals\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(sparsity.strip_pruning(original_mask_model))\n",
    "\n",
    "test_loss, test_accuracy = test_step(original_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test single step of training\n",
    "\n",
    "# # Define the optimizer outside of the function\n",
    "# optimizer = C.OPTIMIZER()\n",
    "# train_one_step: callable = get_train_one_step()\n",
    "# accuracy_metric.reset_states()\n",
    "\n",
    "# # Copy originals\n",
    "# model: keras.Model = copy.deepcopy(original_model)\n",
    "# mask_model: keras.Model = copy.deepcopy(sparsity.strip_pruning(original_mask_model))\n",
    "\n",
    "# # Sanity Check\n",
    "# test_loss, test_accuracy = test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "# print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "# epochs: int = C.TRAINING_EPOCHS\n",
    "\n",
    "# train_accuracies: np.array = np.zeros(epochs)\n",
    "# test_accuracies: np.array = np.zeros(epochs)\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     train_loss, train_accuracy = train_one_step(model, mask_model, X_train, Y_train, optimizer)\n",
    "#     train_accuracies[i] = train_accuracy\n",
    "\n",
    "#     test_loss, test_accuracy = test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "#     test_accuracies[i] = test_accuracy\n",
    "\n",
    "#     print(f'Iteration {i + 1} Train Loss: {train_loss:.6f}, Train Accuracy: {train_accuracy:.6f}, Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')\n",
    "\n",
    "# print(f'Test Accuracies:')\n",
    "# print(test_accuracies)\n",
    "# print(f'Training Accuracies:')\n",
    "# print(train_accuracies)\n",
    "\n",
    "# # Get test parameters\n",
    "# accuracy_metric.reset_states()\n",
    "# test_loss, test_accuracy = test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "# print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing `training_loop` function\n",
    "\n",
    "# epochs: int = C.TRAINING_EPOCHS\n",
    "\n",
    "# # Copy originals\n",
    "# model: keras.Model = copy.deepcopy(original_model)\n",
    "# mask_model: keras.Model = copy.deepcopy(sparsity.strip_pruning(original_mask_model))\n",
    "\n",
    "# # Sanity Check\n",
    "# test_loss, test_accuracy = test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "# print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "# trained_model, training_round = training_loop(0, model, mask_model, load_and_process_mnist, epochs, len(X_train))\n",
    "\n",
    "# print(f'Took {np.sum(training_round.test_accuracies != 0)} / {epochs} epochs')\n",
    "# print(f'Ended with a best training accuracy of {np.max(training_round.train_accuracies) * 100:.2f}% and test accuracy of {np.max(training_round.test_accuracies) * 100:.2f}%')\n",
    "\n",
    "# print(f'Test Accuracies:')\n",
    "# print(training_round.test_accuracies)\n",
    "# print(f'Training Accuracies:')\n",
    "# print(training_round.train_accuracies)\n",
    "\n",
    "# # Get test parameters\n",
    "# test_loss, test_accuracy = test_step(trained_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "# print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing `train` function\n",
    "\n",
    "# # Copy originals\n",
    "# model: keras.Model = copy.deepcopy(original_model)\n",
    "# mask_model: keras.Model = copy.deepcopy(sparsity.strip_pruning(original_mask_model))\n",
    "\n",
    "# # Sanity Check\n",
    "# test_loss, test_accuracy = test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "# print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "# trained_model, pruned_mask_model, training_round = train(0, 0, model, mask_model, load_and_process_mnist, batch_size=len(X_train))\n",
    "\n",
    "# print(f'\\nTook {np.sum(training_round.test_accuracies != 0)} / {C.TRAINING_EPOCHS} epochs')\n",
    "# print(f'Ended with a best training accuracy of {np.max(training_round.train_accuracies) * 100:.2f}% and test accuracy of training accuracy of {np.max(training_round.test_accuracies) * 100:.2f}%')\n",
    "\n",
    "# print(f'Test Accuracies:')\n",
    "# print(training_round.test_accuracies)\n",
    "# print(f'Training Accuracies:')\n",
    "# print(training_round.train_accuracies)\n",
    "\n",
    "# # Get test parameters\n",
    "# test_loss, test_accuracy = test_step(trained_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "# print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the model back\n",
    "loaded_model: keras.Model = load_model(0, 0)\n",
    "\n",
    "# Get test parameters\n",
    "test_loss, test_accuracy = test_step(loaded_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nonzero parameters after training but before pruning:')\n",
    "print(count_params(model)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pruning_parameters(target_sparsity: float, begin_step: int, end_step: int, frequency: int) -> dict:\n",
    "    \"\"\"\n",
    "    Create the dictionary of pruning parameters to be used.\n",
    "\n",
    "    :param target_sparsity: The target sparsity to achieve during pruning, a float value between 0 and 1.\n",
    "    :param begin_step:      The step at which pruning begins.\n",
    "    :param end_step:        The step at which pruning ends.\n",
    "    :param frequency:       The frequency with which pruning is applied, typically expressed in terms of epochs or steps.\n",
    "\n",
    "    :returns: A dictionary containing the pruning parameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'pruning_schedule': sparsity.ConstantSparsity(\n",
    "            target_sparsity=target_sparsity, \n",
    "            begin_step=begin_step,\n",
    "            end_step=end_step, \n",
    "            frequency=frequency\n",
    "        )\n",
    "    }\n",
    "\n",
    "def get_pruning_callbacks(\n",
    "    monitor: str = 'val_loss', \n",
    "    patience: int = C.PATIENCE, \n",
    "    minimum_delta: float = C.MINIMUM_DELTA\n",
    "    ) -> list:\n",
    "    \"\"\"\n",
    "    Create a callback to be performed during pruning.\n",
    "\n",
    "    :param monitor:       Metric to monitor for early stopping (e.g. 'val_loss').\n",
    "    :param patience:      Number of steps without an increase in performance before stopping.\n",
    "    :param minimum_delta: Minimum improvement required to be considered an improvement.\n",
    "\n",
    "    :returns: List of pruning callbacks.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        sparsity.UpdatePruningStep(),\n",
    "        # sparsity.PruningSummaries(log_dir = logdir, profile_batch=0),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=monitor, \n",
    "            patience=patience,\n",
    "            min_delta=minimum_delta\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# For each layer, there are synaptic connections from the previous layer and the neurons\n",
    "\n",
    "\n",
    "def get_pruning_percents(\n",
    "        layer_weight_counts: list[int], \n",
    "        first_step_pruning_percent: float,\n",
    "        target_sparsity: float\n",
    "        ) -> list[np.array]:\n",
    "    \"\"\"\n",
    "    Function to get arrays of model sparsity at each step of pruning based on a constant pruning %\n",
    "    applied to nonzer-parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def total_sparsity(\n",
    "            original_weight_counts: list[int], \n",
    "            current_weight_counts: list[int]\n",
    "            ) -> float:\n",
    "        \"\"\"\n",
    "        Helper function to calculate total sparsity of parameters.\n",
    "        \"\"\"\n",
    "        return np.sum(current_weight_counts) / np.sum(original_weight_counts)\n",
    "    \n",
    "    def sparsify(\n",
    "            original_weight_counts: list[int], \n",
    "            current_weight_counts: list[int], \n",
    "            original_pruning_percent: float\n",
    "            ) -> list[float]:\n",
    "        sparsities: list[float] = []\n",
    "        for idx, (original, current) in enumerate(zip(original_weight_counts, current_weight_counts)):\n",
    "            if original == 0:\n",
    "                continue\n",
    "            new_weight_count: int = np.round(current * (1 - original_pruning_percent))\n",
    "            sparsities.append((original - new_weight_count) / original)\n",
    "            current_weight_counts[idx] = new_weight_count\n",
    "        return np.round(np.mean(sparsities), decimals=5)\n",
    "    \n",
    "    sparsities: list[float] = []\n",
    "    \n",
    "    # Elementwise copy\n",
    "    current_weight_counts: list[int] = [weight_count for weight_count in layer_weight_counts]\n",
    "    \n",
    "    while total_sparsity(layer_weight_counts, current_weight_counts) > target_sparsity:\n",
    "        sparsities.append(sparsify(layer_weight_counts, current_weight_counts, first_step_pruning_percent))\n",
    "\n",
    "    return sparsities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_model: keras.Model = copy.deepcopy(sparsity.strip_pruning(original_mask_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing pruning\n",
    "\n",
    "# Copy originals\n",
    "model: keras.Model = copy.deepcopy(original_model)\n",
    "mask_model: keras.Model = copy.deepcopy(sparsity.strip_pruning(original_mask_model))\n",
    "\n",
    "# Sanity Check\n",
    "test_loss, test_accuracy = test_step(model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}\\n')\n",
    "\n",
    "trained_model, pruned_mask_model, training_round = train(0, 0, model, mask_model, load_and_process_mnist, batch_size=len(X_train))\n",
    "\n",
    "print(f'\\nTook {np.sum(training_round.test_accuracies != 0)} / {C.TRAINING_EPOCHS} epochs')\n",
    "print(f'Ended with a best training accuracy of {np.max(training_round.train_accuracies) * 100:.2f}% and test accuracy of training accuracy of {np.max(training_round.test_accuracies) * 100:.2f}%')\n",
    "\n",
    "print(f'Test Accuracies:')\n",
    "print(training_round.test_accuracies)\n",
    "print(f'Training Accuracies:')\n",
    "print(training_round.train_accuracies)\n",
    "\n",
    "# Get test parameters\n",
    "test_loss, test_accuracy = test_step(trained_model, X_test, Y_test, loss_fn, accuracy_metric)\n",
    "print(f'\\nTest Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callbacks, pruning params, and pruning percents\n",
    "callbacks: list[tf.keras.callbacks.Callback] = get_pruning_callbacks()\n",
    "pruning_parameters: dict = create_pruning_parameters(target_sparsity, 0, end_step, frequency)\n",
    "layer_weight_counts: list[int] = get_layer_weight_counts(trained_model)\n",
    "pruning_percents: list[float] = get_pruning_percents(layer_weight_counts, step_pruning_percent, target_sparsity)\n",
    "print(pruning_percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    random_seed: int,\n",
    "    create_model: callable,\n",
    "    create_masked_model: callable,\n",
    "    pruning_percents: np.array,\n",
    "    ) -> ExperimentData:\n",
    "  pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
