{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "main.ipynb\n",
    "\n",
    "Main file for recreating lottery ticket experiments done in randomly initialized dense neural networks.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.harness import constants as C\n",
    "from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.experiment import experiment\n",
    "from src.harness.model import create_model, LeNet300, load_model\n",
    "from src.harness.pruning import prune_by_percent\n",
    "from src.harness.training import train\n",
    "from src.lottery_ticket.foundations import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Step 0\n",
      "Iteration 1/1000, Loss: 2.3766071796417236\n",
      "Iteration 2/1000, Loss: 2.27860426902771\n",
      "Iteration 3/1000, Loss: 2.2031211853027344\n",
      "Iteration 4/1000, Loss: 2.135913848876953\n",
      "Iteration 5/1000, Loss: 2.072058916091919\n",
      "Iteration 6/1000, Loss: 2.008862018585205\n",
      "Iteration 7/1000, Loss: 1.9449796676635742\n",
      "Iteration 8/1000, Loss: 1.8799101114273071\n",
      "Iteration 9/1000, Loss: 1.8137075901031494\n",
      "Iteration 10/1000, Loss: 1.7467494010925293\n",
      "Iteration 11/1000, Loss: 1.6797231435775757\n",
      "Iteration 12/1000, Loss: 1.6132572889328003\n",
      "Iteration 13/1000, Loss: 1.5479921102523804\n",
      "Iteration 14/1000, Loss: 1.4843674898147583\n",
      "Iteration 15/1000, Loss: 1.4228434562683105\n",
      "Iteration 16/1000, Loss: 1.3637536764144897\n",
      "Iteration 17/1000, Loss: 1.3072665929794312\n",
      "Iteration 18/1000, Loss: 1.2535427808761597\n",
      "Iteration 19/1000, Loss: 1.2027026414871216\n",
      "Iteration 20/1000, Loss: 1.1547727584838867\n",
      "Iteration 21/1000, Loss: 1.1097360849380493\n",
      "Iteration 22/1000, Loss: 1.0675314664840698\n",
      "Iteration 23/1000, Loss: 1.0280914306640625\n",
      "Iteration 24/1000, Loss: 0.9913032054901123\n",
      "Iteration 25/1000, Loss: 0.9570378065109253\n",
      "Iteration 26/1000, Loss: 0.9251744747161865\n",
      "Iteration 27/1000, Loss: 0.8955658674240112\n",
      "Iteration 28/1000, Loss: 0.8680592179298401\n",
      "Iteration 29/1000, Loss: 0.8424948453903198\n",
      "Iteration 30/1000, Loss: 0.8187174201011658\n",
      "Iteration 31/1000, Loss: 0.7965861558914185\n",
      "Iteration 32/1000, Loss: 0.775969922542572\n",
      "Iteration 33/1000, Loss: 0.7567373514175415\n",
      "Iteration 34/1000, Loss: 0.7387685179710388\n",
      "Iteration 35/1000, Loss: 0.7219534516334534\n",
      "Iteration 36/1000, Loss: 0.7062023878097534\n",
      "Iteration 37/1000, Loss: 0.6914178133010864\n",
      "Iteration 38/1000, Loss: 0.6775253415107727\n",
      "Iteration 39/1000, Loss: 0.6644517183303833\n",
      "Iteration 40/1000, Loss: 0.6521284580230713\n",
      "Iteration 41/1000, Loss: 0.6404948830604553\n",
      "Iteration 42/1000, Loss: 0.6295005083084106\n",
      "Iteration 43/1000, Loss: 0.6190969944000244\n",
      "Iteration 44/1000, Loss: 0.609234094619751\n",
      "Iteration 45/1000, Loss: 0.5998691916465759\n",
      "Iteration 46/1000, Loss: 0.5909695029258728\n",
      "Iteration 47/1000, Loss: 0.5825046300888062\n",
      "Iteration 48/1000, Loss: 0.5744453072547913\n",
      "Iteration 49/1000, Loss: 0.5667624473571777\n",
      "Iteration 50/1000, Loss: 0.5594290494918823\n",
      "Iteration 51/1000, Loss: 0.552421510219574\n",
      "Iteration 52/1000, Loss: 0.5457165241241455\n",
      "Iteration 53/1000, Loss: 0.5392956733703613\n",
      "Iteration 54/1000, Loss: 0.5331432819366455\n",
      "Iteration 55/1000, Loss: 0.5272433757781982\n",
      "Iteration 56/1000, Loss: 0.5215784907341003\n",
      "Iteration 57/1000, Loss: 0.5161362886428833\n",
      "Iteration 58/1000, Loss: 0.5109038352966309\n",
      "Iteration 59/1000, Loss: 0.5058690309524536\n",
      "Iteration 60/1000, Loss: 0.5010225176811218\n",
      "Iteration 61/1000, Loss: 0.49635106325149536\n",
      "Iteration 62/1000, Loss: 0.49184492230415344\n",
      "Iteration 63/1000, Loss: 0.48749545216560364\n",
      "Iteration 64/1000, Loss: 0.4832947552204132\n",
      "Iteration 65/1000, Loss: 0.479234904050827\n",
      "Iteration 66/1000, Loss: 0.4753093719482422\n",
      "Iteration 67/1000, Loss: 0.47151049971580505\n",
      "Iteration 68/1000, Loss: 0.46783193945884705\n",
      "Iteration 69/1000, Loss: 0.464267760515213\n",
      "Iteration 70/1000, Loss: 0.460813045501709\n",
      "Iteration 71/1000, Loss: 0.4574619233608246\n",
      "Iteration 72/1000, Loss: 0.4542102813720703\n",
      "Iteration 73/1000, Loss: 0.45105409622192383\n",
      "Iteration 74/1000, Loss: 0.4479896128177643\n",
      "Iteration 75/1000, Loss: 0.44501203298568726\n",
      "Iteration 76/1000, Loss: 0.44211795926094055\n",
      "Iteration 77/1000, Loss: 0.4393039047718048\n",
      "Iteration 78/1000, Loss: 0.43656542897224426\n",
      "Iteration 79/1000, Loss: 0.43389856815338135\n",
      "Iteration 80/1000, Loss: 0.4312998652458191\n",
      "Iteration 81/1000, Loss: 0.4287673234939575\n",
      "Iteration 82/1000, Loss: 0.42629823088645935\n",
      "Iteration 83/1000, Loss: 0.4238908290863037\n",
      "Iteration 84/1000, Loss: 0.4215415418148041\n",
      "Iteration 85/1000, Loss: 0.4192483127117157\n",
      "Iteration 86/1000, Loss: 0.4170094430446625\n",
      "Iteration 87/1000, Loss: 0.4148229956626892\n",
      "Iteration 88/1000, Loss: 0.41268712282180786\n",
      "Iteration 89/1000, Loss: 0.41059938073158264\n",
      "Iteration 90/1000, Loss: 0.4085584282875061\n",
      "Iteration 91/1000, Loss: 0.4065626263618469\n",
      "Iteration 92/1000, Loss: 0.4046107232570648\n",
      "Iteration 93/1000, Loss: 0.4027007818222046\n",
      "Iteration 94/1000, Loss: 0.40083047747612\n",
      "Iteration 95/1000, Loss: 0.3989986181259155\n",
      "Iteration 96/1000, Loss: 0.39720436930656433\n",
      "Iteration 97/1000, Loss: 0.3954468071460724\n",
      "Iteration 98/1000, Loss: 0.3937247693538666\n",
      "Iteration 99/1000, Loss: 0.3920368254184723\n",
      "Iteration 100/1000, Loss: 0.39038190245628357\n",
      "Iteration 101/1000, Loss: 0.3887587785720825\n",
      "Iteration 102/1000, Loss: 0.38716620206832886\n",
      "Iteration 103/1000, Loss: 0.3856036365032196\n",
      "Iteration 104/1000, Loss: 0.38407009840011597\n",
      "Iteration 105/1000, Loss: 0.3825646936893463\n",
      "Iteration 106/1000, Loss: 0.3810863494873047\n",
      "Iteration 107/1000, Loss: 0.379634290933609\n",
      "Iteration 108/1000, Loss: 0.3782070279121399\n",
      "Iteration 109/1000, Loss: 0.37680473923683167\n",
      "Iteration 110/1000, Loss: 0.3754264712333679\n",
      "Iteration 111/1000, Loss: 0.374071329832077\n",
      "Iteration 112/1000, Loss: 0.3727388083934784\n",
      "Iteration 113/1000, Loss: 0.3714279234409332\n",
      "Iteration 114/1000, Loss: 0.37013834714889526\n",
      "Iteration 115/1000, Loss: 0.3688693940639496\n",
      "Iteration 116/1000, Loss: 0.3676202595233917\n",
      "Iteration 117/1000, Loss: 0.36639049649238586\n",
      "Iteration 118/1000, Loss: 0.3651796281337738\n",
      "Iteration 119/1000, Loss: 0.3639872074127197\n",
      "Iteration 120/1000, Loss: 0.362812876701355\n",
      "Iteration 121/1000, Loss: 0.36165621876716614\n",
      "Iteration 122/1000, Loss: 0.3605165183544159\n",
      "Iteration 123/1000, Loss: 0.35939326882362366\n",
      "Iteration 124/1000, Loss: 0.35828593373298645\n",
      "Iteration 125/1000, Loss: 0.35719433426856995\n",
      "Iteration 126/1000, Loss: 0.3561181426048279\n",
      "Iteration 127/1000, Loss: 0.3550572395324707\n",
      "Iteration 128/1000, Loss: 0.35401031374931335\n",
      "Iteration 129/1000, Loss: 0.35297757387161255\n",
      "Iteration 130/1000, Loss: 0.35195857286453247\n",
      "Iteration 131/1000, Loss: 0.35095280408859253\n",
      "Iteration 132/1000, Loss: 0.349960058927536\n",
      "Iteration 133/1000, Loss: 0.3489803373813629\n",
      "Iteration 134/1000, Loss: 0.34801334142684937\n",
      "Iteration 135/1000, Loss: 0.34705835580825806\n",
      "Iteration 136/1000, Loss: 0.3461152911186218\n",
      "Iteration 137/1000, Loss: 0.3451839089393616\n",
      "Iteration 138/1000, Loss: 0.3442639410495758\n",
      "Iteration 139/1000, Loss: 0.3433552086353302\n",
      "Iteration 140/1000, Loss: 0.34245729446411133\n",
      "Iteration 141/1000, Loss: 0.34157007932662964\n",
      "Iteration 142/1000, Loss: 0.34069350361824036\n",
      "Iteration 143/1000, Loss: 0.3398270308971405\n",
      "Iteration 144/1000, Loss: 0.3389706611633301\n",
      "Iteration 145/1000, Loss: 0.33812424540519714\n",
      "Iteration 146/1000, Loss: 0.3372873067855835\n",
      "Iteration 147/1000, Loss: 0.336459755897522\n",
      "Iteration 148/1000, Loss: 0.33564135432243347\n",
      "Iteration 149/1000, Loss: 0.3348316550254822\n",
      "Iteration 150/1000, Loss: 0.33403053879737854\n",
      "Iteration 151/1000, Loss: 0.333237886428833\n",
      "Iteration 152/1000, Loss: 0.3324533998966217\n",
      "Iteration 153/1000, Loss: 0.3316769003868103\n",
      "Iteration 154/1000, Loss: 0.3309083580970764\n",
      "Iteration 155/1000, Loss: 0.3301476836204529\n",
      "Iteration 156/1000, Loss: 0.3293948471546173\n",
      "Iteration 157/1000, Loss: 0.3286498785018921\n",
      "Iteration 158/1000, Loss: 0.3279123306274414\n",
      "Iteration 159/1000, Loss: 0.3271823525428772\n",
      "Iteration 160/1000, Loss: 0.3264593780040741\n",
      "Iteration 161/1000, Loss: 0.3257431983947754\n",
      "Iteration 162/1000, Loss: 0.32503366470336914\n",
      "Iteration 163/1000, Loss: 0.3243306875228882\n",
      "Iteration 164/1000, Loss: 0.32363438606262207\n",
      "Iteration 165/1000, Loss: 0.32294419407844543\n",
      "Iteration 166/1000, Loss: 0.32226040959358215\n",
      "Iteration 167/1000, Loss: 0.32158273458480835\n",
      "Iteration 168/1000, Loss: 0.3209109306335449\n",
      "Iteration 169/1000, Loss: 0.3202451765537262\n",
      "Iteration 170/1000, Loss: 0.3195851445198059\n",
      "Iteration 171/1000, Loss: 0.318930983543396\n",
      "Iteration 172/1000, Loss: 0.31828275322914124\n",
      "Iteration 173/1000, Loss: 0.3176402449607849\n",
      "Iteration 174/1000, Loss: 0.3170028328895569\n",
      "Iteration 175/1000, Loss: 0.3163708448410034\n",
      "Iteration 176/1000, Loss: 0.3157441318035126\n",
      "Iteration 177/1000, Loss: 0.31512296199798584\n",
      "Iteration 178/1000, Loss: 0.3145069479942322\n",
      "Iteration 179/1000, Loss: 0.31389591097831726\n",
      "Iteration 180/1000, Loss: 0.31328970193862915\n",
      "Iteration 181/1000, Loss: 0.31268829107284546\n",
      "Iteration 182/1000, Loss: 0.3120914101600647\n",
      "Iteration 183/1000, Loss: 0.31149888038635254\n",
      "Iteration 184/1000, Loss: 0.3109108805656433\n",
      "Iteration 185/1000, Loss: 0.3103274703025818\n",
      "Iteration 186/1000, Loss: 0.30974870920181274\n",
      "Iteration 187/1000, Loss: 0.30917441844940186\n",
      "Iteration 188/1000, Loss: 0.30860430002212524\n",
      "Iteration 189/1000, Loss: 0.30803820490837097\n",
      "Iteration 190/1000, Loss: 0.30747610330581665\n",
      "Iteration 191/1000, Loss: 0.3069182336330414\n",
      "Iteration 192/1000, Loss: 0.30636459589004517\n",
      "Iteration 193/1000, Loss: 0.3058147132396698\n",
      "Iteration 194/1000, Loss: 0.30526864528656006\n",
      "Iteration 195/1000, Loss: 0.3047264516353607\n",
      "Iteration 196/1000, Loss: 0.3041876554489136\n",
      "Iteration 197/1000, Loss: 0.303652286529541\n",
      "Iteration 198/1000, Loss: 0.303120493888855\n",
      "Iteration 199/1000, Loss: 0.30259233713150024\n",
      "Iteration 200/1000, Loss: 0.3020676076412201\n",
      "Iteration 201/1000, Loss: 0.30154627561569214\n",
      "Iteration 202/1000, Loss: 0.3010282516479492\n",
      "Iteration 203/1000, Loss: 0.30051344633102417\n",
      "Iteration 204/1000, Loss: 0.30000191926956177\n",
      "Iteration 205/1000, Loss: 0.29949381947517395\n",
      "Iteration 206/1000, Loss: 0.29898902773857117\n",
      "Iteration 207/1000, Loss: 0.29848742485046387\n",
      "Iteration 208/1000, Loss: 0.2979891896247864\n",
      "Iteration 209/1000, Loss: 0.29749423265457153\n",
      "Iteration 210/1000, Loss: 0.29700225591659546\n",
      "Iteration 211/1000, Loss: 0.2965134084224701\n",
      "Iteration 212/1000, Loss: 0.2960275709629059\n",
      "Iteration 213/1000, Loss: 0.2955445945262909\n",
      "Iteration 214/1000, Loss: 0.2950645685195923\n",
      "Iteration 215/1000, Loss: 0.29458755254745483\n",
      "Iteration 216/1000, Loss: 0.29411351680755615\n",
      "Iteration 217/1000, Loss: 0.2936423718929291\n",
      "Iteration 218/1000, Loss: 0.29317396879196167\n",
      "Iteration 219/1000, Loss: 0.2927081882953644\n",
      "Iteration 220/1000, Loss: 0.2922452986240387\n",
      "Iteration 221/1000, Loss: 0.291785329580307\n",
      "Iteration 222/1000, Loss: 0.29132798314094543\n",
      "Iteration 223/1000, Loss: 0.2908737361431122\n",
      "Iteration 224/1000, Loss: 0.29042208194732666\n",
      "Iteration 225/1000, Loss: 0.28997278213500977\n",
      "Iteration 226/1000, Loss: 0.28952622413635254\n",
      "Iteration 227/1000, Loss: 0.28908205032348633\n",
      "Iteration 228/1000, Loss: 0.2886402904987335\n",
      "Iteration 229/1000, Loss: 0.288200706243515\n",
      "Iteration 230/1000, Loss: 0.2877633571624756\n",
      "Iteration 231/1000, Loss: 0.2873281240463257\n",
      "Iteration 232/1000, Loss: 0.286895215511322\n",
      "Iteration 233/1000, Loss: 0.286464661359787\n",
      "Iteration 234/1000, Loss: 0.2860361933708191\n",
      "Iteration 235/1000, Loss: 0.2856099605560303\n",
      "Iteration 236/1000, Loss: 0.28518614172935486\n",
      "Iteration 237/1000, Loss: 0.2847643494606018\n",
      "Iteration 238/1000, Loss: 0.28434449434280396\n",
      "Iteration 239/1000, Loss: 0.28392669558525085\n",
      "Iteration 240/1000, Loss: 0.2835110127925873\n",
      "Iteration 241/1000, Loss: 0.28309714794158936\n",
      "Iteration 242/1000, Loss: 0.2826852798461914\n",
      "Iteration 243/1000, Loss: 0.2822754681110382\n",
      "Iteration 244/1000, Loss: 0.28186744451522827\n",
      "Iteration 245/1000, Loss: 0.2814613878726959\n",
      "Iteration 246/1000, Loss: 0.2810574173927307\n",
      "Iteration 247/1000, Loss: 0.2806554436683655\n",
      "Iteration 248/1000, Loss: 0.28025519847869873\n",
      "Iteration 249/1000, Loss: 0.2798568308353424\n",
      "Iteration 250/1000, Loss: 0.27946043014526367\n",
      "Iteration 251/1000, Loss: 0.27906590700149536\n",
      "Iteration 252/1000, Loss: 0.278672993183136\n",
      "Iteration 253/1000, Loss: 0.2782817780971527\n",
      "Iteration 254/1000, Loss: 0.27789226174354553\n",
      "Iteration 255/1000, Loss: 0.27750450372695923\n",
      "Iteration 256/1000, Loss: 0.277118444442749\n",
      "Iteration 257/1000, Loss: 0.27673399448394775\n",
      "Iteration 258/1000, Loss: 0.2763512432575226\n",
      "Iteration 259/1000, Loss: 0.27597007155418396\n",
      "Iteration 260/1000, Loss: 0.27559056878089905\n",
      "Iteration 261/1000, Loss: 0.27521273493766785\n",
      "Iteration 262/1000, Loss: 0.27483639121055603\n",
      "Iteration 263/1000, Loss: 0.2744614779949188\n",
      "Iteration 264/1000, Loss: 0.27408820390701294\n",
      "Iteration 265/1000, Loss: 0.273716539144516\n",
      "Iteration 266/1000, Loss: 0.2733463943004608\n",
      "Iteration 267/1000, Loss: 0.27297765016555786\n",
      "Iteration 268/1000, Loss: 0.2726104259490967\n",
      "Iteration 269/1000, Loss: 0.2722446620464325\n",
      "Iteration 270/1000, Loss: 0.2718803882598877\n",
      "Iteration 271/1000, Loss: 0.27151748538017273\n",
      "Iteration 272/1000, Loss: 0.27115598320961\n",
      "Iteration 273/1000, Loss: 0.2707960903644562\n",
      "Iteration 274/1000, Loss: 0.2704375982284546\n",
      "Iteration 275/1000, Loss: 0.27008047699928284\n",
      "Iteration 276/1000, Loss: 0.2697247862815857\n",
      "Iteration 277/1000, Loss: 0.26937052607536316\n",
      "Iteration 278/1000, Loss: 0.26901760697364807\n",
      "Iteration 279/1000, Loss: 0.2686660885810852\n",
      "Iteration 280/1000, Loss: 0.2683159410953522\n",
      "Iteration 281/1000, Loss: 0.2679673433303833\n",
      "Iteration 282/1000, Loss: 0.26762011647224426\n",
      "Iteration 283/1000, Loss: 0.26727423071861267\n",
      "Iteration 284/1000, Loss: 0.26692962646484375\n",
      "Iteration 285/1000, Loss: 0.2665861248970032\n",
      "Iteration 286/1000, Loss: 0.266244113445282\n",
      "Iteration 287/1000, Loss: 0.2659032642841339\n",
      "Iteration 288/1000, Loss: 0.26556360721588135\n",
      "Iteration 289/1000, Loss: 0.265225350856781\n",
      "Iteration 290/1000, Loss: 0.26488831639289856\n",
      "Iteration 291/1000, Loss: 0.2645525634288788\n",
      "Iteration 292/1000, Loss: 0.2642180323600769\n",
      "Iteration 293/1000, Loss: 0.26388469338417053\n",
      "Iteration 294/1000, Loss: 0.26355260610580444\n",
      "Iteration 295/1000, Loss: 0.26322177052497864\n",
      "Iteration 296/1000, Loss: 0.2628922164440155\n",
      "Iteration 297/1000, Loss: 0.26256367564201355\n",
      "Iteration 298/1000, Loss: 0.26223617792129517\n",
      "Iteration 299/1000, Loss: 0.2619098126888275\n",
      "Iteration 300/1000, Loss: 0.26158463954925537\n",
      "Iteration 301/1000, Loss: 0.2612605094909668\n",
      "Iteration 302/1000, Loss: 0.26093754172325134\n",
      "Iteration 303/1000, Loss: 0.26061585545539856\n",
      "Iteration 304/1000, Loss: 0.26029542088508606\n",
      "Iteration 305/1000, Loss: 0.25997596979141235\n",
      "Iteration 306/1000, Loss: 0.25965774059295654\n",
      "Iteration 307/1000, Loss: 0.259340763092041\n",
      "Iteration 308/1000, Loss: 0.25902488827705383\n",
      "Iteration 309/1000, Loss: 0.2587101459503174\n",
      "Iteration 310/1000, Loss: 0.2583964765071869\n",
      "Iteration 311/1000, Loss: 0.25808385014533997\n",
      "Iteration 312/1000, Loss: 0.257772296667099\n",
      "Iteration 313/1000, Loss: 0.2574616074562073\n",
      "Iteration 314/1000, Loss: 0.25715184211730957\n",
      "Iteration 315/1000, Loss: 0.25684311985969543\n",
      "Iteration 316/1000, Loss: 0.25653544068336487\n",
      "Iteration 317/1000, Loss: 0.2562286853790283\n",
      "Iteration 318/1000, Loss: 0.25592291355133057\n",
      "Iteration 319/1000, Loss: 0.2556180953979492\n",
      "Iteration 320/1000, Loss: 0.2553141713142395\n",
      "Iteration 321/1000, Loss: 0.25501102209091187\n",
      "Iteration 322/1000, Loss: 0.2547086477279663\n",
      "Iteration 323/1000, Loss: 0.2544070780277252\n",
      "Iteration 324/1000, Loss: 0.2541062533855438\n",
      "Iteration 325/1000, Loss: 0.2538064122200012\n",
      "Iteration 326/1000, Loss: 0.2535075545310974\n",
      "Iteration 327/1000, Loss: 0.25320965051651\n",
      "Iteration 328/1000, Loss: 0.25291281938552856\n",
      "Iteration 329/1000, Loss: 0.2526170611381531\n",
      "Iteration 330/1000, Loss: 0.2523222863674164\n",
      "Iteration 331/1000, Loss: 0.2520284056663513\n",
      "Iteration 332/1000, Loss: 0.2517354190349579\n",
      "Iteration 333/1000, Loss: 0.2514432370662689\n",
      "Iteration 334/1000, Loss: 0.2511518597602844\n",
      "Iteration 335/1000, Loss: 0.25086134672164917\n",
      "Iteration 336/1000, Loss: 0.250571608543396\n",
      "Iteration 337/1000, Loss: 0.25028282403945923\n",
      "Iteration 338/1000, Loss: 0.24999487400054932\n",
      "Iteration 339/1000, Loss: 0.2497076839208603\n",
      "Iteration 340/1000, Loss: 0.24942128360271454\n",
      "Iteration 341/1000, Loss: 0.24913561344146729\n",
      "Iteration 342/1000, Loss: 0.24885068833827972\n",
      "Iteration 343/1000, Loss: 0.248566672205925\n",
      "Iteration 344/1000, Loss: 0.24828343093395233\n",
      "Iteration 345/1000, Loss: 0.24800096452236176\n",
      "Iteration 346/1000, Loss: 0.24771930277347565\n",
      "Iteration 347/1000, Loss: 0.24743841588497162\n",
      "Iteration 348/1000, Loss: 0.24715827405452728\n",
      "Iteration 349/1000, Loss: 0.24687893688678741\n",
      "Iteration 350/1000, Loss: 0.24660037457942963\n",
      "Iteration 351/1000, Loss: 0.24632270634174347\n",
      "Iteration 352/1000, Loss: 0.24604591727256775\n",
      "Iteration 353/1000, Loss: 0.24576987326145172\n",
      "Iteration 354/1000, Loss: 0.24549466371536255\n",
      "Iteration 355/1000, Loss: 0.24522018432617188\n",
      "Iteration 356/1000, Loss: 0.2449464499950409\n",
      "Iteration 357/1000, Loss: 0.2446734756231308\n",
      "Iteration 358/1000, Loss: 0.24440114200115204\n",
      "Iteration 359/1000, Loss: 0.24412962794303894\n",
      "Iteration 360/1000, Loss: 0.24385885894298553\n",
      "Iteration 361/1000, Loss: 0.24358873069286346\n",
      "Iteration 362/1000, Loss: 0.24331939220428467\n",
      "Iteration 363/1000, Loss: 0.24305087327957153\n",
      "Iteration 364/1000, Loss: 0.2427830994129181\n",
      "Iteration 365/1000, Loss: 0.24251601099967957\n",
      "Iteration 366/1000, Loss: 0.24224962294101715\n",
      "Iteration 367/1000, Loss: 0.24198397994041443\n",
      "Iteration 368/1000, Loss: 0.24171897768974304\n",
      "Iteration 369/1000, Loss: 0.2414545863866806\n",
      "Iteration 370/1000, Loss: 0.24119092524051666\n",
      "Iteration 371/1000, Loss: 0.24092793464660645\n",
      "Iteration 372/1000, Loss: 0.24066565930843353\n",
      "Iteration 373/1000, Loss: 0.24040406942367554\n",
      "Iteration 374/1000, Loss: 0.24014322459697723\n",
      "Iteration 375/1000, Loss: 0.23988305032253265\n",
      "Iteration 376/1000, Loss: 0.23962347209453583\n",
      "Iteration 377/1000, Loss: 0.23936450481414795\n",
      "Iteration 378/1000, Loss: 0.23910602927207947\n",
      "Iteration 379/1000, Loss: 0.23884820938110352\n",
      "Iteration 380/1000, Loss: 0.2385910302400589\n",
      "Iteration 381/1000, Loss: 0.2383345067501068\n",
      "Iteration 382/1000, Loss: 0.23807868361473083\n",
      "Iteration 383/1000, Loss: 0.23782341182231903\n",
      "Iteration 384/1000, Loss: 0.2375686913728714\n",
      "Iteration 385/1000, Loss: 0.2373146116733551\n",
      "Iteration 386/1000, Loss: 0.23706132173538208\n",
      "Iteration 387/1000, Loss: 0.23680871725082397\n",
      "Iteration 388/1000, Loss: 0.236556738615036\n",
      "Iteration 389/1000, Loss: 0.2363053560256958\n",
      "Iteration 390/1000, Loss: 0.23605462908744812\n",
      "Iteration 391/1000, Loss: 0.2358044534921646\n",
      "Iteration 392/1000, Loss: 0.23555484414100647\n",
      "Iteration 393/1000, Loss: 0.23530584573745728\n",
      "Iteration 394/1000, Loss: 0.23505735397338867\n",
      "Iteration 395/1000, Loss: 0.23480944335460663\n",
      "Iteration 396/1000, Loss: 0.23456208407878876\n",
      "Iteration 397/1000, Loss: 0.23431532084941864\n",
      "Iteration 398/1000, Loss: 0.23406913876533508\n",
      "Iteration 399/1000, Loss: 0.23382356762886047\n",
      "Iteration 400/1000, Loss: 0.23357853293418884\n",
      "Iteration 401/1000, Loss: 0.23333410918712616\n",
      "Iteration 402/1000, Loss: 0.23309029638767242\n",
      "Iteration 403/1000, Loss: 0.23284700512886047\n",
      "Iteration 404/1000, Loss: 0.2326042652130127\n",
      "Iteration 405/1000, Loss: 0.23236200213432312\n",
      "Iteration 406/1000, Loss: 0.23212029039859772\n",
      "Iteration 407/1000, Loss: 0.23187920451164246\n",
      "Iteration 408/1000, Loss: 0.23163872957229614\n",
      "Iteration 409/1000, Loss: 0.23139871656894684\n",
      "Iteration 410/1000, Loss: 0.23115919530391693\n",
      "Iteration 411/1000, Loss: 0.23092031478881836\n",
      "Iteration 412/1000, Loss: 0.23068197071552277\n",
      "Iteration 413/1000, Loss: 0.23044420778751373\n",
      "Iteration 414/1000, Loss: 0.23020713031291962\n",
      "Iteration 415/1000, Loss: 0.22997063398361206\n",
      "Iteration 416/1000, Loss: 0.22973467409610748\n",
      "Iteration 417/1000, Loss: 0.2294991910457611\n",
      "Iteration 418/1000, Loss: 0.2292642444372177\n",
      "Iteration 419/1000, Loss: 0.22902987897396088\n",
      "Iteration 420/1000, Loss: 0.2287960946559906\n",
      "Iteration 421/1000, Loss: 0.22856278717517853\n",
      "Iteration 422/1000, Loss: 0.22833004593849182\n",
      "Iteration 423/1000, Loss: 0.22809788584709167\n",
      "Iteration 424/1000, Loss: 0.22786614298820496\n",
      "Iteration 425/1000, Loss: 0.2276349663734436\n",
      "Iteration 426/1000, Loss: 0.22740446031093597\n",
      "Iteration 427/1000, Loss: 0.2271745800971985\n",
      "Iteration 428/1000, Loss: 0.22694514691829681\n",
      "Iteration 429/1000, Loss: 0.2267163246870041\n",
      "Iteration 430/1000, Loss: 0.22648808360099792\n",
      "Iteration 431/1000, Loss: 0.22626027464866638\n",
      "Iteration 432/1000, Loss: 0.22603294253349304\n",
      "Iteration 433/1000, Loss: 0.2258061021566391\n",
      "Iteration 434/1000, Loss: 0.22557958960533142\n",
      "Iteration 435/1000, Loss: 0.22535350918769836\n",
      "Iteration 436/1000, Loss: 0.22512783110141754\n",
      "Iteration 437/1000, Loss: 0.22490260004997253\n",
      "Iteration 438/1000, Loss: 0.22467796504497528\n",
      "Iteration 439/1000, Loss: 0.2244539111852646\n",
      "Iteration 440/1000, Loss: 0.22423028945922852\n",
      "Iteration 441/1000, Loss: 0.22400718927383423\n",
      "Iteration 442/1000, Loss: 0.22378456592559814\n",
      "Iteration 443/1000, Loss: 0.2235623300075531\n",
      "Iteration 444/1000, Loss: 0.22334063053131104\n",
      "Iteration 445/1000, Loss: 0.22311940789222717\n",
      "Iteration 446/1000, Loss: 0.22289860248565674\n",
      "Iteration 447/1000, Loss: 0.2226780205965042\n",
      "Iteration 448/1000, Loss: 0.22245807945728302\n",
      "Iteration 449/1000, Loss: 0.22223860025405884\n",
      "Iteration 450/1000, Loss: 0.22201953828334808\n",
      "Iteration 451/1000, Loss: 0.2218007743358612\n",
      "Iteration 452/1000, Loss: 0.22158251702785492\n",
      "Iteration 453/1000, Loss: 0.22136475145816803\n",
      "Iteration 454/1000, Loss: 0.22114749252796173\n",
      "Iteration 455/1000, Loss: 0.22093066573143005\n",
      "Iteration 456/1000, Loss: 0.22071416676044464\n",
      "Iteration 457/1000, Loss: 0.22049826383590698\n",
      "Iteration 458/1000, Loss: 0.22028277814388275\n",
      "Iteration 459/1000, Loss: 0.22006773948669434\n",
      "Iteration 460/1000, Loss: 0.2198532074689865\n",
      "Iteration 461/1000, Loss: 0.2196391224861145\n",
      "Iteration 462/1000, Loss: 0.21942542493343353\n",
      "Iteration 463/1000, Loss: 0.21921202540397644\n",
      "Iteration 464/1000, Loss: 0.2189992219209671\n",
      "Iteration 465/1000, Loss: 0.21878685057163239\n",
      "Iteration 466/1000, Loss: 0.21857495605945587\n",
      "Iteration 467/1000, Loss: 0.21836347877979279\n",
      "Iteration 468/1000, Loss: 0.21815229952335358\n",
      "Iteration 469/1000, Loss: 0.21794147789478302\n",
      "Iteration 470/1000, Loss: 0.21773095428943634\n",
      "Iteration 471/1000, Loss: 0.21752086281776428\n",
      "Iteration 472/1000, Loss: 0.21731120347976685\n",
      "Iteration 473/1000, Loss: 0.21710188686847687\n",
      "Iteration 474/1000, Loss: 0.2168930172920227\n",
      "Iteration 475/1000, Loss: 0.21668456494808197\n",
      "Iteration 476/1000, Loss: 0.21647663414478302\n",
      "Iteration 477/1000, Loss: 0.2162691056728363\n",
      "Iteration 478/1000, Loss: 0.21606197953224182\n",
      "Iteration 479/1000, Loss: 0.21585531532764435\n",
      "Iteration 480/1000, Loss: 0.2156490832567215\n",
      "Iteration 481/1000, Loss: 0.21544314920902252\n",
      "Iteration 482/1000, Loss: 0.2152375429868698\n",
      "Iteration 483/1000, Loss: 0.21503232419490814\n",
      "Iteration 484/1000, Loss: 0.21482741832733154\n",
      "Iteration 485/1000, Loss: 0.21462294459342957\n",
      "Iteration 486/1000, Loss: 0.2144189476966858\n",
      "Iteration 487/1000, Loss: 0.21421536803245544\n",
      "Iteration 488/1000, Loss: 0.21401217579841614\n",
      "Iteration 489/1000, Loss: 0.21380944550037384\n",
      "Iteration 490/1000, Loss: 0.21360701322555542\n",
      "Iteration 491/1000, Loss: 0.21340499818325043\n",
      "Iteration 492/1000, Loss: 0.2132035493850708\n",
      "Iteration 493/1000, Loss: 0.21300263702869415\n",
      "Iteration 494/1000, Loss: 0.21280202269554138\n",
      "Iteration 495/1000, Loss: 0.21260185539722443\n",
      "Iteration 496/1000, Loss: 0.2124020904302597\n",
      "Iteration 497/1000, Loss: 0.2122027724981308\n",
      "Iteration 498/1000, Loss: 0.21200384199619293\n",
      "Iteration 499/1000, Loss: 0.21180523931980133\n",
      "Iteration 500/1000, Loss: 0.21160700917243958\n",
      "Iteration 501/1000, Loss: 0.21140916645526886\n",
      "Iteration 502/1000, Loss: 0.21121178567409515\n",
      "Iteration 503/1000, Loss: 0.21101468801498413\n",
      "Iteration 504/1000, Loss: 0.2108178436756134\n",
      "Iteration 505/1000, Loss: 0.21062137186527252\n",
      "Iteration 506/1000, Loss: 0.21042536199092865\n",
      "Iteration 507/1000, Loss: 0.21022969484329224\n",
      "Iteration 508/1000, Loss: 0.21003440022468567\n",
      "Iteration 509/1000, Loss: 0.2098393589258194\n",
      "Iteration 510/1000, Loss: 0.2096446007490158\n",
      "Iteration 511/1000, Loss: 0.20945024490356445\n",
      "Iteration 512/1000, Loss: 0.20925618708133698\n",
      "Iteration 513/1000, Loss: 0.2090623825788498\n",
      "Iteration 514/1000, Loss: 0.20886898040771484\n",
      "Iteration 515/1000, Loss: 0.2086760252714157\n",
      "Iteration 516/1000, Loss: 0.2084835022687912\n",
      "Iteration 517/1000, Loss: 0.20829127728939056\n",
      "Iteration 518/1000, Loss: 0.20809951424598694\n",
      "Iteration 519/1000, Loss: 0.20790816843509674\n",
      "Iteration 520/1000, Loss: 0.207717165350914\n",
      "Iteration 521/1000, Loss: 0.20752646028995514\n",
      "Iteration 522/1000, Loss: 0.20733612775802612\n",
      "Iteration 523/1000, Loss: 0.20714609324932098\n",
      "Iteration 524/1000, Loss: 0.20695644617080688\n",
      "Iteration 525/1000, Loss: 0.20676733553409576\n",
      "Iteration 526/1000, Loss: 0.2065785676240921\n",
      "Iteration 527/1000, Loss: 0.2063901424407959\n",
      "Iteration 528/1000, Loss: 0.20620214939117432\n",
      "Iteration 529/1000, Loss: 0.20601454377174377\n",
      "Iteration 530/1000, Loss: 0.20582737028598785\n",
      "Iteration 531/1000, Loss: 0.2056405246257782\n",
      "Iteration 532/1000, Loss: 0.2054540365934372\n",
      "Iteration 533/1000, Loss: 0.20526781678199768\n",
      "Iteration 534/1000, Loss: 0.205081969499588\n",
      "Iteration 535/1000, Loss: 0.2048964649438858\n",
      "Iteration 536/1000, Loss: 0.20471128821372986\n",
      "Iteration 537/1000, Loss: 0.20452654361724854\n",
      "Iteration 538/1000, Loss: 0.20434218645095825\n",
      "Iteration 539/1000, Loss: 0.20415805280208588\n",
      "Iteration 540/1000, Loss: 0.20397433638572693\n",
      "Iteration 541/1000, Loss: 0.20379094779491425\n",
      "Iteration 542/1000, Loss: 0.20360787212848663\n",
      "Iteration 543/1000, Loss: 0.20342518389225006\n",
      "Iteration 544/1000, Loss: 0.20324280858039856\n",
      "Iteration 545/1000, Loss: 0.20306085050106049\n",
      "Iteration 546/1000, Loss: 0.20287929475307465\n",
      "Iteration 547/1000, Loss: 0.20269809663295746\n",
      "Iteration 548/1000, Loss: 0.20251724123954773\n",
      "Iteration 549/1000, Loss: 0.2023366391658783\n",
      "Iteration 550/1000, Loss: 0.20215635001659393\n",
      "Iteration 551/1000, Loss: 0.20197640359401703\n",
      "Iteration 552/1000, Loss: 0.20179681479930878\n",
      "Iteration 553/1000, Loss: 0.20161756873130798\n",
      "Iteration 554/1000, Loss: 0.20143862068653107\n",
      "Iteration 555/1000, Loss: 0.201260045170784\n",
      "Iteration 556/1000, Loss: 0.2010817676782608\n",
      "Iteration 557/1000, Loss: 0.20090390741825104\n",
      "Iteration 558/1000, Loss: 0.2007264345884323\n",
      "Iteration 559/1000, Loss: 0.20054928958415985\n",
      "Iteration 560/1000, Loss: 0.20037253201007843\n",
      "Iteration 561/1000, Loss: 0.20019613206386566\n",
      "Iteration 562/1000, Loss: 0.20002011954784393\n",
      "Iteration 563/1000, Loss: 0.19984446465969086\n",
      "Iteration 564/1000, Loss: 0.19966916739940643\n",
      "Iteration 565/1000, Loss: 0.1994941681623459\n",
      "Iteration 566/1000, Loss: 0.19931966066360474\n",
      "Iteration 567/1000, Loss: 0.19914554059505463\n",
      "Iteration 568/1000, Loss: 0.19897174835205078\n",
      "Iteration 569/1000, Loss: 0.1987983137369156\n",
      "Iteration 570/1000, Loss: 0.19862523674964905\n",
      "Iteration 571/1000, Loss: 0.19845250248908997\n",
      "Iteration 572/1000, Loss: 0.19828011095523834\n",
      "Iteration 573/1000, Loss: 0.1981079876422882\n",
      "Iteration 574/1000, Loss: 0.19793617725372314\n",
      "Iteration 575/1000, Loss: 0.19776467978954315\n",
      "Iteration 576/1000, Loss: 0.19759342074394226\n",
      "Iteration 577/1000, Loss: 0.19742247462272644\n",
      "Iteration 578/1000, Loss: 0.19725175201892853\n",
      "Iteration 579/1000, Loss: 0.19708144664764404\n",
      "Iteration 580/1000, Loss: 0.19691136479377747\n",
      "Iteration 581/1000, Loss: 0.19674155116081238\n",
      "Iteration 582/1000, Loss: 0.19657206535339355\n",
      "Iteration 583/1000, Loss: 0.196402907371521\n",
      "Iteration 584/1000, Loss: 0.19623415172100067\n",
      "Iteration 585/1000, Loss: 0.19606567919254303\n",
      "Iteration 586/1000, Loss: 0.19589751958847046\n",
      "Iteration 587/1000, Loss: 0.19572970271110535\n",
      "Iteration 588/1000, Loss: 0.19556225836277008\n",
      "Iteration 589/1000, Loss: 0.1953950822353363\n",
      "Iteration 590/1000, Loss: 0.1952282339334488\n",
      "Iteration 591/1000, Loss: 0.19506171345710754\n",
      "Iteration 592/1000, Loss: 0.19489537179470062\n",
      "Iteration 593/1000, Loss: 0.19472932815551758\n",
      "Iteration 594/1000, Loss: 0.19456364214420319\n",
      "Iteration 595/1000, Loss: 0.19439825415611267\n",
      "Iteration 596/1000, Loss: 0.19423314929008484\n",
      "Iteration 597/1000, Loss: 0.1940682828426361\n",
      "Iteration 598/1000, Loss: 0.19390356540679932\n",
      "Iteration 599/1000, Loss: 0.19373919069766998\n",
      "Iteration 600/1000, Loss: 0.19357509911060333\n",
      "Iteration 601/1000, Loss: 0.19341130554676056\n",
      "Iteration 602/1000, Loss: 0.19324778020381927\n",
      "Iteration 603/1000, Loss: 0.1930845081806183\n",
      "Iteration 604/1000, Loss: 0.19292140007019043\n",
      "Iteration 605/1000, Loss: 0.1927585005760193\n",
      "Iteration 606/1000, Loss: 0.19259591400623322\n",
      "Iteration 607/1000, Loss: 0.19243358075618744\n",
      "Iteration 608/1000, Loss: 0.1922713816165924\n",
      "Iteration 609/1000, Loss: 0.19210955500602722\n",
      "Iteration 610/1000, Loss: 0.19194799661636353\n",
      "Iteration 611/1000, Loss: 0.1917867213487625\n",
      "Iteration 612/1000, Loss: 0.191625714302063\n",
      "Iteration 613/1000, Loss: 0.19146506488323212\n",
      "Iteration 614/1000, Loss: 0.19130459427833557\n",
      "Iteration 615/1000, Loss: 0.19114434719085693\n",
      "Iteration 616/1000, Loss: 0.19098439812660217\n",
      "Iteration 617/1000, Loss: 0.19082464277744293\n",
      "Iteration 618/1000, Loss: 0.190665140748024\n",
      "Iteration 619/1000, Loss: 0.19050583243370056\n",
      "Iteration 620/1000, Loss: 0.19034671783447266\n",
      "Iteration 621/1000, Loss: 0.19018791615962982\n",
      "Iteration 622/1000, Loss: 0.19002950191497803\n",
      "Iteration 623/1000, Loss: 0.1898713856935501\n",
      "Iteration 624/1000, Loss: 0.18971356749534607\n",
      "Iteration 625/1000, Loss: 0.18955601751804352\n",
      "Iteration 626/1000, Loss: 0.1893986612558365\n",
      "Iteration 627/1000, Loss: 0.18924155831336975\n",
      "Iteration 628/1000, Loss: 0.18908454477787018\n",
      "Iteration 629/1000, Loss: 0.18892785906791687\n",
      "Iteration 630/1000, Loss: 0.18877138197422028\n",
      "Iteration 631/1000, Loss: 0.18861517310142517\n",
      "Iteration 632/1000, Loss: 0.18845905363559723\n",
      "Iteration 633/1000, Loss: 0.1883031725883484\n",
      "Iteration 634/1000, Loss: 0.188147634267807\n",
      "Iteration 635/1000, Loss: 0.1879924088716507\n",
      "Iteration 636/1000, Loss: 0.18783755600452423\n",
      "Iteration 637/1000, Loss: 0.18768300116062164\n",
      "Iteration 638/1000, Loss: 0.18752874433994293\n",
      "Iteration 639/1000, Loss: 0.1873747706413269\n",
      "Iteration 640/1000, Loss: 0.18722113966941833\n",
      "Iteration 641/1000, Loss: 0.18706776201725006\n",
      "Iteration 642/1000, Loss: 0.18691465258598328\n",
      "Iteration 643/1000, Loss: 0.18676182627677917\n",
      "Iteration 644/1000, Loss: 0.18660928308963776\n",
      "Iteration 645/1000, Loss: 0.18645703792572021\n",
      "Iteration 646/1000, Loss: 0.18630509078502655\n",
      "Iteration 647/1000, Loss: 0.1861533224582672\n",
      "Iteration 648/1000, Loss: 0.18600165843963623\n",
      "Iteration 649/1000, Loss: 0.18585018813610077\n",
      "Iteration 650/1000, Loss: 0.1856989860534668\n",
      "Iteration 651/1000, Loss: 0.1855481117963791\n",
      "Iteration 652/1000, Loss: 0.1853974610567093\n",
      "Iteration 653/1000, Loss: 0.1852470338344574\n",
      "Iteration 654/1000, Loss: 0.1850968599319458\n",
      "Iteration 655/1000, Loss: 0.1849469691514969\n",
      "Iteration 656/1000, Loss: 0.18479736149311066\n",
      "Iteration 657/1000, Loss: 0.1846480518579483\n",
      "Iteration 658/1000, Loss: 0.18449892103672028\n",
      "Iteration 659/1000, Loss: 0.18434996902942657\n",
      "Iteration 660/1000, Loss: 0.1842012256383896\n",
      "Iteration 661/1000, Loss: 0.18405264616012573\n",
      "Iteration 662/1000, Loss: 0.18390439450740814\n",
      "Iteration 663/1000, Loss: 0.18375644087791443\n",
      "Iteration 664/1000, Loss: 0.18360872566699982\n",
      "Iteration 665/1000, Loss: 0.18346132338047028\n",
      "Iteration 666/1000, Loss: 0.18331420421600342\n",
      "Iteration 667/1000, Loss: 0.18316738307476044\n",
      "Iteration 668/1000, Loss: 0.18302077054977417\n",
      "Iteration 669/1000, Loss: 0.18287435173988342\n",
      "Iteration 670/1000, Loss: 0.1827281266450882\n",
      "Iteration 671/1000, Loss: 0.18258212506771088\n",
      "Iteration 672/1000, Loss: 0.18243640661239624\n",
      "Iteration 673/1000, Loss: 0.18229088187217712\n",
      "Iteration 674/1000, Loss: 0.18214552104473114\n",
      "Iteration 675/1000, Loss: 0.18200038373470306\n",
      "Iteration 676/1000, Loss: 0.18185549974441528\n",
      "Iteration 677/1000, Loss: 0.18171080946922302\n",
      "Iteration 678/1000, Loss: 0.1815662384033203\n",
      "Iteration 679/1000, Loss: 0.18142187595367432\n",
      "Iteration 680/1000, Loss: 0.1812777817249298\n",
      "Iteration 681/1000, Loss: 0.1811339408159256\n",
      "Iteration 682/1000, Loss: 0.18099036812782288\n",
      "Iteration 683/1000, Loss: 0.18084706366062164\n",
      "Iteration 684/1000, Loss: 0.18070398271083832\n",
      "Iteration 685/1000, Loss: 0.18056108057498932\n",
      "Iteration 686/1000, Loss: 0.18041843175888062\n",
      "Iteration 687/1000, Loss: 0.1802760362625122\n",
      "Iteration 688/1000, Loss: 0.1801338493824005\n",
      "Iteration 689/1000, Loss: 0.1799919605255127\n",
      "Iteration 690/1000, Loss: 0.17985022068023682\n",
      "Iteration 691/1000, Loss: 0.17970868945121765\n",
      "Iteration 692/1000, Loss: 0.17956732213497162\n",
      "Iteration 693/1000, Loss: 0.17942604422569275\n",
      "Iteration 694/1000, Loss: 0.179284930229187\n",
      "Iteration 695/1000, Loss: 0.17914411425590515\n",
      "Iteration 696/1000, Loss: 0.17900347709655762\n",
      "Iteration 697/1000, Loss: 0.17886310815811157\n",
      "Iteration 698/1000, Loss: 0.17872296273708344\n",
      "Iteration 699/1000, Loss: 0.17858301103115082\n",
      "Iteration 700/1000, Loss: 0.17844322323799133\n",
      "Iteration 701/1000, Loss: 0.17830365896224976\n",
      "Iteration 702/1000, Loss: 0.1781642884016037\n",
      "Iteration 703/1000, Loss: 0.17802514135837555\n",
      "Iteration 704/1000, Loss: 0.17788614332675934\n",
      "Iteration 705/1000, Loss: 0.1777474284172058\n",
      "Iteration 706/1000, Loss: 0.17760895192623138\n",
      "Iteration 707/1000, Loss: 0.17747077345848083\n",
      "Iteration 708/1000, Loss: 0.17733275890350342\n",
      "Iteration 709/1000, Loss: 0.1771950125694275\n",
      "Iteration 710/1000, Loss: 0.1770574152469635\n",
      "Iteration 711/1000, Loss: 0.176920086145401\n",
      "Iteration 712/1000, Loss: 0.17678296566009521\n",
      "Iteration 713/1000, Loss: 0.17664597928524017\n",
      "Iteration 714/1000, Loss: 0.17650914192199707\n",
      "Iteration 715/1000, Loss: 0.17637257277965546\n",
      "Iteration 716/1000, Loss: 0.17623619735240936\n",
      "Iteration 717/1000, Loss: 0.17610013484954834\n",
      "Iteration 718/1000, Loss: 0.17596422135829926\n",
      "Iteration 719/1000, Loss: 0.17582851648330688\n",
      "Iteration 720/1000, Loss: 0.17569300532341003\n",
      "Iteration 721/1000, Loss: 0.1755577176809311\n",
      "Iteration 722/1000, Loss: 0.17542268335819244\n",
      "Iteration 723/1000, Loss: 0.1752878874540329\n",
      "Iteration 724/1000, Loss: 0.17515328526496887\n",
      "Iteration 725/1000, Loss: 0.1750188171863556\n",
      "Iteration 726/1000, Loss: 0.17488449811935425\n",
      "Iteration 727/1000, Loss: 0.1747504621744156\n",
      "Iteration 728/1000, Loss: 0.17461666464805603\n",
      "Iteration 729/1000, Loss: 0.174483060836792\n",
      "Iteration 730/1000, Loss: 0.17434974014759064\n",
      "Iteration 731/1000, Loss: 0.17421673238277435\n",
      "Iteration 732/1000, Loss: 0.17408400774002075\n",
      "Iteration 733/1000, Loss: 0.17395149171352386\n",
      "Iteration 734/1000, Loss: 0.17381927371025085\n",
      "Iteration 735/1000, Loss: 0.17368723452091217\n",
      "Iteration 736/1000, Loss: 0.1735554039478302\n",
      "Iteration 737/1000, Loss: 0.17342376708984375\n",
      "Iteration 738/1000, Loss: 0.173292338848114\n",
      "Iteration 739/1000, Loss: 0.1731610745191574\n",
      "Iteration 740/1000, Loss: 0.17302997410297394\n",
      "Iteration 741/1000, Loss: 0.17289914190769196\n",
      "Iteration 742/1000, Loss: 0.1727684885263443\n",
      "Iteration 743/1000, Loss: 0.1726379096508026\n",
      "Iteration 744/1000, Loss: 0.17250752449035645\n",
      "Iteration 745/1000, Loss: 0.17237736284732819\n",
      "Iteration 746/1000, Loss: 0.17224739491939545\n",
      "Iteration 747/1000, Loss: 0.17211757600307465\n",
      "Iteration 748/1000, Loss: 0.17198802530765533\n",
      "Iteration 749/1000, Loss: 0.17185865342617035\n",
      "Iteration 750/1000, Loss: 0.17172950506210327\n",
      "Iteration 751/1000, Loss: 0.17160052061080933\n",
      "Iteration 752/1000, Loss: 0.17147177457809448\n",
      "Iteration 753/1000, Loss: 0.17134320735931396\n",
      "Iteration 754/1000, Loss: 0.17121486365795135\n",
      "Iteration 755/1000, Loss: 0.17108669877052307\n",
      "Iteration 756/1000, Loss: 0.1709587126970291\n",
      "Iteration 757/1000, Loss: 0.17083092033863068\n",
      "Iteration 758/1000, Loss: 0.17070341110229492\n",
      "Iteration 759/1000, Loss: 0.17057611048221588\n",
      "Iteration 760/1000, Loss: 0.17044900357723236\n",
      "Iteration 761/1000, Loss: 0.17032203078269958\n",
      "Iteration 762/1000, Loss: 0.1701953262090683\n",
      "Iteration 763/1000, Loss: 0.17006875574588776\n",
      "Iteration 764/1000, Loss: 0.16994231939315796\n",
      "Iteration 765/1000, Loss: 0.16981615126132965\n",
      "Iteration 766/1000, Loss: 0.16969023644924164\n",
      "Iteration 767/1000, Loss: 0.16956456005573273\n",
      "Iteration 768/1000, Loss: 0.16943910717964172\n",
      "Iteration 769/1000, Loss: 0.16931380331516266\n",
      "Iteration 770/1000, Loss: 0.1691887378692627\n",
      "Iteration 771/1000, Loss: 0.16906382143497467\n",
      "Iteration 772/1000, Loss: 0.16893912851810455\n",
      "Iteration 773/1000, Loss: 0.16881458461284637\n",
      "Iteration 774/1000, Loss: 0.16869018971920013\n",
      "Iteration 775/1000, Loss: 0.16856598854064941\n",
      "Iteration 776/1000, Loss: 0.1684420257806778\n",
      "Iteration 777/1000, Loss: 0.16831813752651215\n",
      "Iteration 778/1000, Loss: 0.16819441318511963\n",
      "Iteration 779/1000, Loss: 0.16807086765766144\n",
      "Iteration 780/1000, Loss: 0.16794753074645996\n",
      "Iteration 781/1000, Loss: 0.16782432794570923\n",
      "Iteration 782/1000, Loss: 0.1677013337612152\n",
      "Iteration 783/1000, Loss: 0.1675785481929779\n",
      "Iteration 784/1000, Loss: 0.16745597124099731\n",
      "Iteration 785/1000, Loss: 0.16733355820178986\n",
      "Iteration 786/1000, Loss: 0.1672113835811615\n",
      "Iteration 787/1000, Loss: 0.16708938777446747\n",
      "Iteration 788/1000, Loss: 0.16696761548519135\n",
      "Iteration 789/1000, Loss: 0.16684599220752716\n",
      "Iteration 790/1000, Loss: 0.1667245477437973\n",
      "Iteration 791/1000, Loss: 0.16660325229167938\n",
      "Iteration 792/1000, Loss: 0.16648219525814056\n",
      "Iteration 793/1000, Loss: 0.1663612723350525\n",
      "Iteration 794/1000, Loss: 0.16624055802822113\n",
      "Iteration 795/1000, Loss: 0.1661200225353241\n",
      "Iteration 796/1000, Loss: 0.16599959135055542\n",
      "Iteration 797/1000, Loss: 0.16587932407855988\n",
      "Iteration 798/1000, Loss: 0.16575917601585388\n",
      "Iteration 799/1000, Loss: 0.16563914716243744\n",
      "Iteration 800/1000, Loss: 0.16551926732063293\n",
      "Iteration 801/1000, Loss: 0.16539961099624634\n",
      "Iteration 802/1000, Loss: 0.16528013348579407\n",
      "Iteration 803/1000, Loss: 0.16516076028347015\n",
      "Iteration 804/1000, Loss: 0.16504158079624176\n",
      "Iteration 805/1000, Loss: 0.1649225652217865\n",
      "Iteration 806/1000, Loss: 0.1648036688566208\n",
      "Iteration 807/1000, Loss: 0.1646849662065506\n",
      "Iteration 808/1000, Loss: 0.16456644237041473\n",
      "Iteration 809/1000, Loss: 0.16444814205169678\n",
      "Iteration 810/1000, Loss: 0.16433000564575195\n",
      "Iteration 811/1000, Loss: 0.16421204805374146\n",
      "Iteration 812/1000, Loss: 0.16409419476985931\n",
      "Iteration 813/1000, Loss: 0.16397647559642792\n",
      "Iteration 814/1000, Loss: 0.16385896503925323\n",
      "Iteration 815/1000, Loss: 0.16374154388904572\n",
      "Iteration 816/1000, Loss: 0.16362430155277252\n",
      "Iteration 817/1000, Loss: 0.16350716352462769\n",
      "Iteration 818/1000, Loss: 0.16339024901390076\n",
      "Iteration 819/1000, Loss: 0.16327349841594696\n",
      "Iteration 820/1000, Loss: 0.1631568968296051\n",
      "Iteration 821/1000, Loss: 0.16304047405719757\n",
      "Iteration 822/1000, Loss: 0.16292418539524078\n",
      "Iteration 823/1000, Loss: 0.1628081053495407\n",
      "Iteration 824/1000, Loss: 0.16269221901893616\n",
      "Iteration 825/1000, Loss: 0.16257651150226593\n",
      "Iteration 826/1000, Loss: 0.16246098279953003\n",
      "Iteration 827/1000, Loss: 0.16234560310840607\n",
      "Iteration 828/1000, Loss: 0.16223040223121643\n",
      "Iteration 829/1000, Loss: 0.16211536526679993\n",
      "Iteration 830/1000, Loss: 0.16200041770935059\n",
      "Iteration 831/1000, Loss: 0.1618855893611908\n",
      "Iteration 832/1000, Loss: 0.16177088022232056\n",
      "Iteration 833/1000, Loss: 0.16165632009506226\n",
      "Iteration 834/1000, Loss: 0.1615418940782547\n",
      "Iteration 835/1000, Loss: 0.16142767667770386\n",
      "Iteration 836/1000, Loss: 0.16131356358528137\n",
      "Iteration 837/1000, Loss: 0.16119961440563202\n",
      "Iteration 838/1000, Loss: 0.161085844039917\n",
      "Iteration 839/1000, Loss: 0.1609722375869751\n",
      "Iteration 840/1000, Loss: 0.16085880994796753\n",
      "Iteration 841/1000, Loss: 0.1607455015182495\n",
      "Iteration 842/1000, Loss: 0.16063235700130463\n",
      "Iteration 843/1000, Loss: 0.16051940619945526\n",
      "Iteration 844/1000, Loss: 0.16040660440921783\n",
      "Iteration 845/1000, Loss: 0.16029396653175354\n",
      "Iteration 846/1000, Loss: 0.16018146276474\n",
      "Iteration 847/1000, Loss: 0.16006912291049957\n",
      "Iteration 848/1000, Loss: 0.15995696187019348\n",
      "Iteration 849/1000, Loss: 0.15984494984149933\n",
      "Iteration 850/1000, Loss: 0.1597331166267395\n",
      "Iteration 851/1000, Loss: 0.15962152183055878\n",
      "Iteration 852/1000, Loss: 0.1595100462436676\n",
      "Iteration 853/1000, Loss: 0.15939870476722717\n",
      "Iteration 854/1000, Loss: 0.1592874675989151\n",
      "Iteration 855/1000, Loss: 0.15917639434337616\n",
      "Iteration 856/1000, Loss: 0.15906541049480438\n",
      "Iteration 857/1000, Loss: 0.15895459055900574\n",
      "Iteration 858/1000, Loss: 0.15884391963481903\n",
      "Iteration 859/1000, Loss: 0.15873339772224426\n",
      "Iteration 860/1000, Loss: 0.15862299501895905\n",
      "Iteration 861/1000, Loss: 0.15851278603076935\n",
      "Iteration 862/1000, Loss: 0.1584026962518692\n",
      "Iteration 863/1000, Loss: 0.158292755484581\n",
      "Iteration 864/1000, Loss: 0.15818291902542114\n",
      "Iteration 865/1000, Loss: 0.15807324647903442\n",
      "Iteration 866/1000, Loss: 0.15796373784542084\n",
      "Iteration 867/1000, Loss: 0.157854363322258\n",
      "Iteration 868/1000, Loss: 0.1577451080083847\n",
      "Iteration 869/1000, Loss: 0.15763597190380096\n",
      "Iteration 870/1000, Loss: 0.15752698481082916\n",
      "Iteration 871/1000, Loss: 0.1574181467294693\n",
      "Iteration 872/1000, Loss: 0.15730945765972137\n",
      "Iteration 873/1000, Loss: 0.1572008579969406\n",
      "Iteration 874/1000, Loss: 0.15709242224693298\n",
      "Iteration 875/1000, Loss: 0.15698407590389252\n",
      "Iteration 876/1000, Loss: 0.156875878572464\n",
      "Iteration 877/1000, Loss: 0.156767800450325\n",
      "Iteration 878/1000, Loss: 0.15665988624095917\n",
      "Iteration 879/1000, Loss: 0.15655206143856049\n",
      "Iteration 880/1000, Loss: 0.15644435584545135\n",
      "Iteration 881/1000, Loss: 0.15633678436279297\n",
      "Iteration 882/1000, Loss: 0.1562294065952301\n",
      "Iteration 883/1000, Loss: 0.15612216293811798\n",
      "Iteration 884/1000, Loss: 0.15601509809494019\n",
      "Iteration 885/1000, Loss: 0.15590813755989075\n",
      "Iteration 886/1000, Loss: 0.15580137073993683\n",
      "Iteration 887/1000, Loss: 0.15569472312927246\n",
      "Iteration 888/1000, Loss: 0.15558820962905884\n",
      "Iteration 889/1000, Loss: 0.15548183023929596\n",
      "Iteration 890/1000, Loss: 0.15537558495998383\n",
      "Iteration 891/1000, Loss: 0.15526950359344482\n",
      "Iteration 892/1000, Loss: 0.15516357123851776\n",
      "Iteration 893/1000, Loss: 0.15505777299404144\n",
      "Iteration 894/1000, Loss: 0.15495207905769348\n",
      "Iteration 895/1000, Loss: 0.15484654903411865\n",
      "Iteration 896/1000, Loss: 0.15474115312099457\n",
      "Iteration 897/1000, Loss: 0.15463587641716003\n",
      "Iteration 898/1000, Loss: 0.15453074872493744\n",
      "Iteration 899/1000, Loss: 0.154425710439682\n",
      "Iteration 900/1000, Loss: 0.1543208360671997\n",
      "Iteration 901/1000, Loss: 0.15421603620052338\n",
      "Iteration 902/1000, Loss: 0.1541113555431366\n",
      "Iteration 903/1000, Loss: 0.15400686860084534\n",
      "Iteration 904/1000, Loss: 0.15390250086784363\n",
      "Iteration 905/1000, Loss: 0.15379828214645386\n",
      "Iteration 906/1000, Loss: 0.15369421243667603\n",
      "Iteration 907/1000, Loss: 0.15359024703502655\n",
      "Iteration 908/1000, Loss: 0.1534864604473114\n",
      "Iteration 909/1000, Loss: 0.153382807970047\n",
      "Iteration 910/1000, Loss: 0.15327925980091095\n",
      "Iteration 911/1000, Loss: 0.15317584574222565\n",
      "Iteration 912/1000, Loss: 0.1530725657939911\n",
      "Iteration 913/1000, Loss: 0.15296944975852966\n",
      "Iteration 914/1000, Loss: 0.1528664380311966\n",
      "Iteration 915/1000, Loss: 0.15276357531547546\n",
      "Iteration 916/1000, Loss: 0.15266084671020508\n",
      "Iteration 917/1000, Loss: 0.15255826711654663\n",
      "Iteration 918/1000, Loss: 0.15245576202869415\n",
      "Iteration 919/1000, Loss: 0.15235334634780884\n",
      "Iteration 920/1000, Loss: 0.15225112438201904\n",
      "Iteration 921/1000, Loss: 0.1521490216255188\n",
      "Iteration 922/1000, Loss: 0.15204714238643646\n",
      "Iteration 923/1000, Loss: 0.1519453376531601\n",
      "Iteration 924/1000, Loss: 0.15184368193149567\n",
      "Iteration 925/1000, Loss: 0.15174219012260437\n",
      "Iteration 926/1000, Loss: 0.15164075791835785\n",
      "Iteration 927/1000, Loss: 0.1515394151210785\n",
      "Iteration 928/1000, Loss: 0.1514381617307663\n",
      "Iteration 929/1000, Loss: 0.15133704245090485\n",
      "Iteration 930/1000, Loss: 0.15123601257801056\n",
      "Iteration 931/1000, Loss: 0.15113510191440582\n",
      "Iteration 932/1000, Loss: 0.15103435516357422\n",
      "Iteration 933/1000, Loss: 0.15093369781970978\n",
      "Iteration 934/1000, Loss: 0.1508331596851349\n",
      "Iteration 935/1000, Loss: 0.15073271095752716\n",
      "Iteration 936/1000, Loss: 0.15063239634037018\n",
      "Iteration 937/1000, Loss: 0.15053220093250275\n",
      "Iteration 938/1000, Loss: 0.15043213963508606\n",
      "Iteration 939/1000, Loss: 0.1503322273492813\n",
      "Iteration 940/1000, Loss: 0.15023237466812134\n",
      "Iteration 941/1000, Loss: 0.1501326709985733\n",
      "Iteration 942/1000, Loss: 0.15003301203250885\n",
      "Iteration 943/1000, Loss: 0.14993345737457275\n",
      "Iteration 944/1000, Loss: 0.1498340517282486\n",
      "Iteration 945/1000, Loss: 0.149734765291214\n",
      "Iteration 946/1000, Loss: 0.14963564276695251\n",
      "Iteration 947/1000, Loss: 0.14953668415546417\n",
      "Iteration 948/1000, Loss: 0.149437814950943\n",
      "Iteration 949/1000, Loss: 0.14933906495571136\n",
      "Iteration 950/1000, Loss: 0.1492404043674469\n",
      "Iteration 951/1000, Loss: 0.14914186298847198\n",
      "Iteration 952/1000, Loss: 0.14904345571994781\n",
      "Iteration 953/1000, Loss: 0.14894522726535797\n",
      "Iteration 954/1000, Loss: 0.14884711802005768\n",
      "Iteration 955/1000, Loss: 0.14874915778636932\n",
      "Iteration 956/1000, Loss: 0.14865125715732574\n",
      "Iteration 957/1000, Loss: 0.14855347573757172\n",
      "Iteration 958/1000, Loss: 0.14845579862594604\n",
      "Iteration 959/1000, Loss: 0.14835819602012634\n",
      "Iteration 960/1000, Loss: 0.1482607126235962\n",
      "Iteration 961/1000, Loss: 0.1481633186340332\n",
      "Iteration 962/1000, Loss: 0.14806599915027618\n",
      "Iteration 963/1000, Loss: 0.1479688286781311\n",
      "Iteration 964/1000, Loss: 0.14787174761295319\n",
      "Iteration 965/1000, Loss: 0.14777474105358124\n",
      "Iteration 966/1000, Loss: 0.14767791330814362\n",
      "Iteration 967/1000, Loss: 0.14758118987083435\n",
      "Iteration 968/1000, Loss: 0.14748460054397583\n",
      "Iteration 969/1000, Loss: 0.14738819003105164\n",
      "Iteration 970/1000, Loss: 0.1472918838262558\n",
      "Iteration 971/1000, Loss: 0.14719580113887787\n",
      "Iteration 972/1000, Loss: 0.1470998376607895\n",
      "Iteration 973/1000, Loss: 0.14700397849082947\n",
      "Iteration 974/1000, Loss: 0.14690826833248138\n",
      "Iteration 975/1000, Loss: 0.14681266248226166\n",
      "Iteration 976/1000, Loss: 0.14671717584133148\n",
      "Iteration 977/1000, Loss: 0.14662177860736847\n",
      "Iteration 978/1000, Loss: 0.1465265452861786\n",
      "Iteration 979/1000, Loss: 0.14643141627311707\n",
      "Iteration 980/1000, Loss: 0.14633633196353912\n",
      "Iteration 981/1000, Loss: 0.1462414264678955\n",
      "Iteration 982/1000, Loss: 0.14614661037921906\n",
      "Iteration 983/1000, Loss: 0.14605185389518738\n",
      "Iteration 984/1000, Loss: 0.14595724642276764\n",
      "Iteration 985/1000, Loss: 0.14586277306079865\n",
      "Iteration 986/1000, Loss: 0.14576847851276398\n",
      "Iteration 987/1000, Loss: 0.14567431807518005\n",
      "Iteration 988/1000, Loss: 0.14558027684688568\n",
      "Iteration 989/1000, Loss: 0.14548635482788086\n",
      "Iteration 990/1000, Loss: 0.1453925520181656\n",
      "Iteration 991/1000, Loss: 0.14529883861541748\n",
      "Iteration 992/1000, Loss: 0.14520525932312012\n",
      "Iteration 993/1000, Loss: 0.1451118290424347\n",
      "Iteration 994/1000, Loss: 0.1450185477733612\n",
      "Iteration 995/1000, Loss: 0.14492543041706085\n",
      "Iteration 996/1000, Loss: 0.14483241736888885\n",
      "Iteration 997/1000, Loss: 0.14473950862884521\n",
      "Iteration 998/1000, Loss: 0.14464671909809113\n",
      "Iteration 999/1000, Loss: 0.144554004073143\n",
      "Iteration 1000/1000, Loss: 0.14446140825748444\n",
      "Pruning Step 1\n",
      "Iteration 1/1000, Loss: 2.144630193710327\n",
      "Iteration 2/1000, Loss: 2.035400629043579\n",
      "Iteration 3/1000, Loss: 1.9446609020233154\n",
      "Iteration 4/1000, Loss: 1.859442114830017\n",
      "Iteration 5/1000, Loss: 1.7769562005996704\n",
      "Iteration 6/1000, Loss: 1.696776032447815\n",
      "Iteration 7/1000, Loss: 1.6189391613006592\n",
      "Iteration 8/1000, Loss: 1.5437265634536743\n",
      "Iteration 9/1000, Loss: 1.4714804887771606\n",
      "Iteration 10/1000, Loss: 1.4024747610092163\n",
      "Iteration 11/1000, Loss: 1.3369691371917725\n",
      "Iteration 12/1000, Loss: 1.275126576423645\n",
      "Iteration 13/1000, Loss: 1.21705961227417\n",
      "Iteration 14/1000, Loss: 1.1627187728881836\n",
      "Iteration 15/1000, Loss: 1.1120949983596802\n",
      "Iteration 16/1000, Loss: 1.065056324005127\n",
      "Iteration 17/1000, Loss: 1.021432876586914\n",
      "Iteration 18/1000, Loss: 0.981013298034668\n",
      "Iteration 19/1000, Loss: 0.9435998797416687\n",
      "Iteration 20/1000, Loss: 0.9089832305908203\n",
      "Iteration 21/1000, Loss: 0.8769704699516296\n",
      "Iteration 22/1000, Loss: 0.8473529815673828\n",
      "Iteration 23/1000, Loss: 0.8199678659439087\n",
      "Iteration 24/1000, Loss: 0.7946447730064392\n",
      "Iteration 25/1000, Loss: 0.7712000012397766\n",
      "Iteration 26/1000, Loss: 0.749477744102478\n",
      "Iteration 27/1000, Loss: 0.7293234467506409\n",
      "Iteration 28/1000, Loss: 0.7105950117111206\n",
      "Iteration 29/1000, Loss: 0.6931643486022949\n",
      "Iteration 30/1000, Loss: 0.6769181489944458\n",
      "Iteration 31/1000, Loss: 0.6617444157600403\n",
      "Iteration 32/1000, Loss: 0.6475486159324646\n",
      "Iteration 33/1000, Loss: 0.634243369102478\n",
      "Iteration 34/1000, Loss: 0.6217507123947144\n",
      "Iteration 35/1000, Loss: 0.6100013256072998\n",
      "Iteration 36/1000, Loss: 0.5989395976066589\n",
      "Iteration 37/1000, Loss: 0.5885053873062134\n",
      "Iteration 38/1000, Loss: 0.5786476731300354\n",
      "Iteration 39/1000, Loss: 0.5693215131759644\n",
      "Iteration 40/1000, Loss: 0.5604873895645142\n",
      "Iteration 41/1000, Loss: 0.5521042943000793\n",
      "Iteration 42/1000, Loss: 0.5441392660140991\n",
      "Iteration 43/1000, Loss: 0.5365613698959351\n",
      "Iteration 44/1000, Loss: 0.5293449759483337\n",
      "Iteration 45/1000, Loss: 0.5224642157554626\n",
      "Iteration 46/1000, Loss: 0.5158961415290833\n",
      "Iteration 47/1000, Loss: 0.509620189666748\n",
      "Iteration 48/1000, Loss: 0.503618061542511\n",
      "Iteration 49/1000, Loss: 0.4978700578212738\n",
      "Iteration 50/1000, Loss: 0.4923608601093292\n",
      "Iteration 51/1000, Loss: 0.48707520961761475\n",
      "Iteration 52/1000, Loss: 0.48199963569641113\n",
      "Iteration 53/1000, Loss: 0.4771217405796051\n",
      "Iteration 54/1000, Loss: 0.47243010997772217\n",
      "Iteration 55/1000, Loss: 0.4679146707057953\n",
      "Iteration 56/1000, Loss: 0.46356549859046936\n",
      "Iteration 57/1000, Loss: 0.4593733847141266\n",
      "Iteration 58/1000, Loss: 0.4553299844264984\n",
      "Iteration 59/1000, Loss: 0.4514266550540924\n",
      "Iteration 60/1000, Loss: 0.4476562440395355\n",
      "Iteration 61/1000, Loss: 0.44401249289512634\n",
      "Iteration 62/1000, Loss: 0.440488338470459\n",
      "Iteration 63/1000, Loss: 0.4370776116847992\n",
      "Iteration 64/1000, Loss: 0.43377476930618286\n",
      "Iteration 65/1000, Loss: 0.43057388067245483\n",
      "Iteration 66/1000, Loss: 0.42747071385383606\n",
      "Iteration 67/1000, Loss: 0.4244609773159027\n",
      "Iteration 68/1000, Loss: 0.421539843082428\n",
      "Iteration 69/1000, Loss: 0.41870400309562683\n",
      "Iteration 70/1000, Loss: 0.4159490168094635\n",
      "Iteration 71/1000, Loss: 0.4132704734802246\n",
      "Iteration 72/1000, Loss: 0.4106655716896057\n",
      "Iteration 73/1000, Loss: 0.40813106298446655\n",
      "Iteration 74/1000, Loss: 0.40566393733024597\n",
      "Iteration 75/1000, Loss: 0.40326160192489624\n",
      "Iteration 76/1000, Loss: 0.4009222090244293\n",
      "Iteration 77/1000, Loss: 0.39864253997802734\n",
      "Iteration 78/1000, Loss: 0.39641955494880676\n",
      "Iteration 79/1000, Loss: 0.39425113797187805\n",
      "Iteration 80/1000, Loss: 0.392135351896286\n",
      "Iteration 81/1000, Loss: 0.390069842338562\n",
      "Iteration 82/1000, Loss: 0.38805246353149414\n",
      "Iteration 83/1000, Loss: 0.3860815465450287\n",
      "Iteration 84/1000, Loss: 0.3841556906700134\n",
      "Iteration 85/1000, Loss: 0.3822730779647827\n",
      "Iteration 86/1000, Loss: 0.38043156266212463\n",
      "Iteration 87/1000, Loss: 0.3786298930644989\n",
      "Iteration 88/1000, Loss: 0.3768668472766876\n",
      "Iteration 89/1000, Loss: 0.375140905380249\n",
      "Iteration 90/1000, Loss: 0.37345069646835327\n",
      "Iteration 91/1000, Loss: 0.37179505825042725\n",
      "Iteration 92/1000, Loss: 0.37017208337783813\n",
      "Iteration 93/1000, Loss: 0.36858078837394714\n",
      "Iteration 94/1000, Loss: 0.3670208752155304\n",
      "Iteration 95/1000, Loss: 0.36549127101898193\n",
      "Iteration 96/1000, Loss: 0.3639909625053406\n",
      "Iteration 97/1000, Loss: 0.36251890659332275\n",
      "Iteration 98/1000, Loss: 0.36107391119003296\n",
      "Iteration 99/1000, Loss: 0.3596554696559906\n",
      "Iteration 100/1000, Loss: 0.3582625985145569\n",
      "Iteration 101/1000, Loss: 0.35689452290534973\n",
      "Iteration 102/1000, Loss: 0.3555503189563751\n",
      "Iteration 103/1000, Loss: 0.3542294204235077\n",
      "Iteration 104/1000, Loss: 0.35293102264404297\n",
      "Iteration 105/1000, Loss: 0.35165444016456604\n",
      "Iteration 106/1000, Loss: 0.35039928555488586\n",
      "Iteration 107/1000, Loss: 0.3491649627685547\n",
      "Iteration 108/1000, Loss: 0.3479507267475128\n",
      "Iteration 109/1000, Loss: 0.3467559218406677\n",
      "Iteration 110/1000, Loss: 0.34557974338531494\n",
      "Iteration 111/1000, Loss: 0.34442174434661865\n",
      "Iteration 112/1000, Loss: 0.3432818353176117\n",
      "Iteration 113/1000, Loss: 0.3421594798564911\n",
      "Iteration 114/1000, Loss: 0.341054230928421\n",
      "Iteration 115/1000, Loss: 0.3399656116962433\n",
      "Iteration 116/1000, Loss: 0.33889302611351013\n",
      "Iteration 117/1000, Loss: 0.3378355801105499\n",
      "Iteration 118/1000, Loss: 0.3367932438850403\n",
      "Iteration 119/1000, Loss: 0.33576563000679016\n",
      "Iteration 120/1000, Loss: 0.33475247025489807\n",
      "Iteration 121/1000, Loss: 0.33375319838523865\n",
      "Iteration 122/1000, Loss: 0.3327675759792328\n",
      "Iteration 123/1000, Loss: 0.33179524540901184\n",
      "Iteration 124/1000, Loss: 0.33083590865135193\n",
      "Iteration 125/1000, Loss: 0.3298889994621277\n",
      "Iteration 126/1000, Loss: 0.32895442843437195\n",
      "Iteration 127/1000, Loss: 0.3280317187309265\n",
      "Iteration 128/1000, Loss: 0.327120304107666\n",
      "Iteration 129/1000, Loss: 0.32622000575065613\n",
      "Iteration 130/1000, Loss: 0.3253308832645416\n",
      "Iteration 131/1000, Loss: 0.32445257902145386\n",
      "Iteration 132/1000, Loss: 0.32358482480049133\n",
      "Iteration 133/1000, Loss: 0.32272785902023315\n",
      "Iteration 134/1000, Loss: 0.32188084721565247\n",
      "Iteration 135/1000, Loss: 0.32104364037513733\n",
      "Iteration 136/1000, Loss: 0.32021602988243103\n",
      "Iteration 137/1000, Loss: 0.31939759850502014\n",
      "Iteration 138/1000, Loss: 0.3185887038707733\n",
      "Iteration 139/1000, Loss: 0.3177890479564667\n",
      "Iteration 140/1000, Loss: 0.31699830293655396\n",
      "Iteration 141/1000, Loss: 0.3162161409854889\n",
      "Iteration 142/1000, Loss: 0.31544265151023865\n",
      "Iteration 143/1000, Loss: 0.31467777490615845\n",
      "Iteration 144/1000, Loss: 0.3139207661151886\n",
      "Iteration 145/1000, Loss: 0.31317129731178284\n",
      "Iteration 146/1000, Loss: 0.3124293088912964\n",
      "Iteration 147/1000, Loss: 0.31169480085372925\n",
      "Iteration 148/1000, Loss: 0.31096765398979187\n",
      "Iteration 149/1000, Loss: 0.3102477192878723\n",
      "Iteration 150/1000, Loss: 0.30953502655029297\n",
      "Iteration 151/1000, Loss: 0.3088291883468628\n",
      "Iteration 152/1000, Loss: 0.3081304132938385\n",
      "Iteration 153/1000, Loss: 0.3074382245540619\n",
      "Iteration 154/1000, Loss: 0.306752473115921\n",
      "Iteration 155/1000, Loss: 0.3060731887817383\n",
      "Iteration 156/1000, Loss: 0.30540022253990173\n",
      "Iteration 157/1000, Loss: 0.30473384261131287\n",
      "Iteration 158/1000, Loss: 0.3040734827518463\n",
      "Iteration 159/1000, Loss: 0.3034190833568573\n",
      "Iteration 160/1000, Loss: 0.30277055501937866\n",
      "Iteration 161/1000, Loss: 0.30212751030921936\n",
      "Iteration 162/1000, Loss: 0.30149030685424805\n",
      "Iteration 163/1000, Loss: 0.30085885524749756\n",
      "Iteration 164/1000, Loss: 0.3002329170703888\n",
      "Iteration 165/1000, Loss: 0.29961249232292175\n",
      "Iteration 166/1000, Loss: 0.29899710416793823\n",
      "Iteration 167/1000, Loss: 0.2983865737915039\n",
      "Iteration 168/1000, Loss: 0.29778149724006653\n",
      "Iteration 169/1000, Loss: 0.297181099653244\n",
      "Iteration 170/1000, Loss: 0.29658564925193787\n",
      "Iteration 171/1000, Loss: 0.2959950864315033\n",
      "Iteration 172/1000, Loss: 0.2954093813896179\n",
      "Iteration 173/1000, Loss: 0.294828325510025\n",
      "Iteration 174/1000, Loss: 0.29425176978111267\n",
      "Iteration 175/1000, Loss: 0.29367950558662415\n",
      "Iteration 176/1000, Loss: 0.2931113839149475\n",
      "Iteration 177/1000, Loss: 0.2925476133823395\n",
      "Iteration 178/1000, Loss: 0.2919880747795105\n",
      "Iteration 179/1000, Loss: 0.2914327085018158\n",
      "Iteration 180/1000, Loss: 0.2908819019794464\n",
      "Iteration 181/1000, Loss: 0.29033535718917847\n",
      "Iteration 182/1000, Loss: 0.28979289531707764\n",
      "Iteration 183/1000, Loss: 0.2892540395259857\n",
      "Iteration 184/1000, Loss: 0.2887188792228699\n",
      "Iteration 185/1000, Loss: 0.2881876826286316\n",
      "Iteration 186/1000, Loss: 0.2876600921154022\n",
      "Iteration 187/1000, Loss: 0.2871362864971161\n",
      "Iteration 188/1000, Loss: 0.28661614656448364\n",
      "Iteration 189/1000, Loss: 0.2860996425151825\n",
      "Iteration 190/1000, Loss: 0.28558677434921265\n",
      "Iteration 191/1000, Loss: 0.2850773334503174\n",
      "Iteration 192/1000, Loss: 0.2845713496208191\n",
      "Iteration 193/1000, Loss: 0.28406867384910583\n",
      "Iteration 194/1000, Loss: 0.28356924653053284\n",
      "Iteration 195/1000, Loss: 0.28307288885116577\n",
      "Iteration 196/1000, Loss: 0.28257983922958374\n",
      "Iteration 197/1000, Loss: 0.2820899784564972\n",
      "Iteration 198/1000, Loss: 0.2816029191017151\n",
      "Iteration 199/1000, Loss: 0.28111886978149414\n",
      "Iteration 200/1000, Loss: 0.2806378901004791\n",
      "Iteration 201/1000, Loss: 0.28015971183776855\n",
      "Iteration 202/1000, Loss: 0.2796846330165863\n",
      "Iteration 203/1000, Loss: 0.27921250462532043\n",
      "Iteration 204/1000, Loss: 0.278743177652359\n",
      "Iteration 205/1000, Loss: 0.27827662229537964\n",
      "Iteration 206/1000, Loss: 0.27781304717063904\n",
      "Iteration 207/1000, Loss: 0.27735215425491333\n",
      "Iteration 208/1000, Loss: 0.2768939137458801\n",
      "Iteration 209/1000, Loss: 0.2764383852481842\n",
      "Iteration 210/1000, Loss: 0.27598580718040466\n",
      "Iteration 211/1000, Loss: 0.2755359709262848\n",
      "Iteration 212/1000, Loss: 0.2750888168811798\n",
      "Iteration 213/1000, Loss: 0.2746441960334778\n",
      "Iteration 214/1000, Loss: 0.2742020785808563\n",
      "Iteration 215/1000, Loss: 0.2737627327442169\n",
      "Iteration 216/1000, Loss: 0.27332592010498047\n",
      "Iteration 217/1000, Loss: 0.2728916108608246\n",
      "Iteration 218/1000, Loss: 0.27245965600013733\n",
      "Iteration 219/1000, Loss: 0.27202990651130676\n",
      "Iteration 220/1000, Loss: 0.2716025114059448\n",
      "Iteration 221/1000, Loss: 0.2711775302886963\n",
      "Iteration 222/1000, Loss: 0.27075472474098206\n",
      "Iteration 223/1000, Loss: 0.27033427357673645\n",
      "Iteration 224/1000, Loss: 0.2699161171913147\n",
      "Iteration 225/1000, Loss: 0.269500195980072\n",
      "Iteration 226/1000, Loss: 0.26908665895462036\n",
      "Iteration 227/1000, Loss: 0.2686753571033478\n",
      "Iteration 228/1000, Loss: 0.26826632022857666\n",
      "Iteration 229/1000, Loss: 0.2678593397140503\n",
      "Iteration 230/1000, Loss: 0.2674542963504791\n",
      "Iteration 231/1000, Loss: 0.2670512795448303\n",
      "Iteration 232/1000, Loss: 0.26665034890174866\n",
      "Iteration 233/1000, Loss: 0.26625147461891174\n",
      "Iteration 234/1000, Loss: 0.2658546268939972\n",
      "Iteration 235/1000, Loss: 0.26545971632003784\n",
      "Iteration 236/1000, Loss: 0.26506665349006653\n",
      "Iteration 237/1000, Loss: 0.26467567682266235\n",
      "Iteration 238/1000, Loss: 0.26428666710853577\n",
      "Iteration 239/1000, Loss: 0.26389944553375244\n",
      "Iteration 240/1000, Loss: 0.2635142207145691\n",
      "Iteration 241/1000, Loss: 0.2631307542324066\n",
      "Iteration 242/1000, Loss: 0.2627491056919098\n",
      "Iteration 243/1000, Loss: 0.26236939430236816\n",
      "Iteration 244/1000, Loss: 0.261991411447525\n",
      "Iteration 245/1000, Loss: 0.26161524653434753\n",
      "Iteration 246/1000, Loss: 0.261240690946579\n",
      "Iteration 247/1000, Loss: 0.2608676552772522\n",
      "Iteration 248/1000, Loss: 0.26049619913101196\n",
      "Iteration 249/1000, Loss: 0.26012635231018066\n",
      "Iteration 250/1000, Loss: 0.2597581148147583\n",
      "Iteration 251/1000, Loss: 0.2593914270401001\n",
      "Iteration 252/1000, Loss: 0.25902634859085083\n",
      "Iteration 253/1000, Loss: 0.2586628794670105\n",
      "Iteration 254/1000, Loss: 0.2583010494709015\n",
      "Iteration 255/1000, Loss: 0.25794073939323425\n",
      "Iteration 256/1000, Loss: 0.257581889629364\n",
      "Iteration 257/1000, Loss: 0.25722450017929077\n",
      "Iteration 258/1000, Loss: 0.25686851143836975\n",
      "Iteration 259/1000, Loss: 0.2565140128135681\n",
      "Iteration 260/1000, Loss: 0.25616079568862915\n",
      "Iteration 261/1000, Loss: 0.25580915808677673\n",
      "Iteration 262/1000, Loss: 0.25545889139175415\n",
      "Iteration 263/1000, Loss: 0.2551102638244629\n",
      "Iteration 264/1000, Loss: 0.254763126373291\n",
      "Iteration 265/1000, Loss: 0.25441741943359375\n",
      "Iteration 266/1000, Loss: 0.25407326221466064\n",
      "Iteration 267/1000, Loss: 0.25373056530952454\n",
      "Iteration 268/1000, Loss: 0.2533891797065735\n",
      "Iteration 269/1000, Loss: 0.25304895639419556\n",
      "Iteration 270/1000, Loss: 0.25271013379096985\n",
      "Iteration 271/1000, Loss: 0.2523726522922516\n",
      "Iteration 272/1000, Loss: 0.25203660130500793\n",
      "Iteration 273/1000, Loss: 0.2517019212245941\n",
      "Iteration 274/1000, Loss: 0.25136852264404297\n",
      "Iteration 275/1000, Loss: 0.25103652477264404\n",
      "Iteration 276/1000, Loss: 0.2507059872150421\n",
      "Iteration 277/1000, Loss: 0.25037676095962524\n",
      "Iteration 278/1000, Loss: 0.2500489354133606\n",
      "Iteration 279/1000, Loss: 0.24972233176231384\n",
      "Iteration 280/1000, Loss: 0.2493969351053238\n",
      "Iteration 281/1000, Loss: 0.24907279014587402\n",
      "Iteration 282/1000, Loss: 0.24874980747699738\n",
      "Iteration 283/1000, Loss: 0.2484281212091446\n",
      "Iteration 284/1000, Loss: 0.2481076568365097\n",
      "Iteration 285/1000, Loss: 0.247788667678833\n",
      "Iteration 286/1000, Loss: 0.24747106432914734\n",
      "Iteration 287/1000, Loss: 0.24715475738048553\n",
      "Iteration 288/1000, Loss: 0.24683968722820282\n",
      "Iteration 289/1000, Loss: 0.24652576446533203\n",
      "Iteration 290/1000, Loss: 0.24621282517910004\n",
      "Iteration 291/1000, Loss: 0.245900958776474\n",
      "Iteration 292/1000, Loss: 0.2455902397632599\n",
      "Iteration 293/1000, Loss: 0.24528057873249054\n",
      "Iteration 294/1000, Loss: 0.24497200548648834\n",
      "Iteration 295/1000, Loss: 0.24466468393802643\n",
      "Iteration 296/1000, Loss: 0.24435847997665405\n",
      "Iteration 297/1000, Loss: 0.2440534383058548\n",
      "Iteration 298/1000, Loss: 0.24374960362911224\n",
      "Iteration 299/1000, Loss: 0.24344684183597565\n",
      "Iteration 300/1000, Loss: 0.24314510822296143\n",
      "Iteration 301/1000, Loss: 0.24284449219703674\n",
      "Iteration 302/1000, Loss: 0.242545023560524\n",
      "Iteration 303/1000, Loss: 0.24224652349948883\n",
      "Iteration 304/1000, Loss: 0.24194899201393127\n",
      "Iteration 305/1000, Loss: 0.24165253341197968\n",
      "Iteration 306/1000, Loss: 0.24135705828666687\n",
      "Iteration 307/1000, Loss: 0.24106237292289734\n",
      "Iteration 308/1000, Loss: 0.2407686561346054\n",
      "Iteration 309/1000, Loss: 0.24047601222991943\n",
      "Iteration 310/1000, Loss: 0.24018444120883942\n",
      "Iteration 311/1000, Loss: 0.23989376425743103\n",
      "Iteration 312/1000, Loss: 0.23960407078266144\n",
      "Iteration 313/1000, Loss: 0.23931531608104706\n",
      "Iteration 314/1000, Loss: 0.2390274554491043\n",
      "Iteration 315/1000, Loss: 0.23874056339263916\n",
      "Iteration 316/1000, Loss: 0.2384546995162964\n",
      "Iteration 317/1000, Loss: 0.23816987872123718\n",
      "Iteration 318/1000, Loss: 0.23788614571094513\n",
      "Iteration 319/1000, Loss: 0.23760344088077545\n",
      "Iteration 320/1000, Loss: 0.2373216301202774\n",
      "Iteration 321/1000, Loss: 0.23704062402248383\n",
      "Iteration 322/1000, Loss: 0.23676054179668427\n",
      "Iteration 323/1000, Loss: 0.2364814132452011\n",
      "Iteration 324/1000, Loss: 0.23620331287384033\n",
      "Iteration 325/1000, Loss: 0.23592627048492432\n",
      "Iteration 326/1000, Loss: 0.2356501668691635\n",
      "Iteration 327/1000, Loss: 0.23537492752075195\n",
      "Iteration 328/1000, Loss: 0.23510035872459412\n",
      "Iteration 329/1000, Loss: 0.2348264753818512\n",
      "Iteration 330/1000, Loss: 0.23455365002155304\n",
      "Iteration 331/1000, Loss: 0.23428182303905487\n",
      "Iteration 332/1000, Loss: 0.2340107411146164\n",
      "Iteration 333/1000, Loss: 0.23374049365520477\n",
      "Iteration 334/1000, Loss: 0.23347118496894836\n",
      "Iteration 335/1000, Loss: 0.23320266604423523\n",
      "Iteration 336/1000, Loss: 0.23293499648571014\n",
      "Iteration 337/1000, Loss: 0.23266810178756714\n",
      "Iteration 338/1000, Loss: 0.23240193724632263\n",
      "Iteration 339/1000, Loss: 0.2321365922689438\n",
      "Iteration 340/1000, Loss: 0.2318720519542694\n",
      "Iteration 341/1000, Loss: 0.23160825669765472\n",
      "Iteration 342/1000, Loss: 0.23134517669677734\n",
      "Iteration 343/1000, Loss: 0.231082946062088\n",
      "Iteration 344/1000, Loss: 0.23082154989242554\n",
      "Iteration 345/1000, Loss: 0.2305608093738556\n",
      "Iteration 346/1000, Loss: 0.2303008884191513\n",
      "Iteration 347/1000, Loss: 0.23004180192947388\n",
      "Iteration 348/1000, Loss: 0.2297835350036621\n",
      "Iteration 349/1000, Loss: 0.22952599823474884\n",
      "Iteration 350/1000, Loss: 0.22926923632621765\n",
      "Iteration 351/1000, Loss: 0.229013130068779\n",
      "Iteration 352/1000, Loss: 0.2287577986717224\n",
      "Iteration 353/1000, Loss: 0.22850309312343597\n",
      "Iteration 354/1000, Loss: 0.22824913263320923\n",
      "Iteration 355/1000, Loss: 0.22799576818943024\n",
      "Iteration 356/1000, Loss: 0.22774305939674377\n",
      "Iteration 357/1000, Loss: 0.22749097645282745\n",
      "Iteration 358/1000, Loss: 0.2272397130727768\n",
      "Iteration 359/1000, Loss: 0.22698922455310822\n",
      "Iteration 360/1000, Loss: 0.22673945128917694\n",
      "Iteration 361/1000, Loss: 0.22649040818214417\n",
      "Iteration 362/1000, Loss: 0.22624218463897705\n",
      "Iteration 363/1000, Loss: 0.22599464654922485\n",
      "Iteration 364/1000, Loss: 0.22574777901172638\n",
      "Iteration 365/1000, Loss: 0.22550149261951447\n",
      "Iteration 366/1000, Loss: 0.22525592148303986\n",
      "Iteration 367/1000, Loss: 0.22501108050346375\n",
      "Iteration 368/1000, Loss: 0.22476695477962494\n",
      "Iteration 369/1000, Loss: 0.22452348470687866\n",
      "Iteration 370/1000, Loss: 0.2242807000875473\n",
      "Iteration 371/1000, Loss: 0.22403860092163086\n",
      "Iteration 372/1000, Loss: 0.22379709780216217\n",
      "Iteration 373/1000, Loss: 0.22355622053146362\n",
      "Iteration 374/1000, Loss: 0.2233159840106964\n",
      "Iteration 375/1000, Loss: 0.22307641804218292\n",
      "Iteration 376/1000, Loss: 0.2228374034166336\n",
      "Iteration 377/1000, Loss: 0.2225990742444992\n",
      "Iteration 378/1000, Loss: 0.22236129641532898\n",
      "Iteration 379/1000, Loss: 0.22212421894073486\n",
      "Iteration 380/1000, Loss: 0.22188778221607208\n",
      "Iteration 381/1000, Loss: 0.22165200114250183\n",
      "Iteration 382/1000, Loss: 0.2214168757200241\n",
      "Iteration 383/1000, Loss: 0.2211824208498001\n",
      "Iteration 384/1000, Loss: 0.22094853222370148\n",
      "Iteration 385/1000, Loss: 0.22071526944637299\n",
      "Iteration 386/1000, Loss: 0.22048258781433105\n",
      "Iteration 387/1000, Loss: 0.2202504724264145\n",
      "Iteration 388/1000, Loss: 0.2200189083814621\n",
      "Iteration 389/1000, Loss: 0.21978774666786194\n",
      "Iteration 390/1000, Loss: 0.2195572406053543\n",
      "Iteration 391/1000, Loss: 0.21932731568813324\n",
      "Iteration 392/1000, Loss: 0.2190980166196823\n",
      "Iteration 393/1000, Loss: 0.21886932849884033\n",
      "Iteration 394/1000, Loss: 0.21864137053489685\n",
      "Iteration 395/1000, Loss: 0.21841396391391754\n",
      "Iteration 396/1000, Loss: 0.21818706393241882\n",
      "Iteration 397/1000, Loss: 0.2179606705904007\n",
      "Iteration 398/1000, Loss: 0.21773481369018555\n",
      "Iteration 399/1000, Loss: 0.21750961244106293\n",
      "Iteration 400/1000, Loss: 0.21728500723838806\n",
      "Iteration 401/1000, Loss: 0.21706107258796692\n",
      "Iteration 402/1000, Loss: 0.21683785319328308\n",
      "Iteration 403/1000, Loss: 0.21661534905433655\n",
      "Iteration 404/1000, Loss: 0.21639351546764374\n",
      "Iteration 405/1000, Loss: 0.2161722630262375\n",
      "Iteration 406/1000, Loss: 0.21595162153244019\n",
      "Iteration 407/1000, Loss: 0.21573154628276825\n",
      "Iteration 408/1000, Loss: 0.21551191806793213\n",
      "Iteration 409/1000, Loss: 0.21529288589954376\n",
      "Iteration 410/1000, Loss: 0.2150745838880539\n",
      "Iteration 411/1000, Loss: 0.2148568332195282\n",
      "Iteration 412/1000, Loss: 0.2146395444869995\n",
      "Iteration 413/1000, Loss: 0.2144227921962738\n",
      "Iteration 414/1000, Loss: 0.21420647203922272\n",
      "Iteration 415/1000, Loss: 0.2139906883239746\n",
      "Iteration 416/1000, Loss: 0.21377548575401306\n",
      "Iteration 417/1000, Loss: 0.2135608047246933\n",
      "Iteration 418/1000, Loss: 0.21334664523601532\n",
      "Iteration 419/1000, Loss: 0.2131330668926239\n",
      "Iteration 420/1000, Loss: 0.21292008459568024\n",
      "Iteration 421/1000, Loss: 0.2127075493335724\n",
      "Iteration 422/1000, Loss: 0.21249550580978394\n",
      "Iteration 423/1000, Loss: 0.21228404343128204\n",
      "Iteration 424/1000, Loss: 0.2120731621980667\n",
      "Iteration 425/1000, Loss: 0.21186289191246033\n",
      "Iteration 426/1000, Loss: 0.21165312826633453\n",
      "Iteration 427/1000, Loss: 0.2114439159631729\n",
      "Iteration 428/1000, Loss: 0.21123531460762024\n",
      "Iteration 429/1000, Loss: 0.2110271453857422\n",
      "Iteration 430/1000, Loss: 0.2108195275068283\n",
      "Iteration 431/1000, Loss: 0.2106124311685562\n",
      "Iteration 432/1000, Loss: 0.21040596067905426\n",
      "Iteration 433/1000, Loss: 0.21019995212554932\n",
      "Iteration 434/1000, Loss: 0.2099943310022354\n",
      "Iteration 435/1000, Loss: 0.20978926122188568\n",
      "Iteration 436/1000, Loss: 0.20958472788333893\n",
      "Iteration 437/1000, Loss: 0.20938071608543396\n",
      "Iteration 438/1000, Loss: 0.20917721092700958\n",
      "Iteration 439/1000, Loss: 0.20897424221038818\n",
      "Iteration 440/1000, Loss: 0.20877185463905334\n",
      "Iteration 441/1000, Loss: 0.2085699588060379\n",
      "Iteration 442/1000, Loss: 0.20836852490901947\n",
      "Iteration 443/1000, Loss: 0.20816750824451447\n",
      "Iteration 444/1000, Loss: 0.2079668641090393\n",
      "Iteration 445/1000, Loss: 0.20776666700839996\n",
      "Iteration 446/1000, Loss: 0.20756693184375763\n",
      "Iteration 447/1000, Loss: 0.2073676586151123\n",
      "Iteration 448/1000, Loss: 0.20716889202594757\n",
      "Iteration 449/1000, Loss: 0.20697051286697388\n",
      "Iteration 450/1000, Loss: 0.206772580742836\n",
      "Iteration 451/1000, Loss: 0.2065749615430832\n",
      "Iteration 452/1000, Loss: 0.2063777893781662\n",
      "Iteration 453/1000, Loss: 0.2061811238527298\n",
      "Iteration 454/1000, Loss: 0.20598483085632324\n",
      "Iteration 455/1000, Loss: 0.2057889699935913\n",
      "Iteration 456/1000, Loss: 0.20559360086917877\n",
      "Iteration 457/1000, Loss: 0.20539866387844086\n",
      "Iteration 458/1000, Loss: 0.20520418882369995\n",
      "Iteration 459/1000, Loss: 0.20501013100147247\n",
      "Iteration 460/1000, Loss: 0.204816535115242\n",
      "Iteration 461/1000, Loss: 0.2046235054731369\n",
      "Iteration 462/1000, Loss: 0.2044309824705124\n",
      "Iteration 463/1000, Loss: 0.2042389065027237\n",
      "Iteration 464/1000, Loss: 0.20404717326164246\n",
      "Iteration 465/1000, Loss: 0.20385584235191345\n",
      "Iteration 466/1000, Loss: 0.20366491377353668\n",
      "Iteration 467/1000, Loss: 0.20347441732883453\n",
      "Iteration 468/1000, Loss: 0.20328444242477417\n",
      "Iteration 469/1000, Loss: 0.203094944357872\n",
      "Iteration 470/1000, Loss: 0.20290590822696686\n",
      "Iteration 471/1000, Loss: 0.20271727442741394\n",
      "Iteration 472/1000, Loss: 0.20252907276153564\n",
      "Iteration 473/1000, Loss: 0.20234127342700958\n",
      "Iteration 474/1000, Loss: 0.20215384662151337\n",
      "Iteration 475/1000, Loss: 0.20196670293807983\n",
      "Iteration 476/1000, Loss: 0.20177999138832092\n",
      "Iteration 477/1000, Loss: 0.20159365236759186\n",
      "Iteration 478/1000, Loss: 0.20140765607357025\n",
      "Iteration 479/1000, Loss: 0.2012220323085785\n",
      "Iteration 480/1000, Loss: 0.2010367214679718\n",
      "Iteration 481/1000, Loss: 0.20085185766220093\n",
      "Iteration 482/1000, Loss: 0.20066750049591064\n",
      "Iteration 483/1000, Loss: 0.200483500957489\n",
      "Iteration 484/1000, Loss: 0.20029985904693604\n",
      "Iteration 485/1000, Loss: 0.20011663436889648\n",
      "Iteration 486/1000, Loss: 0.19993391633033752\n",
      "Iteration 487/1000, Loss: 0.19975154101848602\n",
      "Iteration 488/1000, Loss: 0.19956961274147034\n",
      "Iteration 489/1000, Loss: 0.1993880718946457\n",
      "Iteration 490/1000, Loss: 0.19920691847801208\n",
      "Iteration 491/1000, Loss: 0.19902607798576355\n",
      "Iteration 492/1000, Loss: 0.19884563982486725\n",
      "Iteration 493/1000, Loss: 0.19866561889648438\n",
      "Iteration 494/1000, Loss: 0.19848591089248657\n",
      "Iteration 495/1000, Loss: 0.19830657541751862\n",
      "Iteration 496/1000, Loss: 0.19812767207622528\n",
      "Iteration 497/1000, Loss: 0.19794924557209015\n",
      "Iteration 498/1000, Loss: 0.19777119159698486\n",
      "Iteration 499/1000, Loss: 0.197593554854393\n",
      "Iteration 500/1000, Loss: 0.19741640985012054\n",
      "Iteration 501/1000, Loss: 0.1972396820783615\n",
      "Iteration 502/1000, Loss: 0.19706328213214874\n",
      "Iteration 503/1000, Loss: 0.1968870759010315\n",
      "Iteration 504/1000, Loss: 0.19671125710010529\n",
      "Iteration 505/1000, Loss: 0.19653581082820892\n",
      "Iteration 506/1000, Loss: 0.1963607370853424\n",
      "Iteration 507/1000, Loss: 0.1961860954761505\n",
      "Iteration 508/1000, Loss: 0.19601181149482727\n",
      "Iteration 509/1000, Loss: 0.19583788514137268\n",
      "Iteration 510/1000, Loss: 0.19566428661346436\n",
      "Iteration 511/1000, Loss: 0.19549104571342468\n",
      "Iteration 512/1000, Loss: 0.19531813263893127\n",
      "Iteration 513/1000, Loss: 0.19514545798301697\n",
      "Iteration 514/1000, Loss: 0.19497303664684296\n",
      "Iteration 515/1000, Loss: 0.19480100274085999\n",
      "Iteration 516/1000, Loss: 0.19462932646274567\n",
      "Iteration 517/1000, Loss: 0.19445796310901642\n",
      "Iteration 518/1000, Loss: 0.19428691267967224\n",
      "Iteration 519/1000, Loss: 0.19411614537239075\n",
      "Iteration 520/1000, Loss: 0.19394567608833313\n",
      "Iteration 521/1000, Loss: 0.1937754899263382\n",
      "Iteration 522/1000, Loss: 0.19360563158988953\n",
      "Iteration 523/1000, Loss: 0.19343611598014832\n",
      "Iteration 524/1000, Loss: 0.19326692819595337\n",
      "Iteration 525/1000, Loss: 0.1930980086326599\n",
      "Iteration 526/1000, Loss: 0.1929294615983963\n",
      "Iteration 527/1000, Loss: 0.19276122748851776\n",
      "Iteration 528/1000, Loss: 0.19259323179721832\n",
      "Iteration 529/1000, Loss: 0.19242551922798157\n",
      "Iteration 530/1000, Loss: 0.19225814938545227\n",
      "Iteration 531/1000, Loss: 0.19209112226963043\n",
      "Iteration 532/1000, Loss: 0.19192442297935486\n",
      "Iteration 533/1000, Loss: 0.19175812602043152\n",
      "Iteration 534/1000, Loss: 0.19159221649169922\n",
      "Iteration 535/1000, Loss: 0.19142669439315796\n",
      "Iteration 536/1000, Loss: 0.191261425614357\n",
      "Iteration 537/1000, Loss: 0.1910964846611023\n",
      "Iteration 538/1000, Loss: 0.1909317970275879\n",
      "Iteration 539/1000, Loss: 0.19076743721961975\n",
      "Iteration 540/1000, Loss: 0.19060340523719788\n",
      "Iteration 541/1000, Loss: 0.19043971598148346\n",
      "Iteration 542/1000, Loss: 0.1902763694524765\n",
      "Iteration 543/1000, Loss: 0.19011333584785461\n",
      "Iteration 544/1000, Loss: 0.1899505853652954\n",
      "Iteration 545/1000, Loss: 0.18978817760944366\n",
      "Iteration 546/1000, Loss: 0.18962611258029938\n",
      "Iteration 547/1000, Loss: 0.18946442008018494\n",
      "Iteration 548/1000, Loss: 0.18930304050445557\n",
      "Iteration 549/1000, Loss: 0.18914197385311127\n",
      "Iteration 550/1000, Loss: 0.18898111581802368\n",
      "Iteration 551/1000, Loss: 0.18882054090499878\n",
      "Iteration 552/1000, Loss: 0.18866029381752014\n",
      "Iteration 553/1000, Loss: 0.18850035965442657\n",
      "Iteration 554/1000, Loss: 0.18834078311920166\n",
      "Iteration 555/1000, Loss: 0.1881815493106842\n",
      "Iteration 556/1000, Loss: 0.18802255392074585\n",
      "Iteration 557/1000, Loss: 0.18786396086215973\n",
      "Iteration 558/1000, Loss: 0.18770566582679749\n",
      "Iteration 559/1000, Loss: 0.1875476837158203\n",
      "Iteration 560/1000, Loss: 0.18739010393619537\n",
      "Iteration 561/1000, Loss: 0.18723294138908386\n",
      "Iteration 562/1000, Loss: 0.1870761215686798\n",
      "Iteration 563/1000, Loss: 0.18691964447498322\n",
      "Iteration 564/1000, Loss: 0.18676340579986572\n",
      "Iteration 565/1000, Loss: 0.1866074800491333\n",
      "Iteration 566/1000, Loss: 0.1864517778158188\n",
      "Iteration 567/1000, Loss: 0.18629637360572815\n",
      "Iteration 568/1000, Loss: 0.1861412525177002\n",
      "Iteration 569/1000, Loss: 0.1859864592552185\n",
      "Iteration 570/1000, Loss: 0.1858319342136383\n",
      "Iteration 571/1000, Loss: 0.18567776679992676\n",
      "Iteration 572/1000, Loss: 0.18552395701408386\n",
      "Iteration 573/1000, Loss: 0.18537044525146484\n",
      "Iteration 574/1000, Loss: 0.18521718680858612\n",
      "Iteration 575/1000, Loss: 0.1850641965866089\n",
      "Iteration 576/1000, Loss: 0.18491148948669434\n",
      "Iteration 577/1000, Loss: 0.18475908041000366\n",
      "Iteration 578/1000, Loss: 0.18460699915885925\n",
      "Iteration 579/1000, Loss: 0.18445508182048798\n",
      "Iteration 580/1000, Loss: 0.1843033730983734\n",
      "Iteration 581/1000, Loss: 0.18415182828903198\n",
      "Iteration 582/1000, Loss: 0.18400049209594727\n",
      "Iteration 583/1000, Loss: 0.18384945392608643\n",
      "Iteration 584/1000, Loss: 0.1836986541748047\n",
      "Iteration 585/1000, Loss: 0.18354813754558563\n",
      "Iteration 586/1000, Loss: 0.18339788913726807\n",
      "Iteration 587/1000, Loss: 0.1832478642463684\n",
      "Iteration 588/1000, Loss: 0.1830982118844986\n",
      "Iteration 589/1000, Loss: 0.18294884264469147\n",
      "Iteration 590/1000, Loss: 0.18279974162578583\n",
      "Iteration 591/1000, Loss: 0.18265095353126526\n",
      "Iteration 592/1000, Loss: 0.18250250816345215\n",
      "Iteration 593/1000, Loss: 0.18235436081886292\n",
      "Iteration 594/1000, Loss: 0.18220652639865875\n",
      "Iteration 595/1000, Loss: 0.18205899000167847\n",
      "Iteration 596/1000, Loss: 0.1819116175174713\n",
      "Iteration 597/1000, Loss: 0.18176452815532684\n",
      "Iteration 598/1000, Loss: 0.18161773681640625\n",
      "Iteration 599/1000, Loss: 0.18147122859954834\n",
      "Iteration 600/1000, Loss: 0.18132498860359192\n",
      "Iteration 601/1000, Loss: 0.18117903172969818\n",
      "Iteration 602/1000, Loss: 0.18103332817554474\n",
      "Iteration 603/1000, Loss: 0.18088796734809875\n",
      "Iteration 604/1000, Loss: 0.18074293434619904\n",
      "Iteration 605/1000, Loss: 0.18059822916984558\n",
      "Iteration 606/1000, Loss: 0.18045367300510406\n",
      "Iteration 607/1000, Loss: 0.18030937016010284\n",
      "Iteration 608/1000, Loss: 0.1801653355360031\n",
      "Iteration 609/1000, Loss: 0.1800215095281601\n",
      "Iteration 610/1000, Loss: 0.17987802624702454\n",
      "Iteration 611/1000, Loss: 0.17973482608795166\n",
      "Iteration 612/1000, Loss: 0.17959192395210266\n",
      "Iteration 613/1000, Loss: 0.17944931983947754\n",
      "Iteration 614/1000, Loss: 0.1793069988489151\n",
      "Iteration 615/1000, Loss: 0.17916487157344818\n",
      "Iteration 616/1000, Loss: 0.17902301251888275\n",
      "Iteration 617/1000, Loss: 0.17888137698173523\n",
      "Iteration 618/1000, Loss: 0.17873996496200562\n",
      "Iteration 619/1000, Loss: 0.1785987764596939\n",
      "Iteration 620/1000, Loss: 0.1784578263759613\n",
      "Iteration 621/1000, Loss: 0.178317129611969\n",
      "Iteration 622/1000, Loss: 0.1781766712665558\n",
      "Iteration 623/1000, Loss: 0.17803649604320526\n",
      "Iteration 624/1000, Loss: 0.17789654433727264\n",
      "Iteration 625/1000, Loss: 0.1777568757534027\n",
      "Iteration 626/1000, Loss: 0.17761749029159546\n",
      "Iteration 627/1000, Loss: 0.1774783879518509\n",
      "Iteration 628/1000, Loss: 0.1773395538330078\n",
      "Iteration 629/1000, Loss: 0.17720094323158264\n",
      "Iteration 630/1000, Loss: 0.17706257104873657\n",
      "Iteration 631/1000, Loss: 0.17692440748214722\n",
      "Iteration 632/1000, Loss: 0.17678652703762054\n",
      "Iteration 633/1000, Loss: 0.17664889991283417\n",
      "Iteration 634/1000, Loss: 0.17651152610778809\n",
      "Iteration 635/1000, Loss: 0.17637445032596588\n",
      "Iteration 636/1000, Loss: 0.1762375682592392\n",
      "Iteration 637/1000, Loss: 0.1761009246110916\n",
      "Iteration 638/1000, Loss: 0.17596454918384552\n",
      "Iteration 639/1000, Loss: 0.17582841217517853\n",
      "Iteration 640/1000, Loss: 0.17569248378276825\n",
      "Iteration 641/1000, Loss: 0.17555682361125946\n",
      "Iteration 642/1000, Loss: 0.17542146146297455\n",
      "Iteration 643/1000, Loss: 0.17528636753559113\n",
      "Iteration 644/1000, Loss: 0.17515146732330322\n",
      "Iteration 645/1000, Loss: 0.17501674592494965\n",
      "Iteration 646/1000, Loss: 0.17488229274749756\n",
      "Iteration 647/1000, Loss: 0.17474810779094696\n",
      "Iteration 648/1000, Loss: 0.17461420595645905\n",
      "Iteration 649/1000, Loss: 0.17448057234287262\n",
      "Iteration 650/1000, Loss: 0.1743471324443817\n",
      "Iteration 651/1000, Loss: 0.1742139607667923\n",
      "Iteration 652/1000, Loss: 0.17408102750778198\n",
      "Iteration 653/1000, Loss: 0.17394837737083435\n",
      "Iteration 654/1000, Loss: 0.17381595075130463\n",
      "Iteration 655/1000, Loss: 0.17368382215499878\n",
      "Iteration 656/1000, Loss: 0.17355188727378845\n",
      "Iteration 657/1000, Loss: 0.17342022061347961\n",
      "Iteration 658/1000, Loss: 0.17328880727291107\n",
      "Iteration 659/1000, Loss: 0.17315757274627686\n",
      "Iteration 660/1000, Loss: 0.17302653193473816\n",
      "Iteration 661/1000, Loss: 0.1728956699371338\n",
      "Iteration 662/1000, Loss: 0.17276504635810852\n",
      "Iteration 663/1000, Loss: 0.17263461649417877\n",
      "Iteration 664/1000, Loss: 0.17250442504882812\n",
      "Iteration 665/1000, Loss: 0.17237447202205658\n",
      "Iteration 666/1000, Loss: 0.17224475741386414\n",
      "Iteration 667/1000, Loss: 0.1721152663230896\n",
      "Iteration 668/1000, Loss: 0.17198596894741058\n",
      "Iteration 669/1000, Loss: 0.1718568056821823\n",
      "Iteration 670/1000, Loss: 0.17172788083553314\n",
      "Iteration 671/1000, Loss: 0.17159922420978546\n",
      "Iteration 672/1000, Loss: 0.17147080600261688\n",
      "Iteration 673/1000, Loss: 0.17134256660938263\n",
      "Iteration 674/1000, Loss: 0.17121458053588867\n",
      "Iteration 675/1000, Loss: 0.17108681797981262\n",
      "Iteration 676/1000, Loss: 0.1709592640399933\n",
      "Iteration 677/1000, Loss: 0.17083194851875305\n",
      "Iteration 678/1000, Loss: 0.17070481181144714\n",
      "Iteration 679/1000, Loss: 0.17057792842388153\n",
      "Iteration 680/1000, Loss: 0.1704512983560562\n",
      "Iteration 681/1000, Loss: 0.17032490670681\n",
      "Iteration 682/1000, Loss: 0.17019866406917572\n",
      "Iteration 683/1000, Loss: 0.17007265985012054\n",
      "Iteration 684/1000, Loss: 0.16994674503803253\n",
      "Iteration 685/1000, Loss: 0.16982096433639526\n",
      "Iteration 686/1000, Loss: 0.1696954369544983\n",
      "Iteration 687/1000, Loss: 0.16957011818885803\n",
      "Iteration 688/1000, Loss: 0.1694450080394745\n",
      "Iteration 689/1000, Loss: 0.16932010650634766\n",
      "Iteration 690/1000, Loss: 0.16919539868831635\n",
      "Iteration 691/1000, Loss: 0.16907083988189697\n",
      "Iteration 692/1000, Loss: 0.1689465194940567\n",
      "Iteration 693/1000, Loss: 0.16882236301898956\n",
      "Iteration 694/1000, Loss: 0.16869837045669556\n",
      "Iteration 695/1000, Loss: 0.16857458651065826\n",
      "Iteration 696/1000, Loss: 0.16845104098320007\n",
      "Iteration 697/1000, Loss: 0.1683276742696762\n",
      "Iteration 698/1000, Loss: 0.16820447146892548\n",
      "Iteration 699/1000, Loss: 0.16808146238327026\n",
      "Iteration 700/1000, Loss: 0.16795869171619415\n",
      "Iteration 701/1000, Loss: 0.16783615946769714\n",
      "Iteration 702/1000, Loss: 0.16771386563777924\n",
      "Iteration 703/1000, Loss: 0.16759179532527924\n",
      "Iteration 704/1000, Loss: 0.16746991872787476\n",
      "Iteration 705/1000, Loss: 0.16734826564788818\n",
      "Iteration 706/1000, Loss: 0.16722682118415833\n",
      "Iteration 707/1000, Loss: 0.16710558533668518\n",
      "Iteration 708/1000, Loss: 0.16698454320430756\n",
      "Iteration 709/1000, Loss: 0.16686370968818665\n",
      "Iteration 710/1000, Loss: 0.16674306988716125\n",
      "Iteration 711/1000, Loss: 0.16662262380123138\n",
      "Iteration 712/1000, Loss: 0.16650235652923584\n",
      "Iteration 713/1000, Loss: 0.16638223826885223\n",
      "Iteration 714/1000, Loss: 0.16626229882240295\n",
      "Iteration 715/1000, Loss: 0.1661425083875656\n",
      "Iteration 716/1000, Loss: 0.1660229116678238\n",
      "Iteration 717/1000, Loss: 0.16590356826782227\n",
      "Iteration 718/1000, Loss: 0.16578438878059387\n",
      "Iteration 719/1000, Loss: 0.1656654328107834\n",
      "Iteration 720/1000, Loss: 0.16554662585258484\n",
      "Iteration 721/1000, Loss: 0.16542799770832062\n",
      "Iteration 722/1000, Loss: 0.16530953347682953\n",
      "Iteration 723/1000, Loss: 0.16519124805927277\n",
      "Iteration 724/1000, Loss: 0.16507314145565033\n",
      "Iteration 725/1000, Loss: 0.1649552285671234\n",
      "Iteration 726/1000, Loss: 0.16483746469020844\n",
      "Iteration 727/1000, Loss: 0.16471992433071136\n",
      "Iteration 728/1000, Loss: 0.16460250318050385\n",
      "Iteration 729/1000, Loss: 0.16448527574539185\n",
      "Iteration 730/1000, Loss: 0.16436819732189178\n",
      "Iteration 731/1000, Loss: 0.16425129771232605\n",
      "Iteration 732/1000, Loss: 0.16413457691669464\n",
      "Iteration 733/1000, Loss: 0.16401797533035278\n",
      "Iteration 734/1000, Loss: 0.16390156745910645\n",
      "Iteration 735/1000, Loss: 0.16378532350063324\n",
      "Iteration 736/1000, Loss: 0.16366925835609436\n",
      "Iteration 737/1000, Loss: 0.1635533571243286\n",
      "Iteration 738/1000, Loss: 0.1634376496076584\n",
      "Iteration 739/1000, Loss: 0.1633220762014389\n",
      "Iteration 740/1000, Loss: 0.16320666670799255\n",
      "Iteration 741/1000, Loss: 0.16309143602848053\n",
      "Iteration 742/1000, Loss: 0.16297633945941925\n",
      "Iteration 743/1000, Loss: 0.1628614217042923\n",
      "Iteration 744/1000, Loss: 0.16274665296077728\n",
      "Iteration 745/1000, Loss: 0.16263209283351898\n",
      "Iteration 746/1000, Loss: 0.16251777112483978\n",
      "Iteration 747/1000, Loss: 0.1624036282300949\n",
      "Iteration 748/1000, Loss: 0.16228961944580078\n",
      "Iteration 749/1000, Loss: 0.1621757447719574\n",
      "Iteration 750/1000, Loss: 0.16206206381320953\n",
      "Iteration 751/1000, Loss: 0.16194860637187958\n",
      "Iteration 752/1000, Loss: 0.16183528304100037\n",
      "Iteration 753/1000, Loss: 0.16172218322753906\n",
      "Iteration 754/1000, Loss: 0.1616092473268509\n",
      "Iteration 755/1000, Loss: 0.16149644553661346\n",
      "Iteration 756/1000, Loss: 0.16138383746147156\n",
      "Iteration 757/1000, Loss: 0.1612713485956192\n",
      "Iteration 758/1000, Loss: 0.16115905344486237\n",
      "Iteration 759/1000, Loss: 0.16104696691036224\n",
      "Iteration 760/1000, Loss: 0.16093508899211884\n",
      "Iteration 761/1000, Loss: 0.16082333028316498\n",
      "Iteration 762/1000, Loss: 0.16071172058582306\n",
      "Iteration 763/1000, Loss: 0.16060025990009308\n",
      "Iteration 764/1000, Loss: 0.1604890078306198\n",
      "Iteration 765/1000, Loss: 0.1603778898715973\n",
      "Iteration 766/1000, Loss: 0.16026698052883148\n",
      "Iteration 767/1000, Loss: 0.16015625\n",
      "Iteration 768/1000, Loss: 0.16004568338394165\n",
      "Iteration 769/1000, Loss: 0.15993529558181763\n",
      "Iteration 770/1000, Loss: 0.15982508659362793\n",
      "Iteration 771/1000, Loss: 0.159714937210083\n",
      "Iteration 772/1000, Loss: 0.15960495173931122\n",
      "Iteration 773/1000, Loss: 0.15949508547782898\n",
      "Iteration 774/1000, Loss: 0.15938538312911987\n",
      "Iteration 775/1000, Loss: 0.1592758744955063\n",
      "Iteration 776/1000, Loss: 0.15916647017002106\n",
      "Iteration 777/1000, Loss: 0.15905722975730896\n",
      "Iteration 778/1000, Loss: 0.1589481383562088\n",
      "Iteration 779/1000, Loss: 0.15883921086788177\n",
      "Iteration 780/1000, Loss: 0.15873046219348907\n",
      "Iteration 781/1000, Loss: 0.1586218774318695\n",
      "Iteration 782/1000, Loss: 0.15851347148418427\n",
      "Iteration 783/1000, Loss: 0.15840525925159454\n",
      "Iteration 784/1000, Loss: 0.15829716622829437\n",
      "Iteration 785/1000, Loss: 0.15818923711776733\n",
      "Iteration 786/1000, Loss: 0.15808142721652985\n",
      "Iteration 787/1000, Loss: 0.15797379612922668\n",
      "Iteration 788/1000, Loss: 0.15786634385585785\n",
      "Iteration 789/1000, Loss: 0.15775905549526215\n",
      "Iteration 790/1000, Loss: 0.15765191614627838\n",
      "Iteration 791/1000, Loss: 0.15754486620426178\n",
      "Iteration 792/1000, Loss: 0.1574380248785019\n",
      "Iteration 793/1000, Loss: 0.15733131766319275\n",
      "Iteration 794/1000, Loss: 0.15722477436065674\n",
      "Iteration 795/1000, Loss: 0.15711836516857147\n",
      "Iteration 796/1000, Loss: 0.15701206028461456\n",
      "Iteration 797/1000, Loss: 0.1569059193134308\n",
      "Iteration 798/1000, Loss: 0.15679997205734253\n",
      "Iteration 799/1000, Loss: 0.15669414401054382\n",
      "Iteration 800/1000, Loss: 0.15658849477767944\n",
      "Iteration 801/1000, Loss: 0.15648292005062103\n",
      "Iteration 802/1000, Loss: 0.15637749433517456\n",
      "Iteration 803/1000, Loss: 0.15627221763134003\n",
      "Iteration 804/1000, Loss: 0.15616711974143982\n",
      "Iteration 805/1000, Loss: 0.15606215596199036\n",
      "Iteration 806/1000, Loss: 0.15595732629299164\n",
      "Iteration 807/1000, Loss: 0.15585264563560486\n",
      "Iteration 808/1000, Loss: 0.15574811398983002\n",
      "Iteration 809/1000, Loss: 0.1556437611579895\n",
      "Iteration 810/1000, Loss: 0.15553951263427734\n",
      "Iteration 811/1000, Loss: 0.1554354727268219\n",
      "Iteration 812/1000, Loss: 0.1553315371274948\n",
      "Iteration 813/1000, Loss: 0.15522776544094086\n",
      "Iteration 814/1000, Loss: 0.15512414276599884\n",
      "Iteration 815/1000, Loss: 0.15502066910266876\n",
      "Iteration 816/1000, Loss: 0.15491732954978943\n",
      "Iteration 817/1000, Loss: 0.15481413900852203\n",
      "Iteration 818/1000, Loss: 0.15471109747886658\n",
      "Iteration 819/1000, Loss: 0.15460819005966187\n",
      "Iteration 820/1000, Loss: 0.1545053869485855\n",
      "Iteration 821/1000, Loss: 0.1544027030467987\n",
      "Iteration 822/1000, Loss: 0.15430012345314026\n",
      "Iteration 823/1000, Loss: 0.15419770777225494\n",
      "Iteration 824/1000, Loss: 0.15409545600414276\n",
      "Iteration 825/1000, Loss: 0.15399332344532013\n",
      "Iteration 826/1000, Loss: 0.15389136970043182\n",
      "Iteration 827/1000, Loss: 0.15378959476947784\n",
      "Iteration 828/1000, Loss: 0.1536879539489746\n",
      "Iteration 829/1000, Loss: 0.1535864621400833\n",
      "Iteration 830/1000, Loss: 0.15348508954048157\n",
      "Iteration 831/1000, Loss: 0.15338386595249176\n",
      "Iteration 832/1000, Loss: 0.1532827615737915\n",
      "Iteration 833/1000, Loss: 0.15318180620670319\n",
      "Iteration 834/1000, Loss: 0.15308095514774323\n",
      "Iteration 835/1000, Loss: 0.1529802829027176\n",
      "Iteration 836/1000, Loss: 0.1528797298669815\n",
      "Iteration 837/1000, Loss: 0.15277931094169617\n",
      "Iteration 838/1000, Loss: 0.15267902612686157\n",
      "Iteration 839/1000, Loss: 0.15257887542247772\n",
      "Iteration 840/1000, Loss: 0.15247885882854462\n",
      "Iteration 841/1000, Loss: 0.15237899124622345\n",
      "Iteration 842/1000, Loss: 0.15227922797203064\n",
      "Iteration 843/1000, Loss: 0.15217965841293335\n",
      "Iteration 844/1000, Loss: 0.1520802080631256\n",
      "Iteration 845/1000, Loss: 0.151980921626091\n",
      "Iteration 846/1000, Loss: 0.15188176929950714\n",
      "Iteration 847/1000, Loss: 0.15178275108337402\n",
      "Iteration 848/1000, Loss: 0.15168388187885284\n",
      "Iteration 849/1000, Loss: 0.15158511698246002\n",
      "Iteration 850/1000, Loss: 0.1514865607023239\n",
      "Iteration 851/1000, Loss: 0.15138810873031616\n",
      "Iteration 852/1000, Loss: 0.15128979086875916\n",
      "Iteration 853/1000, Loss: 0.1511915624141693\n",
      "Iteration 854/1000, Loss: 0.1510934829711914\n",
      "Iteration 855/1000, Loss: 0.15099550783634186\n",
      "Iteration 856/1000, Loss: 0.15089765191078186\n",
      "Iteration 857/1000, Loss: 0.1507999151945114\n",
      "Iteration 858/1000, Loss: 0.1507023125886917\n",
      "Iteration 859/1000, Loss: 0.15060484409332275\n",
      "Iteration 860/1000, Loss: 0.15050755441188812\n",
      "Iteration 861/1000, Loss: 0.15041041374206543\n",
      "Iteration 862/1000, Loss: 0.15031340718269348\n",
      "Iteration 863/1000, Loss: 0.15021653473377228\n",
      "Iteration 864/1000, Loss: 0.15011975169181824\n",
      "Iteration 865/1000, Loss: 0.15002310276031494\n",
      "Iteration 866/1000, Loss: 0.14992661774158478\n",
      "Iteration 867/1000, Loss: 0.14983025193214417\n",
      "Iteration 868/1000, Loss: 0.14973405003547668\n",
      "Iteration 869/1000, Loss: 0.14963795244693756\n",
      "Iteration 870/1000, Loss: 0.149541974067688\n",
      "Iteration 871/1000, Loss: 0.14944612979888916\n",
      "Iteration 872/1000, Loss: 0.1493503898382187\n",
      "Iteration 873/1000, Loss: 0.14925473928451538\n",
      "Iteration 874/1000, Loss: 0.14915919303894043\n",
      "Iteration 875/1000, Loss: 0.14906376600265503\n",
      "Iteration 876/1000, Loss: 0.14896847307682037\n",
      "Iteration 877/1000, Loss: 0.14887332916259766\n",
      "Iteration 878/1000, Loss: 0.1487782895565033\n",
      "Iteration 879/1000, Loss: 0.14868339896202087\n",
      "Iteration 880/1000, Loss: 0.14858856797218323\n",
      "Iteration 881/1000, Loss: 0.14849384129047394\n",
      "Iteration 882/1000, Loss: 0.148399218916893\n",
      "Iteration 883/1000, Loss: 0.1483047753572464\n",
      "Iteration 884/1000, Loss: 0.14821043610572815\n",
      "Iteration 885/1000, Loss: 0.14811621606349945\n",
      "Iteration 886/1000, Loss: 0.1480221301317215\n",
      "Iteration 887/1000, Loss: 0.14792819321155548\n",
      "Iteration 888/1000, Loss: 0.14783437550067902\n",
      "Iteration 889/1000, Loss: 0.14774072170257568\n",
      "Iteration 890/1000, Loss: 0.1476471722126007\n",
      "Iteration 891/1000, Loss: 0.1475537270307541\n",
      "Iteration 892/1000, Loss: 0.1474604457616806\n",
      "Iteration 893/1000, Loss: 0.14736732840538025\n",
      "Iteration 894/1000, Loss: 0.14727433025836945\n",
      "Iteration 895/1000, Loss: 0.147181436419487\n",
      "Iteration 896/1000, Loss: 0.14708870649337769\n",
      "Iteration 897/1000, Loss: 0.14699611067771912\n",
      "Iteration 898/1000, Loss: 0.1469036489725113\n",
      "Iteration 899/1000, Loss: 0.14681126177310944\n",
      "Iteration 900/1000, Loss: 0.14671897888183594\n",
      "Iteration 901/1000, Loss: 0.14662683010101318\n",
      "Iteration 902/1000, Loss: 0.14653471112251282\n",
      "Iteration 903/1000, Loss: 0.14644277095794678\n",
      "Iteration 904/1000, Loss: 0.14635097980499268\n",
      "Iteration 905/1000, Loss: 0.14625930786132812\n",
      "Iteration 906/1000, Loss: 0.14616775512695312\n",
      "Iteration 907/1000, Loss: 0.14607633650302887\n",
      "Iteration 908/1000, Loss: 0.14598500728607178\n",
      "Iteration 909/1000, Loss: 0.14589381217956543\n",
      "Iteration 910/1000, Loss: 0.14580273628234863\n",
      "Iteration 911/1000, Loss: 0.1457117795944214\n",
      "Iteration 912/1000, Loss: 0.1456209272146225\n",
      "Iteration 913/1000, Loss: 0.14553020894527435\n",
      "Iteration 914/1000, Loss: 0.14543955028057098\n",
      "Iteration 915/1000, Loss: 0.14534898102283478\n",
      "Iteration 916/1000, Loss: 0.14525851607322693\n",
      "Iteration 917/1000, Loss: 0.14516817033290863\n",
      "Iteration 918/1000, Loss: 0.14507798850536346\n",
      "Iteration 919/1000, Loss: 0.14498791098594666\n",
      "Iteration 920/1000, Loss: 0.1448979377746582\n",
      "Iteration 921/1000, Loss: 0.1448080986738205\n",
      "Iteration 922/1000, Loss: 0.14471842348575592\n",
      "Iteration 923/1000, Loss: 0.1446288377046585\n",
      "Iteration 924/1000, Loss: 0.14453934133052826\n",
      "Iteration 925/1000, Loss: 0.14444996416568756\n",
      "Iteration 926/1000, Loss: 0.14436069130897522\n",
      "Iteration 927/1000, Loss: 0.14427155256271362\n",
      "Iteration 928/1000, Loss: 0.14418253302574158\n",
      "Iteration 929/1000, Loss: 0.1440935879945755\n",
      "Iteration 930/1000, Loss: 0.14400474727153778\n",
      "Iteration 931/1000, Loss: 0.14391598105430603\n",
      "Iteration 932/1000, Loss: 0.14382727444171906\n",
      "Iteration 933/1000, Loss: 0.14373870193958282\n",
      "Iteration 934/1000, Loss: 0.14365023374557495\n",
      "Iteration 935/1000, Loss: 0.14356188476085663\n",
      "Iteration 936/1000, Loss: 0.14347359538078308\n",
      "Iteration 937/1000, Loss: 0.14338545501232147\n",
      "Iteration 938/1000, Loss: 0.14329741895198822\n",
      "Iteration 939/1000, Loss: 0.14320948719978333\n",
      "Iteration 940/1000, Loss: 0.14312171936035156\n",
      "Iteration 941/1000, Loss: 0.14303401112556458\n",
      "Iteration 942/1000, Loss: 0.14294639229774475\n",
      "Iteration 943/1000, Loss: 0.14285892248153687\n",
      "Iteration 944/1000, Loss: 0.14277152717113495\n",
      "Iteration 945/1000, Loss: 0.1426842212677002\n",
      "Iteration 946/1000, Loss: 0.1425970494747162\n",
      "Iteration 947/1000, Loss: 0.14250996708869934\n",
      "Iteration 948/1000, Loss: 0.14242303371429443\n",
      "Iteration 949/1000, Loss: 0.14233621954917908\n",
      "Iteration 950/1000, Loss: 0.14224953949451447\n",
      "Iteration 951/1000, Loss: 0.14216294884681702\n",
      "Iteration 952/1000, Loss: 0.14207644760608673\n",
      "Iteration 953/1000, Loss: 0.1419900357723236\n",
      "Iteration 954/1000, Loss: 0.14190374314785004\n",
      "Iteration 955/1000, Loss: 0.14181749522686005\n",
      "Iteration 956/1000, Loss: 0.1417313516139984\n",
      "Iteration 957/1000, Loss: 0.14164528250694275\n",
      "Iteration 958/1000, Loss: 0.14155930280685425\n",
      "Iteration 959/1000, Loss: 0.1414734423160553\n",
      "Iteration 960/1000, Loss: 0.14138765633106232\n",
      "Iteration 961/1000, Loss: 0.1413019746541977\n",
      "Iteration 962/1000, Loss: 0.14121639728546143\n",
      "Iteration 963/1000, Loss: 0.14113090932369232\n",
      "Iteration 964/1000, Loss: 0.14104554057121277\n",
      "Iteration 965/1000, Loss: 0.14096029102802277\n",
      "Iteration 966/1000, Loss: 0.14087514579296112\n",
      "Iteration 967/1000, Loss: 0.14079011976718903\n",
      "Iteration 968/1000, Loss: 0.1407051831483841\n",
      "Iteration 969/1000, Loss: 0.14062035083770752\n",
      "Iteration 970/1000, Loss: 0.1405356079339981\n",
      "Iteration 971/1000, Loss: 0.14045096933841705\n",
      "Iteration 972/1000, Loss: 0.14036646485328674\n",
      "Iteration 973/1000, Loss: 0.1402820348739624\n",
      "Iteration 974/1000, Loss: 0.1401977241039276\n",
      "Iteration 975/1000, Loss: 0.14011354744434357\n",
      "Iteration 976/1000, Loss: 0.14002946019172668\n",
      "Iteration 977/1000, Loss: 0.13994550704956055\n",
      "Iteration 978/1000, Loss: 0.13986164331436157\n",
      "Iteration 979/1000, Loss: 0.13977788388729095\n",
      "Iteration 980/1000, Loss: 0.13969416916370392\n",
      "Iteration 981/1000, Loss: 0.13961054384708405\n",
      "Iteration 982/1000, Loss: 0.13952699303627014\n",
      "Iteration 983/1000, Loss: 0.1394435614347458\n",
      "Iteration 984/1000, Loss: 0.1393602192401886\n",
      "Iteration 985/1000, Loss: 0.13927698135375977\n",
      "Iteration 986/1000, Loss: 0.1391938328742981\n",
      "Iteration 987/1000, Loss: 0.1391107589006424\n",
      "Iteration 988/1000, Loss: 0.13902780413627625\n",
      "Iteration 989/1000, Loss: 0.13894489407539368\n",
      "Iteration 990/1000, Loss: 0.13886214792728424\n",
      "Iteration 991/1000, Loss: 0.13877947628498077\n",
      "Iteration 992/1000, Loss: 0.13869693875312805\n",
      "Iteration 993/1000, Loss: 0.1386144906282425\n",
      "Iteration 994/1000, Loss: 0.1385321319103241\n",
      "Iteration 995/1000, Loss: 0.13844986259937286\n",
      "Iteration 996/1000, Loss: 0.13836771249771118\n",
      "Iteration 997/1000, Loss: 0.13828562200069427\n",
      "Iteration 998/1000, Loss: 0.13820365071296692\n",
      "Iteration 999/1000, Loss: 0.13812170922756195\n",
      "Iteration 1000/1000, Loss: 0.13803984224796295\n",
      "Pruning Step 2\n",
      "Iteration 1/1000, Loss: 1.9187960624694824\n",
      "Iteration 2/1000, Loss: 1.7982374429702759\n",
      "Iteration 3/1000, Loss: 1.6992346048355103\n",
      "Iteration 4/1000, Loss: 1.607883095741272\n",
      "Iteration 5/1000, Loss: 1.5222967863082886\n",
      "Iteration 6/1000, Loss: 1.4420220851898193\n",
      "Iteration 7/1000, Loss: 1.3669853210449219\n",
      "Iteration 8/1000, Loss: 1.2969670295715332\n",
      "Iteration 9/1000, Loss: 1.2317992448806763\n",
      "Iteration 10/1000, Loss: 1.1712977886199951\n",
      "Iteration 11/1000, Loss: 1.1153109073638916\n",
      "Iteration 12/1000, Loss: 1.0636320114135742\n",
      "Iteration 13/1000, Loss: 1.0160493850708008\n",
      "Iteration 14/1000, Loss: 0.9722880125045776\n",
      "Iteration 15/1000, Loss: 0.9321260452270508\n",
      "Iteration 16/1000, Loss: 0.895286500453949\n",
      "Iteration 17/1000, Loss: 0.8614863157272339\n",
      "Iteration 18/1000, Loss: 0.830447793006897\n",
      "Iteration 19/1000, Loss: 0.8019136786460876\n",
      "Iteration 20/1000, Loss: 0.7756582498550415\n",
      "Iteration 21/1000, Loss: 0.7514647841453552\n",
      "Iteration 22/1000, Loss: 0.7291259169578552\n",
      "Iteration 23/1000, Loss: 0.7084634304046631\n",
      "Iteration 24/1000, Loss: 0.6893213391304016\n",
      "Iteration 25/1000, Loss: 0.6715559959411621\n",
      "Iteration 26/1000, Loss: 0.6550397276878357\n",
      "Iteration 27/1000, Loss: 0.6396577954292297\n",
      "Iteration 28/1000, Loss: 0.625302255153656\n",
      "Iteration 29/1000, Loss: 0.6118857860565186\n",
      "Iteration 30/1000, Loss: 0.5993199348449707\n",
      "Iteration 31/1000, Loss: 0.5875328183174133\n",
      "Iteration 32/1000, Loss: 0.5764560103416443\n",
      "Iteration 33/1000, Loss: 0.5660269260406494\n",
      "Iteration 34/1000, Loss: 0.5561970472335815\n",
      "Iteration 35/1000, Loss: 0.546917200088501\n",
      "Iteration 36/1000, Loss: 0.5381402373313904\n",
      "Iteration 37/1000, Loss: 0.5298283696174622\n",
      "Iteration 38/1000, Loss: 0.5219460725784302\n",
      "Iteration 39/1000, Loss: 0.5144610404968262\n",
      "Iteration 40/1000, Loss: 0.5073438882827759\n",
      "Iteration 41/1000, Loss: 0.5005679130554199\n",
      "Iteration 42/1000, Loss: 0.4941082298755646\n",
      "Iteration 43/1000, Loss: 0.4879438281059265\n",
      "Iteration 44/1000, Loss: 0.48205503821372986\n",
      "Iteration 45/1000, Loss: 0.47642213106155396\n",
      "Iteration 46/1000, Loss: 0.47102734446525574\n",
      "Iteration 47/1000, Loss: 0.46585705876350403\n",
      "Iteration 48/1000, Loss: 0.4608969986438751\n",
      "Iteration 49/1000, Loss: 0.45613548159599304\n",
      "Iteration 50/1000, Loss: 0.4515591859817505\n",
      "Iteration 51/1000, Loss: 0.4471563696861267\n",
      "Iteration 52/1000, Loss: 0.44291868805885315\n",
      "Iteration 53/1000, Loss: 0.43883606791496277\n",
      "Iteration 54/1000, Loss: 0.4348995089530945\n",
      "Iteration 55/1000, Loss: 0.43110060691833496\n",
      "Iteration 56/1000, Loss: 0.4274330735206604\n",
      "Iteration 57/1000, Loss: 0.42389023303985596\n",
      "Iteration 58/1000, Loss: 0.42046427726745605\n",
      "Iteration 59/1000, Loss: 0.4171496629714966\n",
      "Iteration 60/1000, Loss: 0.4139400124549866\n",
      "Iteration 61/1000, Loss: 0.4108312427997589\n",
      "Iteration 62/1000, Loss: 0.4078182578086853\n",
      "Iteration 63/1000, Loss: 0.4048960208892822\n",
      "Iteration 64/1000, Loss: 0.4020597040653229\n",
      "Iteration 65/1000, Loss: 0.39930588006973267\n",
      "Iteration 66/1000, Loss: 0.39663049578666687\n",
      "Iteration 67/1000, Loss: 0.3940299153327942\n",
      "Iteration 68/1000, Loss: 0.39150115847587585\n",
      "Iteration 69/1000, Loss: 0.3890412151813507\n",
      "Iteration 70/1000, Loss: 0.38664641976356506\n",
      "Iteration 71/1000, Loss: 0.38431432843208313\n",
      "Iteration 72/1000, Loss: 0.3820425868034363\n",
      "Iteration 73/1000, Loss: 0.37982815504074097\n",
      "Iteration 74/1000, Loss: 0.3776690661907196\n",
      "Iteration 75/1000, Loss: 0.37556329369544983\n",
      "Iteration 76/1000, Loss: 0.37350836396217346\n",
      "Iteration 77/1000, Loss: 0.37150251865386963\n",
      "Iteration 78/1000, Loss: 0.36954376101493835\n",
      "Iteration 79/1000, Loss: 0.3676304817199707\n",
      "Iteration 80/1000, Loss: 0.3657606244087219\n",
      "Iteration 81/1000, Loss: 0.36393287777900696\n",
      "Iteration 82/1000, Loss: 0.36214542388916016\n",
      "Iteration 83/1000, Loss: 0.36039623618125916\n",
      "Iteration 84/1000, Loss: 0.3586844503879547\n",
      "Iteration 85/1000, Loss: 0.35700902342796326\n",
      "Iteration 86/1000, Loss: 0.3553682267665863\n",
      "Iteration 87/1000, Loss: 0.353761225938797\n",
      "Iteration 88/1000, Loss: 0.352186918258667\n",
      "Iteration 89/1000, Loss: 0.3506441116333008\n",
      "Iteration 90/1000, Loss: 0.3491317629814148\n",
      "Iteration 91/1000, Loss: 0.34764862060546875\n",
      "Iteration 92/1000, Loss: 0.34619376063346863\n",
      "Iteration 93/1000, Loss: 0.3447657525539398\n",
      "Iteration 94/1000, Loss: 0.3433643579483032\n",
      "Iteration 95/1000, Loss: 0.34198832511901855\n",
      "Iteration 96/1000, Loss: 0.34063708782196045\n",
      "Iteration 97/1000, Loss: 0.3393097519874573\n",
      "Iteration 98/1000, Loss: 0.33800560235977173\n",
      "Iteration 99/1000, Loss: 0.33672380447387695\n",
      "Iteration 100/1000, Loss: 0.33546367287635803\n",
      "Iteration 101/1000, Loss: 0.3342248499393463\n",
      "Iteration 102/1000, Loss: 0.3330065608024597\n",
      "Iteration 103/1000, Loss: 0.331807941198349\n",
      "Iteration 104/1000, Loss: 0.33062830567359924\n",
      "Iteration 105/1000, Loss: 0.3294673264026642\n",
      "Iteration 106/1000, Loss: 0.32832467555999756\n",
      "Iteration 107/1000, Loss: 0.32719990611076355\n",
      "Iteration 108/1000, Loss: 0.32609251141548157\n",
      "Iteration 109/1000, Loss: 0.32500213384628296\n",
      "Iteration 110/1000, Loss: 0.3239280879497528\n",
      "Iteration 111/1000, Loss: 0.3228699266910553\n",
      "Iteration 112/1000, Loss: 0.32182714343070984\n",
      "Iteration 113/1000, Loss: 0.320799320936203\n",
      "Iteration 114/1000, Loss: 0.3197861909866333\n",
      "Iteration 115/1000, Loss: 0.3187873661518097\n",
      "Iteration 116/1000, Loss: 0.3178025484085083\n",
      "Iteration 117/1000, Loss: 0.3168313205242157\n",
      "Iteration 118/1000, Loss: 0.31587326526641846\n",
      "Iteration 119/1000, Loss: 0.31492844223976135\n",
      "Iteration 120/1000, Loss: 0.3139961063861847\n",
      "Iteration 121/1000, Loss: 0.3130761981010437\n",
      "Iteration 122/1000, Loss: 0.31216830015182495\n",
      "Iteration 123/1000, Loss: 0.311271995306015\n",
      "Iteration 124/1000, Loss: 0.31038713455200195\n",
      "Iteration 125/1000, Loss: 0.3095134496688843\n",
      "Iteration 126/1000, Loss: 0.30865034461021423\n",
      "Iteration 127/1000, Loss: 0.30779770016670227\n",
      "Iteration 128/1000, Loss: 0.30695539712905884\n",
      "Iteration 129/1000, Loss: 0.306123286485672\n",
      "Iteration 130/1000, Loss: 0.30530089139938354\n",
      "Iteration 131/1000, Loss: 0.3044881820678711\n",
      "Iteration 132/1000, Loss: 0.30368494987487793\n",
      "Iteration 133/1000, Loss: 0.3028910756111145\n",
      "Iteration 134/1000, Loss: 0.30210644006729126\n",
      "Iteration 135/1000, Loss: 0.30133092403411865\n",
      "Iteration 136/1000, Loss: 0.3005641996860504\n",
      "Iteration 137/1000, Loss: 0.2998061776161194\n",
      "Iteration 138/1000, Loss: 0.29905644059181213\n",
      "Iteration 139/1000, Loss: 0.29831457138061523\n",
      "Iteration 140/1000, Loss: 0.29758039116859436\n",
      "Iteration 141/1000, Loss: 0.2968537211418152\n",
      "Iteration 142/1000, Loss: 0.2961345314979553\n",
      "Iteration 143/1000, Loss: 0.29542282223701477\n",
      "Iteration 144/1000, Loss: 0.2947182357311249\n",
      "Iteration 145/1000, Loss: 0.2940208315849304\n",
      "Iteration 146/1000, Loss: 0.2933303713798523\n",
      "Iteration 147/1000, Loss: 0.29264673590660095\n",
      "Iteration 148/1000, Loss: 0.2919696569442749\n",
      "Iteration 149/1000, Loss: 0.29129916429519653\n",
      "Iteration 150/1000, Loss: 0.2906351685523987\n",
      "Iteration 151/1000, Loss: 0.2899772822856903\n",
      "Iteration 152/1000, Loss: 0.289325475692749\n",
      "Iteration 153/1000, Loss: 0.2886800169944763\n",
      "Iteration 154/1000, Loss: 0.28804048895835876\n",
      "Iteration 155/1000, Loss: 0.2874065935611725\n",
      "Iteration 156/1000, Loss: 0.28677818179130554\n",
      "Iteration 157/1000, Loss: 0.28615546226501465\n",
      "Iteration 158/1000, Loss: 0.2855379283428192\n",
      "Iteration 159/1000, Loss: 0.2849258482456207\n",
      "Iteration 160/1000, Loss: 0.28431907296180725\n",
      "Iteration 161/1000, Loss: 0.2837175726890564\n",
      "Iteration 162/1000, Loss: 0.2831212878227234\n",
      "Iteration 163/1000, Loss: 0.2825300097465515\n",
      "Iteration 164/1000, Loss: 0.28194373846054077\n",
      "Iteration 165/1000, Loss: 0.28136247396469116\n",
      "Iteration 166/1000, Loss: 0.2807859182357788\n",
      "Iteration 167/1000, Loss: 0.2802138328552246\n",
      "Iteration 168/1000, Loss: 0.2796463668346405\n",
      "Iteration 169/1000, Loss: 0.2790836989879608\n",
      "Iteration 170/1000, Loss: 0.27852535247802734\n",
      "Iteration 171/1000, Loss: 0.27797141671180725\n",
      "Iteration 172/1000, Loss: 0.2774217426776886\n",
      "Iteration 173/1000, Loss: 0.276876300573349\n",
      "Iteration 174/1000, Loss: 0.27633529901504517\n",
      "Iteration 175/1000, Loss: 0.275798499584198\n",
      "Iteration 176/1000, Loss: 0.27526581287384033\n",
      "Iteration 177/1000, Loss: 0.2747371196746826\n",
      "Iteration 178/1000, Loss: 0.2742123603820801\n",
      "Iteration 179/1000, Loss: 0.2736915647983551\n",
      "Iteration 180/1000, Loss: 0.2731746435165405\n",
      "Iteration 181/1000, Loss: 0.2726614475250244\n",
      "Iteration 182/1000, Loss: 0.2721518874168396\n",
      "Iteration 183/1000, Loss: 0.27164602279663086\n",
      "Iteration 184/1000, Loss: 0.2711436450481415\n",
      "Iteration 185/1000, Loss: 0.270644873380661\n",
      "Iteration 186/1000, Loss: 0.2701497972011566\n",
      "Iteration 187/1000, Loss: 0.2696583867073059\n",
      "Iteration 188/1000, Loss: 0.2691703140735626\n",
      "Iteration 189/1000, Loss: 0.26868581771850586\n",
      "Iteration 190/1000, Loss: 0.2682044804096222\n",
      "Iteration 191/1000, Loss: 0.26772648096084595\n",
      "Iteration 192/1000, Loss: 0.2672517001628876\n",
      "Iteration 193/1000, Loss: 0.26678013801574707\n",
      "Iteration 194/1000, Loss: 0.26631176471710205\n",
      "Iteration 195/1000, Loss: 0.2658464014530182\n",
      "Iteration 196/1000, Loss: 0.26538413763046265\n",
      "Iteration 197/1000, Loss: 0.26492491364479065\n",
      "Iteration 198/1000, Loss: 0.264468789100647\n",
      "Iteration 199/1000, Loss: 0.26401564478874207\n",
      "Iteration 200/1000, Loss: 0.263565331697464\n",
      "Iteration 201/1000, Loss: 0.2631179392337799\n",
      "Iteration 202/1000, Loss: 0.2626734673976898\n",
      "Iteration 203/1000, Loss: 0.2622317969799042\n",
      "Iteration 204/1000, Loss: 0.26179251074790955\n",
      "Iteration 205/1000, Loss: 0.2613556385040283\n",
      "Iteration 206/1000, Loss: 0.2609213888645172\n",
      "Iteration 207/1000, Loss: 0.26048967242240906\n",
      "Iteration 208/1000, Loss: 0.26006048917770386\n",
      "Iteration 209/1000, Loss: 0.25963377952575684\n",
      "Iteration 210/1000, Loss: 0.25920945405960083\n",
      "Iteration 211/1000, Loss: 0.2587876319885254\n",
      "Iteration 212/1000, Loss: 0.25836819410324097\n",
      "Iteration 213/1000, Loss: 0.25795117020606995\n",
      "Iteration 214/1000, Loss: 0.25753650069236755\n",
      "Iteration 215/1000, Loss: 0.25712406635284424\n",
      "Iteration 216/1000, Loss: 0.25671395659446716\n",
      "Iteration 217/1000, Loss: 0.25630611181259155\n",
      "Iteration 218/1000, Loss: 0.2559005618095398\n",
      "Iteration 219/1000, Loss: 0.25549712777137756\n",
      "Iteration 220/1000, Loss: 0.2550957500934601\n",
      "Iteration 221/1000, Loss: 0.2546965479850769\n",
      "Iteration 222/1000, Loss: 0.2542996406555176\n",
      "Iteration 223/1000, Loss: 0.253904789686203\n",
      "Iteration 224/1000, Loss: 0.25351205468177795\n",
      "Iteration 225/1000, Loss: 0.25312143564224243\n",
      "Iteration 226/1000, Loss: 0.2527328431606293\n",
      "Iteration 227/1000, Loss: 0.2523461878299713\n",
      "Iteration 228/1000, Loss: 0.2519616484642029\n",
      "Iteration 229/1000, Loss: 0.2515791058540344\n",
      "Iteration 230/1000, Loss: 0.25119853019714355\n",
      "Iteration 231/1000, Loss: 0.25081995129585266\n",
      "Iteration 232/1000, Loss: 0.25044330954551697\n",
      "Iteration 233/1000, Loss: 0.250068724155426\n",
      "Iteration 234/1000, Loss: 0.2496960312128067\n",
      "Iteration 235/1000, Loss: 0.2493252456188202\n",
      "Iteration 236/1000, Loss: 0.24895638227462769\n",
      "Iteration 237/1000, Loss: 0.2485891878604889\n",
      "Iteration 238/1000, Loss: 0.24822388589382172\n",
      "Iteration 239/1000, Loss: 0.24786047637462616\n",
      "Iteration 240/1000, Loss: 0.24749895930290222\n",
      "Iteration 241/1000, Loss: 0.2471391260623932\n",
      "Iteration 242/1000, Loss: 0.24678096175193787\n",
      "Iteration 243/1000, Loss: 0.24642443656921387\n",
      "Iteration 244/1000, Loss: 0.24606956541538239\n",
      "Iteration 245/1000, Loss: 0.24571649730205536\n",
      "Iteration 246/1000, Loss: 0.24536505341529846\n",
      "Iteration 247/1000, Loss: 0.2450152337551117\n",
      "Iteration 248/1000, Loss: 0.2446669489145279\n",
      "Iteration 249/1000, Loss: 0.2443201243877411\n",
      "Iteration 250/1000, Loss: 0.24397481977939606\n",
      "Iteration 251/1000, Loss: 0.2436310052871704\n",
      "Iteration 252/1000, Loss: 0.24328890442848206\n",
      "Iteration 253/1000, Loss: 0.24294832348823547\n",
      "Iteration 254/1000, Loss: 0.24260957539081573\n",
      "Iteration 255/1000, Loss: 0.24227239191532135\n",
      "Iteration 256/1000, Loss: 0.24193668365478516\n",
      "Iteration 257/1000, Loss: 0.2416023463010788\n",
      "Iteration 258/1000, Loss: 0.2412695586681366\n",
      "Iteration 259/1000, Loss: 0.24093811213970184\n",
      "Iteration 260/1000, Loss: 0.2406080663204193\n",
      "Iteration 261/1000, Loss: 0.24027946591377258\n",
      "Iteration 262/1000, Loss: 0.23995229601860046\n",
      "Iteration 263/1000, Loss: 0.23962652683258057\n",
      "Iteration 264/1000, Loss: 0.23930205404758453\n",
      "Iteration 265/1000, Loss: 0.23897896707057953\n",
      "Iteration 266/1000, Loss: 0.23865726590156555\n",
      "Iteration 267/1000, Loss: 0.23833692073822021\n",
      "Iteration 268/1000, Loss: 0.23801787197589874\n",
      "Iteration 269/1000, Loss: 0.23770025372505188\n",
      "Iteration 270/1000, Loss: 0.23738400638103485\n",
      "Iteration 271/1000, Loss: 0.2370690256357193\n",
      "Iteration 272/1000, Loss: 0.23675529658794403\n",
      "Iteration 273/1000, Loss: 0.23644286394119263\n",
      "Iteration 274/1000, Loss: 0.23613177239894867\n",
      "Iteration 275/1000, Loss: 0.23582206666469574\n",
      "Iteration 276/1000, Loss: 0.2355135977268219\n",
      "Iteration 277/1000, Loss: 0.23520627617835999\n",
      "Iteration 278/1000, Loss: 0.23490026593208313\n",
      "Iteration 279/1000, Loss: 0.23459553718566895\n",
      "Iteration 280/1000, Loss: 0.23429211974143982\n",
      "Iteration 281/1000, Loss: 0.23399004340171814\n",
      "Iteration 282/1000, Loss: 0.23368921875953674\n",
      "Iteration 283/1000, Loss: 0.23338967561721802\n",
      "Iteration 284/1000, Loss: 0.23309116065502167\n",
      "Iteration 285/1000, Loss: 0.23279376327991486\n",
      "Iteration 286/1000, Loss: 0.2324974536895752\n",
      "Iteration 287/1000, Loss: 0.2322022020816803\n",
      "Iteration 288/1000, Loss: 0.23190800845623016\n",
      "Iteration 289/1000, Loss: 0.23161500692367554\n",
      "Iteration 290/1000, Loss: 0.23132306337356567\n",
      "Iteration 291/1000, Loss: 0.2310321033000946\n",
      "Iteration 292/1000, Loss: 0.2307421863079071\n",
      "Iteration 293/1000, Loss: 0.2304532527923584\n",
      "Iteration 294/1000, Loss: 0.230165496468544\n",
      "Iteration 295/1000, Loss: 0.22987887263298035\n",
      "Iteration 296/1000, Loss: 0.22959336638450623\n",
      "Iteration 297/1000, Loss: 0.2293090522289276\n",
      "Iteration 298/1000, Loss: 0.22902578115463257\n",
      "Iteration 299/1000, Loss: 0.22874367237091064\n",
      "Iteration 300/1000, Loss: 0.22846269607543945\n",
      "Iteration 301/1000, Loss: 0.22818274796009064\n",
      "Iteration 302/1000, Loss: 0.2279038429260254\n",
      "Iteration 303/1000, Loss: 0.22762607038021088\n",
      "Iteration 304/1000, Loss: 0.22734925150871277\n",
      "Iteration 305/1000, Loss: 0.22707340121269226\n",
      "Iteration 306/1000, Loss: 0.22679856419563293\n",
      "Iteration 307/1000, Loss: 0.22652463614940643\n",
      "Iteration 308/1000, Loss: 0.22625169157981873\n",
      "Iteration 309/1000, Loss: 0.22597968578338623\n",
      "Iteration 310/1000, Loss: 0.22570857405662537\n",
      "Iteration 311/1000, Loss: 0.22543840110301971\n",
      "Iteration 312/1000, Loss: 0.2251691371202469\n",
      "Iteration 313/1000, Loss: 0.22490081191062927\n",
      "Iteration 314/1000, Loss: 0.22463347017765045\n",
      "Iteration 315/1000, Loss: 0.2243669182062149\n",
      "Iteration 316/1000, Loss: 0.22410136461257935\n",
      "Iteration 317/1000, Loss: 0.22383669018745422\n",
      "Iteration 318/1000, Loss: 0.22357288002967834\n",
      "Iteration 319/1000, Loss: 0.22331002354621887\n",
      "Iteration 320/1000, Loss: 0.22304800152778625\n",
      "Iteration 321/1000, Loss: 0.22278694808483124\n",
      "Iteration 322/1000, Loss: 0.2225266396999359\n",
      "Iteration 323/1000, Loss: 0.2222672551870346\n",
      "Iteration 324/1000, Loss: 0.22200888395309448\n",
      "Iteration 325/1000, Loss: 0.22175155580043793\n",
      "Iteration 326/1000, Loss: 0.22149516642093658\n",
      "Iteration 327/1000, Loss: 0.2212395966053009\n",
      "Iteration 328/1000, Loss: 0.22098487615585327\n",
      "Iteration 329/1000, Loss: 0.22073093056678772\n",
      "Iteration 330/1000, Loss: 0.220477893948555\n",
      "Iteration 331/1000, Loss: 0.2202257364988327\n",
      "Iteration 332/1000, Loss: 0.2199743390083313\n",
      "Iteration 333/1000, Loss: 0.21972374618053436\n",
      "Iteration 334/1000, Loss: 0.21947401762008667\n",
      "Iteration 335/1000, Loss: 0.21922515332698822\n",
      "Iteration 336/1000, Loss: 0.21897710859775543\n",
      "Iteration 337/1000, Loss: 0.2187299132347107\n",
      "Iteration 338/1000, Loss: 0.21848346292972565\n",
      "Iteration 339/1000, Loss: 0.2182377576828003\n",
      "Iteration 340/1000, Loss: 0.2179928719997406\n",
      "Iteration 341/1000, Loss: 0.217748761177063\n",
      "Iteration 342/1000, Loss: 0.2175053358078003\n",
      "Iteration 343/1000, Loss: 0.21726271510124207\n",
      "Iteration 344/1000, Loss: 0.21702079474925995\n",
      "Iteration 345/1000, Loss: 0.21677955985069275\n",
      "Iteration 346/1000, Loss: 0.21653905510902405\n",
      "Iteration 347/1000, Loss: 0.2162994146347046\n",
      "Iteration 348/1000, Loss: 0.21606051921844482\n",
      "Iteration 349/1000, Loss: 0.21582229435443878\n",
      "Iteration 350/1000, Loss: 0.21558479964733124\n",
      "Iteration 351/1000, Loss: 0.21534819900989532\n",
      "Iteration 352/1000, Loss: 0.21511243283748627\n",
      "Iteration 353/1000, Loss: 0.21487733721733093\n",
      "Iteration 354/1000, Loss: 0.2146429717540741\n",
      "Iteration 355/1000, Loss: 0.2144092172384262\n",
      "Iteration 356/1000, Loss: 0.21417617797851562\n",
      "Iteration 357/1000, Loss: 0.21394391357898712\n",
      "Iteration 358/1000, Loss: 0.21371246874332428\n",
      "Iteration 359/1000, Loss: 0.2134818732738495\n",
      "Iteration 360/1000, Loss: 0.21325205266475677\n",
      "Iteration 361/1000, Loss: 0.21302294731140137\n",
      "Iteration 362/1000, Loss: 0.21279451251029968\n",
      "Iteration 363/1000, Loss: 0.21256670355796814\n",
      "Iteration 364/1000, Loss: 0.21233944594860077\n",
      "Iteration 365/1000, Loss: 0.2121128886938095\n",
      "Iteration 366/1000, Loss: 0.21188700199127197\n",
      "Iteration 367/1000, Loss: 0.2116616815328598\n",
      "Iteration 368/1000, Loss: 0.2114369422197342\n",
      "Iteration 369/1000, Loss: 0.21121279895305634\n",
      "Iteration 370/1000, Loss: 0.21098925173282623\n",
      "Iteration 371/1000, Loss: 0.21076643466949463\n",
      "Iteration 372/1000, Loss: 0.21054428815841675\n",
      "Iteration 373/1000, Loss: 0.21032290160655975\n",
      "Iteration 374/1000, Loss: 0.21010205149650574\n",
      "Iteration 375/1000, Loss: 0.20988187193870544\n",
      "Iteration 376/1000, Loss: 0.20966234803199768\n",
      "Iteration 377/1000, Loss: 0.20944339036941528\n",
      "Iteration 378/1000, Loss: 0.2092251032590866\n",
      "Iteration 379/1000, Loss: 0.20900748670101166\n",
      "Iteration 380/1000, Loss: 0.20879052579402924\n",
      "Iteration 381/1000, Loss: 0.20857419073581696\n",
      "Iteration 382/1000, Loss: 0.2083585262298584\n",
      "Iteration 383/1000, Loss: 0.2081434428691864\n",
      "Iteration 384/1000, Loss: 0.20792894065380096\n",
      "Iteration 385/1000, Loss: 0.20771503448486328\n",
      "Iteration 386/1000, Loss: 0.20750170946121216\n",
      "Iteration 387/1000, Loss: 0.20728906989097595\n",
      "Iteration 388/1000, Loss: 0.20707695186138153\n",
      "Iteration 389/1000, Loss: 0.2068653702735901\n",
      "Iteration 390/1000, Loss: 0.20665429532527924\n",
      "Iteration 391/1000, Loss: 0.20644362270832062\n",
      "Iteration 392/1000, Loss: 0.20623350143432617\n",
      "Iteration 393/1000, Loss: 0.2060239166021347\n",
      "Iteration 394/1000, Loss: 0.20581483840942383\n",
      "Iteration 395/1000, Loss: 0.20560647547245026\n",
      "Iteration 396/1000, Loss: 0.20539884269237518\n",
      "Iteration 397/1000, Loss: 0.20519183576107025\n",
      "Iteration 398/1000, Loss: 0.20498532056808472\n",
      "Iteration 399/1000, Loss: 0.204779252409935\n",
      "Iteration 400/1000, Loss: 0.20457369089126587\n",
      "Iteration 401/1000, Loss: 0.20436875522136688\n",
      "Iteration 402/1000, Loss: 0.2041642814874649\n",
      "Iteration 403/1000, Loss: 0.2039603441953659\n",
      "Iteration 404/1000, Loss: 0.2037569284439087\n",
      "Iteration 405/1000, Loss: 0.20355404913425446\n",
      "Iteration 406/1000, Loss: 0.20335179567337036\n",
      "Iteration 407/1000, Loss: 0.2031501680612564\n",
      "Iteration 408/1000, Loss: 0.20294909179210663\n",
      "Iteration 409/1000, Loss: 0.20274870097637177\n",
      "Iteration 410/1000, Loss: 0.2025488317012787\n",
      "Iteration 411/1000, Loss: 0.2023494988679886\n",
      "Iteration 412/1000, Loss: 0.20215077698230743\n",
      "Iteration 413/1000, Loss: 0.20195269584655762\n",
      "Iteration 414/1000, Loss: 0.20175513625144958\n",
      "Iteration 415/1000, Loss: 0.20155824720859528\n",
      "Iteration 416/1000, Loss: 0.20136187970638275\n",
      "Iteration 417/1000, Loss: 0.20116600394248962\n",
      "Iteration 418/1000, Loss: 0.2009706050157547\n",
      "Iteration 419/1000, Loss: 0.20077581703662872\n",
      "Iteration 420/1000, Loss: 0.20058152079582214\n",
      "Iteration 421/1000, Loss: 0.20038767158985138\n",
      "Iteration 422/1000, Loss: 0.20019428431987762\n",
      "Iteration 423/1000, Loss: 0.20000137388706207\n",
      "Iteration 424/1000, Loss: 0.19980895519256592\n",
      "Iteration 425/1000, Loss: 0.19961704313755035\n",
      "Iteration 426/1000, Loss: 0.19942565262317657\n",
      "Iteration 427/1000, Loss: 0.199234738945961\n",
      "Iteration 428/1000, Loss: 0.19904424250125885\n",
      "Iteration 429/1000, Loss: 0.19885431230068207\n",
      "Iteration 430/1000, Loss: 0.19866488873958588\n",
      "Iteration 431/1000, Loss: 0.1984759420156479\n",
      "Iteration 432/1000, Loss: 0.1982874870300293\n",
      "Iteration 433/1000, Loss: 0.1980995088815689\n",
      "Iteration 434/1000, Loss: 0.19791200757026672\n",
      "Iteration 435/1000, Loss: 0.19772498309612274\n",
      "Iteration 436/1000, Loss: 0.1975383162498474\n",
      "Iteration 437/1000, Loss: 0.19735223054885864\n",
      "Iteration 438/1000, Loss: 0.19716663658618927\n",
      "Iteration 439/1000, Loss: 0.19698144495487213\n",
      "Iteration 440/1000, Loss: 0.196796715259552\n",
      "Iteration 441/1000, Loss: 0.19661247730255127\n",
      "Iteration 442/1000, Loss: 0.19642873108386993\n",
      "Iteration 443/1000, Loss: 0.19624535739421844\n",
      "Iteration 444/1000, Loss: 0.19606241583824158\n",
      "Iteration 445/1000, Loss: 0.19587984681129456\n",
      "Iteration 446/1000, Loss: 0.19569775462150574\n",
      "Iteration 447/1000, Loss: 0.19551606476306915\n",
      "Iteration 448/1000, Loss: 0.19533489644527435\n",
      "Iteration 449/1000, Loss: 0.19515419006347656\n",
      "Iteration 450/1000, Loss: 0.1949739307165146\n",
      "Iteration 451/1000, Loss: 0.19479404389858246\n",
      "Iteration 452/1000, Loss: 0.19461464881896973\n",
      "Iteration 453/1000, Loss: 0.19443567097187042\n",
      "Iteration 454/1000, Loss: 0.19425715506076813\n",
      "Iteration 455/1000, Loss: 0.19407910108566284\n",
      "Iteration 456/1000, Loss: 0.1939014494419098\n",
      "Iteration 457/1000, Loss: 0.19372421503067017\n",
      "Iteration 458/1000, Loss: 0.19354739785194397\n",
      "Iteration 459/1000, Loss: 0.1933709979057312\n",
      "Iteration 460/1000, Loss: 0.19319498538970947\n",
      "Iteration 461/1000, Loss: 0.19301947951316833\n",
      "Iteration 462/1000, Loss: 0.19284440577030182\n",
      "Iteration 463/1000, Loss: 0.1926698535680771\n",
      "Iteration 464/1000, Loss: 0.1924957036972046\n",
      "Iteration 465/1000, Loss: 0.19232189655303955\n",
      "Iteration 466/1000, Loss: 0.19214852154254913\n",
      "Iteration 467/1000, Loss: 0.19197551906108856\n",
      "Iteration 468/1000, Loss: 0.1918029487133026\n",
      "Iteration 469/1000, Loss: 0.1916307657957077\n",
      "Iteration 470/1000, Loss: 0.19145898520946503\n",
      "Iteration 471/1000, Loss: 0.19128753244876862\n",
      "Iteration 472/1000, Loss: 0.19111649692058563\n",
      "Iteration 473/1000, Loss: 0.19094587862491608\n",
      "Iteration 474/1000, Loss: 0.19077558815479279\n",
      "Iteration 475/1000, Loss: 0.19060568511486053\n",
      "Iteration 476/1000, Loss: 0.19043618440628052\n",
      "Iteration 477/1000, Loss: 0.19026708602905273\n",
      "Iteration 478/1000, Loss: 0.190098375082016\n",
      "Iteration 479/1000, Loss: 0.18992994725704193\n",
      "Iteration 480/1000, Loss: 0.1897619515657425\n",
      "Iteration 481/1000, Loss: 0.18959440290927887\n",
      "Iteration 482/1000, Loss: 0.1894271820783615\n",
      "Iteration 483/1000, Loss: 0.189260333776474\n",
      "Iteration 484/1000, Loss: 0.18909387290477753\n",
      "Iteration 485/1000, Loss: 0.1889277696609497\n",
      "Iteration 486/1000, Loss: 0.18876203894615173\n",
      "Iteration 487/1000, Loss: 0.18859665095806122\n",
      "Iteration 488/1000, Loss: 0.18843165040016174\n",
      "Iteration 489/1000, Loss: 0.18826697766780853\n",
      "Iteration 490/1000, Loss: 0.1881026178598404\n",
      "Iteration 491/1000, Loss: 0.1879386007785797\n",
      "Iteration 492/1000, Loss: 0.18777501583099365\n",
      "Iteration 493/1000, Loss: 0.18761183321475983\n",
      "Iteration 494/1000, Loss: 0.18744905292987823\n",
      "Iteration 495/1000, Loss: 0.1872866302728653\n",
      "Iteration 496/1000, Loss: 0.1871246099472046\n",
      "Iteration 497/1000, Loss: 0.1869630217552185\n",
      "Iteration 498/1000, Loss: 0.1868017613887787\n",
      "Iteration 499/1000, Loss: 0.18664085865020752\n",
      "Iteration 500/1000, Loss: 0.1864803433418274\n",
      "Iteration 501/1000, Loss: 0.18632018566131592\n",
      "Iteration 502/1000, Loss: 0.18616041541099548\n",
      "Iteration 503/1000, Loss: 0.1860009729862213\n",
      "Iteration 504/1000, Loss: 0.18584191799163818\n",
      "Iteration 505/1000, Loss: 0.1856832355260849\n",
      "Iteration 506/1000, Loss: 0.1855248659849167\n",
      "Iteration 507/1000, Loss: 0.18536686897277832\n",
      "Iteration 508/1000, Loss: 0.18520916998386383\n",
      "Iteration 509/1000, Loss: 0.1850517839193344\n",
      "Iteration 510/1000, Loss: 0.18489475548267365\n",
      "Iteration 511/1000, Loss: 0.18473811447620392\n",
      "Iteration 512/1000, Loss: 0.18458183109760284\n",
      "Iteration 513/1000, Loss: 0.1844259351491928\n",
      "Iteration 514/1000, Loss: 0.18427041172981262\n",
      "Iteration 515/1000, Loss: 0.18411526083946228\n",
      "Iteration 516/1000, Loss: 0.1839604526758194\n",
      "Iteration 517/1000, Loss: 0.1838059425354004\n",
      "Iteration 518/1000, Loss: 0.18365174531936646\n",
      "Iteration 519/1000, Loss: 0.18349792063236237\n",
      "Iteration 520/1000, Loss: 0.18334443867206573\n",
      "Iteration 521/1000, Loss: 0.18319135904312134\n",
      "Iteration 522/1000, Loss: 0.18303857743740082\n",
      "Iteration 523/1000, Loss: 0.18288609385490417\n",
      "Iteration 524/1000, Loss: 0.1827339380979538\n",
      "Iteration 525/1000, Loss: 0.1825820952653885\n",
      "Iteration 526/1000, Loss: 0.18243058025836945\n",
      "Iteration 527/1000, Loss: 0.18227939307689667\n",
      "Iteration 528/1000, Loss: 0.1821284294128418\n",
      "Iteration 529/1000, Loss: 0.1819777488708496\n",
      "Iteration 530/1000, Loss: 0.18182741105556488\n",
      "Iteration 531/1000, Loss: 0.1816774159669876\n",
      "Iteration 532/1000, Loss: 0.1815277636051178\n",
      "Iteration 533/1000, Loss: 0.18137848377227783\n",
      "Iteration 534/1000, Loss: 0.18122953176498413\n",
      "Iteration 535/1000, Loss: 0.1810808777809143\n",
      "Iteration 536/1000, Loss: 0.18093249201774597\n",
      "Iteration 537/1000, Loss: 0.1807844191789627\n",
      "Iteration 538/1000, Loss: 0.18063661456108093\n",
      "Iteration 539/1000, Loss: 0.1804891675710678\n",
      "Iteration 540/1000, Loss: 0.18034209311008453\n",
      "Iteration 541/1000, Loss: 0.18019533157348633\n",
      "Iteration 542/1000, Loss: 0.18004882335662842\n",
      "Iteration 543/1000, Loss: 0.17990261316299438\n",
      "Iteration 544/1000, Loss: 0.1797567456960678\n",
      "Iteration 545/1000, Loss: 0.17961113154888153\n",
      "Iteration 546/1000, Loss: 0.17946578562259674\n",
      "Iteration 547/1000, Loss: 0.17932073771953583\n",
      "Iteration 548/1000, Loss: 0.1791759431362152\n",
      "Iteration 549/1000, Loss: 0.17903144657611847\n",
      "Iteration 550/1000, Loss: 0.17888721823692322\n",
      "Iteration 551/1000, Loss: 0.17874327301979065\n",
      "Iteration 552/1000, Loss: 0.17859956622123718\n",
      "Iteration 553/1000, Loss: 0.1784561425447464\n",
      "Iteration 554/1000, Loss: 0.1783130168914795\n",
      "Iteration 555/1000, Loss: 0.17817018926143646\n",
      "Iteration 556/1000, Loss: 0.1780276745557785\n",
      "Iteration 557/1000, Loss: 0.17788538336753845\n",
      "Iteration 558/1000, Loss: 0.17774342000484467\n",
      "Iteration 559/1000, Loss: 0.17760178446769714\n",
      "Iteration 560/1000, Loss: 0.1774604618549347\n",
      "Iteration 561/1000, Loss: 0.17731943726539612\n",
      "Iteration 562/1000, Loss: 0.17717860639095306\n",
      "Iteration 563/1000, Loss: 0.17703813314437866\n",
      "Iteration 564/1000, Loss: 0.17689791321754456\n",
      "Iteration 565/1000, Loss: 0.17675794661045074\n",
      "Iteration 566/1000, Loss: 0.17661824822425842\n",
      "Iteration 567/1000, Loss: 0.176478773355484\n",
      "Iteration 568/1000, Loss: 0.17633962631225586\n",
      "Iteration 569/1000, Loss: 0.176200732588768\n",
      "Iteration 570/1000, Loss: 0.17606204748153687\n",
      "Iteration 571/1000, Loss: 0.17592363059520721\n",
      "Iteration 572/1000, Loss: 0.17578545212745667\n",
      "Iteration 573/1000, Loss: 0.1756475418806076\n",
      "Iteration 574/1000, Loss: 0.17550991475582123\n",
      "Iteration 575/1000, Loss: 0.17537258565425873\n",
      "Iteration 576/1000, Loss: 0.17523550987243652\n",
      "Iteration 577/1000, Loss: 0.17509867250919342\n",
      "Iteration 578/1000, Loss: 0.17496204376220703\n",
      "Iteration 579/1000, Loss: 0.17482566833496094\n",
      "Iteration 580/1000, Loss: 0.17468957602977753\n",
      "Iteration 581/1000, Loss: 0.17455372214317322\n",
      "Iteration 582/1000, Loss: 0.17441803216934204\n",
      "Iteration 583/1000, Loss: 0.17428261041641235\n",
      "Iteration 584/1000, Loss: 0.17414742708206177\n",
      "Iteration 585/1000, Loss: 0.17401252686977386\n",
      "Iteration 586/1000, Loss: 0.17387792468070984\n",
      "Iteration 587/1000, Loss: 0.1737436205148697\n",
      "Iteration 588/1000, Loss: 0.17360951006412506\n",
      "Iteration 589/1000, Loss: 0.17347566783428192\n",
      "Iteration 590/1000, Loss: 0.17334212362766266\n",
      "Iteration 591/1000, Loss: 0.1732088029384613\n",
      "Iteration 592/1000, Loss: 0.17307575047016144\n",
      "Iteration 593/1000, Loss: 0.17294296622276306\n",
      "Iteration 594/1000, Loss: 0.17281045019626617\n",
      "Iteration 595/1000, Loss: 0.17267821729183197\n",
      "Iteration 596/1000, Loss: 0.17254619300365448\n",
      "Iteration 597/1000, Loss: 0.1724143922328949\n",
      "Iteration 598/1000, Loss: 0.1722828447818756\n",
      "Iteration 599/1000, Loss: 0.17215153574943542\n",
      "Iteration 600/1000, Loss: 0.17202052474021912\n",
      "Iteration 601/1000, Loss: 0.1718897521495819\n",
      "Iteration 602/1000, Loss: 0.171759232878685\n",
      "Iteration 603/1000, Loss: 0.1716288924217224\n",
      "Iteration 604/1000, Loss: 0.17149877548217773\n",
      "Iteration 605/1000, Loss: 0.17136886715888977\n",
      "Iteration 606/1000, Loss: 0.1712391972541809\n",
      "Iteration 607/1000, Loss: 0.17110972106456757\n",
      "Iteration 608/1000, Loss: 0.17098043859004974\n",
      "Iteration 609/1000, Loss: 0.17085139453411102\n",
      "Iteration 610/1000, Loss: 0.17072263360023499\n",
      "Iteration 611/1000, Loss: 0.17059403657913208\n",
      "Iteration 612/1000, Loss: 0.17046569287776947\n",
      "Iteration 613/1000, Loss: 0.17033755779266357\n",
      "Iteration 614/1000, Loss: 0.1702096313238144\n",
      "Iteration 615/1000, Loss: 0.17008192837238312\n",
      "Iteration 616/1000, Loss: 0.16995446383953094\n",
      "Iteration 617/1000, Loss: 0.16982720792293549\n",
      "Iteration 618/1000, Loss: 0.16970017552375793\n",
      "Iteration 619/1000, Loss: 0.1695733666419983\n",
      "Iteration 620/1000, Loss: 0.16944675147533417\n",
      "Iteration 621/1000, Loss: 0.16932040452957153\n",
      "Iteration 622/1000, Loss: 0.169194296002388\n",
      "Iteration 623/1000, Loss: 0.16906844079494476\n",
      "Iteration 624/1000, Loss: 0.16894283890724182\n",
      "Iteration 625/1000, Loss: 0.1688174456357956\n",
      "Iteration 626/1000, Loss: 0.16869229078292847\n",
      "Iteration 627/1000, Loss: 0.16856740415096283\n",
      "Iteration 628/1000, Loss: 0.1684427261352539\n",
      "Iteration 629/1000, Loss: 0.1683182567358017\n",
      "Iteration 630/1000, Loss: 0.1681940257549286\n",
      "Iteration 631/1000, Loss: 0.168069988489151\n",
      "Iteration 632/1000, Loss: 0.16794613003730774\n",
      "Iteration 633/1000, Loss: 0.16782249510288239\n",
      "Iteration 634/1000, Loss: 0.16769908368587494\n",
      "Iteration 635/1000, Loss: 0.1675758808851242\n",
      "Iteration 636/1000, Loss: 0.167452871799469\n",
      "Iteration 637/1000, Loss: 0.16733011603355408\n",
      "Iteration 638/1000, Loss: 0.16720758378505707\n",
      "Iteration 639/1000, Loss: 0.16708526015281677\n",
      "Iteration 640/1000, Loss: 0.1669631451368332\n",
      "Iteration 641/1000, Loss: 0.1668412834405899\n",
      "Iteration 642/1000, Loss: 0.16671961545944214\n",
      "Iteration 643/1000, Loss: 0.1665981262922287\n",
      "Iteration 644/1000, Loss: 0.1664767861366272\n",
      "Iteration 645/1000, Loss: 0.1663556545972824\n",
      "Iteration 646/1000, Loss: 0.16623473167419434\n",
      "Iteration 647/1000, Loss: 0.16611400246620178\n",
      "Iteration 648/1000, Loss: 0.16599349677562714\n",
      "Iteration 649/1000, Loss: 0.16587314009666443\n",
      "Iteration 650/1000, Loss: 0.16575296223163605\n",
      "Iteration 651/1000, Loss: 0.16563297808170319\n",
      "Iteration 652/1000, Loss: 0.16551315784454346\n",
      "Iteration 653/1000, Loss: 0.16539351642131805\n",
      "Iteration 654/1000, Loss: 0.16527412831783295\n",
      "Iteration 655/1000, Loss: 0.16515491902828217\n",
      "Iteration 656/1000, Loss: 0.16503594815731049\n",
      "Iteration 657/1000, Loss: 0.16491717100143433\n",
      "Iteration 658/1000, Loss: 0.16479863226413727\n",
      "Iteration 659/1000, Loss: 0.16468021273612976\n",
      "Iteration 660/1000, Loss: 0.16456197202205658\n",
      "Iteration 661/1000, Loss: 0.1644439846277237\n",
      "Iteration 662/1000, Loss: 0.16432610154151917\n",
      "Iteration 663/1000, Loss: 0.16420841217041016\n",
      "Iteration 664/1000, Loss: 0.16409093141555786\n",
      "Iteration 665/1000, Loss: 0.16397368907928467\n",
      "Iteration 666/1000, Loss: 0.1638566553592682\n",
      "Iteration 667/1000, Loss: 0.16373983025550842\n",
      "Iteration 668/1000, Loss: 0.16362321376800537\n",
      "Iteration 669/1000, Loss: 0.16350677609443665\n",
      "Iteration 670/1000, Loss: 0.16339053213596344\n",
      "Iteration 671/1000, Loss: 0.16327449679374695\n",
      "Iteration 672/1000, Loss: 0.16315868496894836\n",
      "Iteration 673/1000, Loss: 0.1630430668592453\n",
      "Iteration 674/1000, Loss: 0.16292767226696014\n",
      "Iteration 675/1000, Loss: 0.1628124713897705\n",
      "Iteration 676/1000, Loss: 0.162697434425354\n",
      "Iteration 677/1000, Loss: 0.1625826060771942\n",
      "Iteration 678/1000, Loss: 0.16246794164180756\n",
      "Iteration 679/1000, Loss: 0.1623534858226776\n",
      "Iteration 680/1000, Loss: 0.162239208817482\n",
      "Iteration 681/1000, Loss: 0.1621251404285431\n",
      "Iteration 682/1000, Loss: 0.1620112955570221\n",
      "Iteration 683/1000, Loss: 0.16189762949943542\n",
      "Iteration 684/1000, Loss: 0.16178414225578308\n",
      "Iteration 685/1000, Loss: 0.16167086362838745\n",
      "Iteration 686/1000, Loss: 0.16155773401260376\n",
      "Iteration 687/1000, Loss: 0.16144481301307678\n",
      "Iteration 688/1000, Loss: 0.16133207082748413\n",
      "Iteration 689/1000, Loss: 0.1612194925546646\n",
      "Iteration 690/1000, Loss: 0.1611071676015854\n",
      "Iteration 691/1000, Loss: 0.1609949916601181\n",
      "Iteration 692/1000, Loss: 0.16088300943374634\n",
      "Iteration 693/1000, Loss: 0.16077116131782532\n",
      "Iteration 694/1000, Loss: 0.160659521818161\n",
      "Iteration 695/1000, Loss: 0.16054804623126984\n",
      "Iteration 696/1000, Loss: 0.1604367345571518\n",
      "Iteration 697/1000, Loss: 0.16032561659812927\n",
      "Iteration 698/1000, Loss: 0.16021467745304108\n",
      "Iteration 699/1000, Loss: 0.1601039320230484\n",
      "Iteration 700/1000, Loss: 0.15999338030815125\n",
      "Iteration 701/1000, Loss: 0.15988300740718842\n",
      "Iteration 702/1000, Loss: 0.1597728133201599\n",
      "Iteration 703/1000, Loss: 0.15966278314590454\n",
      "Iteration 704/1000, Loss: 0.1595529317855835\n",
      "Iteration 705/1000, Loss: 0.15944328904151917\n",
      "Iteration 706/1000, Loss: 0.15933382511138916\n",
      "Iteration 707/1000, Loss: 0.15922456979751587\n",
      "Iteration 708/1000, Loss: 0.1591155081987381\n",
      "Iteration 709/1000, Loss: 0.15900664031505585\n",
      "Iteration 710/1000, Loss: 0.15889793634414673\n",
      "Iteration 711/1000, Loss: 0.15878938138484955\n",
      "Iteration 712/1000, Loss: 0.1586809903383255\n",
      "Iteration 713/1000, Loss: 0.15857279300689697\n",
      "Iteration 714/1000, Loss: 0.15846480429172516\n",
      "Iteration 715/1000, Loss: 0.15835697948932648\n",
      "Iteration 716/1000, Loss: 0.15824934840202332\n",
      "Iteration 717/1000, Loss: 0.1581418663263321\n",
      "Iteration 718/1000, Loss: 0.1580345332622528\n",
      "Iteration 719/1000, Loss: 0.15792740881443024\n",
      "Iteration 720/1000, Loss: 0.1578204333782196\n",
      "Iteration 721/1000, Loss: 0.15771357715129852\n",
      "Iteration 722/1000, Loss: 0.157606840133667\n",
      "Iteration 723/1000, Loss: 0.15750029683113098\n",
      "Iteration 724/1000, Loss: 0.1573939025402069\n",
      "Iteration 725/1000, Loss: 0.1572876274585724\n",
      "Iteration 726/1000, Loss: 0.157181516289711\n",
      "Iteration 727/1000, Loss: 0.15707549452781677\n",
      "Iteration 728/1000, Loss: 0.15696965157985687\n",
      "Iteration 729/1000, Loss: 0.15686389803886414\n",
      "Iteration 730/1000, Loss: 0.1567583531141281\n",
      "Iteration 731/1000, Loss: 0.15665292739868164\n",
      "Iteration 732/1000, Loss: 0.1565476357936859\n",
      "Iteration 733/1000, Loss: 0.15644249320030212\n",
      "Iteration 734/1000, Loss: 0.15633751451969147\n",
      "Iteration 735/1000, Loss: 0.15623271465301514\n",
      "Iteration 736/1000, Loss: 0.15612809360027313\n",
      "Iteration 737/1000, Loss: 0.15602357685565948\n",
      "Iteration 738/1000, Loss: 0.15591923892498016\n",
      "Iteration 739/1000, Loss: 0.1558150351047516\n",
      "Iteration 740/1000, Loss: 0.15571103990077972\n",
      "Iteration 741/1000, Loss: 0.1556071937084198\n",
      "Iteration 742/1000, Loss: 0.15550346672534943\n",
      "Iteration 743/1000, Loss: 0.15539990365505219\n",
      "Iteration 744/1000, Loss: 0.15529648959636688\n",
      "Iteration 745/1000, Loss: 0.15519320964813232\n",
      "Iteration 746/1000, Loss: 0.15509013831615448\n",
      "Iteration 747/1000, Loss: 0.1549871563911438\n",
      "Iteration 748/1000, Loss: 0.15488436818122864\n",
      "Iteration 749/1000, Loss: 0.1547817438840866\n",
      "Iteration 750/1000, Loss: 0.15467926859855652\n",
      "Iteration 751/1000, Loss: 0.1545768827199936\n",
      "Iteration 752/1000, Loss: 0.15447470545768738\n",
      "Iteration 753/1000, Loss: 0.1543726623058319\n",
      "Iteration 754/1000, Loss: 0.15427076816558838\n",
      "Iteration 755/1000, Loss: 0.1541690081357956\n",
      "Iteration 756/1000, Loss: 0.15406741201877594\n",
      "Iteration 757/1000, Loss: 0.15396595001220703\n",
      "Iteration 758/1000, Loss: 0.15386462211608887\n",
      "Iteration 759/1000, Loss: 0.15376344323158264\n",
      "Iteration 760/1000, Loss: 0.15366241335868835\n",
      "Iteration 761/1000, Loss: 0.153561532497406\n",
      "Iteration 762/1000, Loss: 0.1534608006477356\n",
      "Iteration 763/1000, Loss: 0.15336021780967712\n",
      "Iteration 764/1000, Loss: 0.1532597690820694\n",
      "Iteration 765/1000, Loss: 0.1531594693660736\n",
      "Iteration 766/1000, Loss: 0.15305930376052856\n",
      "Iteration 767/1000, Loss: 0.15295934677124023\n",
      "Iteration 768/1000, Loss: 0.15285953879356384\n",
      "Iteration 769/1000, Loss: 0.1527598649263382\n",
      "Iteration 770/1000, Loss: 0.15266035497188568\n",
      "Iteration 771/1000, Loss: 0.15256094932556152\n",
      "Iteration 772/1000, Loss: 0.15246173739433289\n",
      "Iteration 773/1000, Loss: 0.15236268937587738\n",
      "Iteration 774/1000, Loss: 0.1522638350725174\n",
      "Iteration 775/1000, Loss: 0.15216509997844696\n",
      "Iteration 776/1000, Loss: 0.15206648409366608\n",
      "Iteration 777/1000, Loss: 0.15196800231933594\n",
      "Iteration 778/1000, Loss: 0.15186965465545654\n",
      "Iteration 779/1000, Loss: 0.15177150070667267\n",
      "Iteration 780/1000, Loss: 0.15167348086833954\n",
      "Iteration 781/1000, Loss: 0.15157563984394073\n",
      "Iteration 782/1000, Loss: 0.15147793292999268\n",
      "Iteration 783/1000, Loss: 0.15138036012649536\n",
      "Iteration 784/1000, Loss: 0.1512829214334488\n",
      "Iteration 785/1000, Loss: 0.15118564665317535\n",
      "Iteration 786/1000, Loss: 0.15108850598335266\n",
      "Iteration 787/1000, Loss: 0.1509914994239807\n",
      "Iteration 788/1000, Loss: 0.1508946567773819\n",
      "Iteration 789/1000, Loss: 0.15079794824123383\n",
      "Iteration 790/1000, Loss: 0.1507013887166977\n",
      "Iteration 791/1000, Loss: 0.1506049782037735\n",
      "Iteration 792/1000, Loss: 0.15050868690013885\n",
      "Iteration 793/1000, Loss: 0.15041254460811615\n",
      "Iteration 794/1000, Loss: 0.15031655132770538\n",
      "Iteration 795/1000, Loss: 0.15022070705890656\n",
      "Iteration 796/1000, Loss: 0.15012499690055847\n",
      "Iteration 797/1000, Loss: 0.15002945065498352\n",
      "Iteration 798/1000, Loss: 0.14993402361869812\n",
      "Iteration 799/1000, Loss: 0.14983880519866943\n",
      "Iteration 800/1000, Loss: 0.1497437059879303\n",
      "Iteration 801/1000, Loss: 0.1496487259864807\n",
      "Iteration 802/1000, Loss: 0.14955385029315948\n",
      "Iteration 803/1000, Loss: 0.1494591236114502\n",
      "Iteration 804/1000, Loss: 0.14936451613903046\n",
      "Iteration 805/1000, Loss: 0.14927001297473907\n",
      "Iteration 806/1000, Loss: 0.14917568862438202\n",
      "Iteration 807/1000, Loss: 0.14908148348331451\n",
      "Iteration 808/1000, Loss: 0.14898741245269775\n",
      "Iteration 809/1000, Loss: 0.14889346063137054\n",
      "Iteration 810/1000, Loss: 0.1487996131181717\n",
      "Iteration 811/1000, Loss: 0.14870585501194\n",
      "Iteration 812/1000, Loss: 0.14861224591732025\n",
      "Iteration 813/1000, Loss: 0.14851875603199005\n",
      "Iteration 814/1000, Loss: 0.14842543005943298\n",
      "Iteration 815/1000, Loss: 0.14833219349384308\n",
      "Iteration 816/1000, Loss: 0.14823907613754272\n",
      "Iteration 817/1000, Loss: 0.1481461226940155\n",
      "Iteration 818/1000, Loss: 0.14805325865745544\n",
      "Iteration 819/1000, Loss: 0.14796052873134613\n",
      "Iteration 820/1000, Loss: 0.14786791801452637\n",
      "Iteration 821/1000, Loss: 0.14777545630931854\n",
      "Iteration 822/1000, Loss: 0.14768312871456146\n",
      "Iteration 823/1000, Loss: 0.14759092032909393\n",
      "Iteration 824/1000, Loss: 0.14749880135059357\n",
      "Iteration 825/1000, Loss: 0.14740680158138275\n",
      "Iteration 826/1000, Loss: 0.1473149061203003\n",
      "Iteration 827/1000, Loss: 0.14722314476966858\n",
      "Iteration 828/1000, Loss: 0.14713147282600403\n",
      "Iteration 829/1000, Loss: 0.14703990519046783\n",
      "Iteration 830/1000, Loss: 0.14694851636886597\n",
      "Iteration 831/1000, Loss: 0.14685726165771484\n",
      "Iteration 832/1000, Loss: 0.14676611125469208\n",
      "Iteration 833/1000, Loss: 0.14667510986328125\n",
      "Iteration 834/1000, Loss: 0.14658427238464355\n",
      "Iteration 835/1000, Loss: 0.14649353921413422\n",
      "Iteration 836/1000, Loss: 0.14640295505523682\n",
      "Iteration 837/1000, Loss: 0.14631250500679016\n",
      "Iteration 838/1000, Loss: 0.14622218906879425\n",
      "Iteration 839/1000, Loss: 0.1461319774389267\n",
      "Iteration 840/1000, Loss: 0.1460418999195099\n",
      "Iteration 841/1000, Loss: 0.14595195651054382\n",
      "Iteration 842/1000, Loss: 0.1458621770143509\n",
      "Iteration 843/1000, Loss: 0.1457725167274475\n",
      "Iteration 844/1000, Loss: 0.1456829458475113\n",
      "Iteration 845/1000, Loss: 0.145593523979187\n",
      "Iteration 846/1000, Loss: 0.1455042064189911\n",
      "Iteration 847/1000, Loss: 0.1454150378704071\n",
      "Iteration 848/1000, Loss: 0.1453259438276291\n",
      "Iteration 849/1000, Loss: 0.14523693919181824\n",
      "Iteration 850/1000, Loss: 0.14514809846878052\n",
      "Iteration 851/1000, Loss: 0.14505937695503235\n",
      "Iteration 852/1000, Loss: 0.14497077465057373\n",
      "Iteration 853/1000, Loss: 0.14488227665424347\n",
      "Iteration 854/1000, Loss: 0.14479391276836395\n",
      "Iteration 855/1000, Loss: 0.1447056531906128\n",
      "Iteration 856/1000, Loss: 0.14461749792099\n",
      "Iteration 857/1000, Loss: 0.14452944695949554\n",
      "Iteration 858/1000, Loss: 0.14444150030612946\n",
      "Iteration 859/1000, Loss: 0.14435367286205292\n",
      "Iteration 860/1000, Loss: 0.14426597952842712\n",
      "Iteration 861/1000, Loss: 0.1441783905029297\n",
      "Iteration 862/1000, Loss: 0.1440909504890442\n",
      "Iteration 863/1000, Loss: 0.14400358498096466\n",
      "Iteration 864/1000, Loss: 0.14391633868217468\n",
      "Iteration 865/1000, Loss: 0.14382916688919067\n",
      "Iteration 866/1000, Loss: 0.1437421292066574\n",
      "Iteration 867/1000, Loss: 0.14365513622760773\n",
      "Iteration 868/1000, Loss: 0.1435682773590088\n",
      "Iteration 869/1000, Loss: 0.1434815227985382\n",
      "Iteration 870/1000, Loss: 0.14339491724967957\n",
      "Iteration 871/1000, Loss: 0.14330841600894928\n",
      "Iteration 872/1000, Loss: 0.14322203397750854\n",
      "Iteration 873/1000, Loss: 0.14313574135303497\n",
      "Iteration 874/1000, Loss: 0.14304953813552856\n",
      "Iteration 875/1000, Loss: 0.1429634392261505\n",
      "Iteration 876/1000, Loss: 0.1428774744272232\n",
      "Iteration 877/1000, Loss: 0.14279162883758545\n",
      "Iteration 878/1000, Loss: 0.14270590245723724\n",
      "Iteration 879/1000, Loss: 0.142620250582695\n",
      "Iteration 880/1000, Loss: 0.14253470301628113\n",
      "Iteration 881/1000, Loss: 0.1424492448568344\n",
      "Iteration 882/1000, Loss: 0.14236390590667725\n",
      "Iteration 883/1000, Loss: 0.14227864146232605\n",
      "Iteration 884/1000, Loss: 0.14219355583190918\n",
      "Iteration 885/1000, Loss: 0.14210857450962067\n",
      "Iteration 886/1000, Loss: 0.1420236974954605\n",
      "Iteration 887/1000, Loss: 0.1419389396905899\n",
      "Iteration 888/1000, Loss: 0.14185430109500885\n",
      "Iteration 889/1000, Loss: 0.14176979660987854\n",
      "Iteration 890/1000, Loss: 0.1416853815317154\n",
      "Iteration 891/1000, Loss: 0.1416010707616806\n",
      "Iteration 892/1000, Loss: 0.14151689410209656\n",
      "Iteration 893/1000, Loss: 0.14143285155296326\n",
      "Iteration 894/1000, Loss: 0.14134888350963593\n",
      "Iteration 895/1000, Loss: 0.14126504957675934\n",
      "Iteration 896/1000, Loss: 0.1411813497543335\n",
      "Iteration 897/1000, Loss: 0.14109770953655243\n",
      "Iteration 898/1000, Loss: 0.1410142183303833\n",
      "Iteration 899/1000, Loss: 0.14093086123466492\n",
      "Iteration 900/1000, Loss: 0.1408476084470749\n",
      "Iteration 901/1000, Loss: 0.14076445996761322\n",
      "Iteration 902/1000, Loss: 0.14068137109279633\n",
      "Iteration 903/1000, Loss: 0.14059840142726898\n",
      "Iteration 904/1000, Loss: 0.14051556587219238\n",
      "Iteration 905/1000, Loss: 0.14043277502059937\n",
      "Iteration 906/1000, Loss: 0.1403501182794571\n",
      "Iteration 907/1000, Loss: 0.14026755094528198\n",
      "Iteration 908/1000, Loss: 0.14018508791923523\n",
      "Iteration 909/1000, Loss: 0.14010277390480042\n",
      "Iteration 910/1000, Loss: 0.14002051949501038\n",
      "Iteration 911/1000, Loss: 0.13993841409683228\n",
      "Iteration 912/1000, Loss: 0.13985638320446014\n",
      "Iteration 913/1000, Loss: 0.13977448642253876\n",
      "Iteration 914/1000, Loss: 0.13969267904758453\n",
      "Iteration 915/1000, Loss: 0.13961097598075867\n",
      "Iteration 916/1000, Loss: 0.13952936232089996\n",
      "Iteration 917/1000, Loss: 0.1394478678703308\n",
      "Iteration 918/1000, Loss: 0.1393664926290512\n",
      "Iteration 919/1000, Loss: 0.13928519189357758\n",
      "Iteration 920/1000, Loss: 0.1392040103673935\n",
      "Iteration 921/1000, Loss: 0.13912291824817657\n",
      "Iteration 922/1000, Loss: 0.1390419453382492\n",
      "Iteration 923/1000, Loss: 0.1389610320329666\n",
      "Iteration 924/1000, Loss: 0.13888020813465118\n",
      "Iteration 925/1000, Loss: 0.1387995183467865\n",
      "Iteration 926/1000, Loss: 0.13871891796588898\n",
      "Iteration 927/1000, Loss: 0.13863840699195862\n",
      "Iteration 928/1000, Loss: 0.1385580152273178\n",
      "Iteration 929/1000, Loss: 0.13847769796848297\n",
      "Iteration 930/1000, Loss: 0.1383974701166153\n",
      "Iteration 931/1000, Loss: 0.13831737637519836\n",
      "Iteration 932/1000, Loss: 0.13823740184307098\n",
      "Iteration 933/1000, Loss: 0.13815753161907196\n",
      "Iteration 934/1000, Loss: 0.1380777806043625\n",
      "Iteration 935/1000, Loss: 0.13799811899662018\n",
      "Iteration 936/1000, Loss: 0.13791850209236145\n",
      "Iteration 937/1000, Loss: 0.13783897459506989\n",
      "Iteration 938/1000, Loss: 0.13775958120822906\n",
      "Iteration 939/1000, Loss: 0.1376802772283554\n",
      "Iteration 940/1000, Loss: 0.13760100305080414\n",
      "Iteration 941/1000, Loss: 0.1375218629837036\n",
      "Iteration 942/1000, Loss: 0.13744281232357025\n",
      "Iteration 943/1000, Loss: 0.13736383616924286\n",
      "Iteration 944/1000, Loss: 0.1372849941253662\n",
      "Iteration 945/1000, Loss: 0.13720625638961792\n",
      "Iteration 946/1000, Loss: 0.1371275782585144\n",
      "Iteration 947/1000, Loss: 0.13704898953437805\n",
      "Iteration 948/1000, Loss: 0.13697050511837006\n",
      "Iteration 949/1000, Loss: 0.1368921548128128\n",
      "Iteration 950/1000, Loss: 0.13681389391422272\n",
      "Iteration 951/1000, Loss: 0.1367357075214386\n",
      "Iteration 952/1000, Loss: 0.13665764033794403\n",
      "Iteration 953/1000, Loss: 0.13657963275909424\n",
      "Iteration 954/1000, Loss: 0.1365017145872116\n",
      "Iteration 955/1000, Loss: 0.13642387092113495\n",
      "Iteration 956/1000, Loss: 0.13634614646434784\n",
      "Iteration 957/1000, Loss: 0.1362685263156891\n",
      "Iteration 958/1000, Loss: 0.1361909955739975\n",
      "Iteration 959/1000, Loss: 0.13611356914043427\n",
      "Iteration 960/1000, Loss: 0.1360362470149994\n",
      "Iteration 961/1000, Loss: 0.13595899939537048\n",
      "Iteration 962/1000, Loss: 0.13588185608386993\n",
      "Iteration 963/1000, Loss: 0.13580483198165894\n",
      "Iteration 964/1000, Loss: 0.1357279270887375\n",
      "Iteration 965/1000, Loss: 0.1356511265039444\n",
      "Iteration 966/1000, Loss: 0.13557441532611847\n",
      "Iteration 967/1000, Loss: 0.1354977935552597\n",
      "Iteration 968/1000, Loss: 0.13542123138904572\n",
      "Iteration 969/1000, Loss: 0.13534478843212128\n",
      "Iteration 970/1000, Loss: 0.1352684050798416\n",
      "Iteration 971/1000, Loss: 0.1351921111345291\n",
      "Iteration 972/1000, Loss: 0.13511589169502258\n",
      "Iteration 973/1000, Loss: 0.1350397914648056\n",
      "Iteration 974/1000, Loss: 0.1349637806415558\n",
      "Iteration 975/1000, Loss: 0.13488787412643433\n",
      "Iteration 976/1000, Loss: 0.13481207191944122\n",
      "Iteration 977/1000, Loss: 0.13473635911941528\n",
      "Iteration 978/1000, Loss: 0.1346607208251953\n",
      "Iteration 979/1000, Loss: 0.1345851570367813\n",
      "Iteration 980/1000, Loss: 0.13450971245765686\n",
      "Iteration 981/1000, Loss: 0.13443435728549957\n",
      "Iteration 982/1000, Loss: 0.13435912132263184\n",
      "Iteration 983/1000, Loss: 0.13428395986557007\n",
      "Iteration 984/1000, Loss: 0.13420888781547546\n",
      "Iteration 985/1000, Loss: 0.13413390517234802\n",
      "Iteration 986/1000, Loss: 0.13405901193618774\n",
      "Iteration 987/1000, Loss: 0.13398419320583344\n",
      "Iteration 988/1000, Loss: 0.13390947878360748\n",
      "Iteration 989/1000, Loss: 0.1338348090648651\n",
      "Iteration 990/1000, Loss: 0.1337602585554123\n",
      "Iteration 991/1000, Loss: 0.13368576765060425\n",
      "Iteration 992/1000, Loss: 0.13361138105392456\n",
      "Iteration 993/1000, Loss: 0.13353709876537323\n",
      "Iteration 994/1000, Loss: 0.13346287608146667\n",
      "Iteration 995/1000, Loss: 0.13338875770568848\n",
      "Iteration 996/1000, Loss: 0.13331469893455505\n",
      "Iteration 997/1000, Loss: 0.13324074447155\n",
      "Iteration 998/1000, Loss: 0.1331668645143509\n",
      "Iteration 999/1000, Loss: 0.13309302926063538\n",
      "Iteration 1000/1000, Loss: 0.13301925361156464\n",
      "Pruning Step 3\n",
      "Iteration 1/1000, Loss: 1.7074230909347534\n",
      "Iteration 2/1000, Loss: 1.5886821746826172\n",
      "Iteration 3/1000, Loss: 1.4925644397735596\n",
      "Iteration 4/1000, Loss: 1.4058705568313599\n",
      "Iteration 5/1000, Loss: 1.3266770839691162\n",
      "Iteration 6/1000, Loss: 1.2541307210922241\n",
      "Iteration 7/1000, Loss: 1.187625527381897\n",
      "Iteration 8/1000, Loss: 1.1266822814941406\n",
      "Iteration 9/1000, Loss: 1.0708681344985962\n",
      "Iteration 10/1000, Loss: 1.0198134183883667\n",
      "Iteration 11/1000, Loss: 0.9731631278991699\n",
      "Iteration 12/1000, Loss: 0.9305286407470703\n",
      "Iteration 13/1000, Loss: 0.8915615677833557\n",
      "Iteration 14/1000, Loss: 0.8559264540672302\n",
      "Iteration 15/1000, Loss: 0.8233172297477722\n",
      "Iteration 16/1000, Loss: 0.7934383749961853\n",
      "Iteration 17/1000, Loss: 0.7660182118415833\n",
      "Iteration 18/1000, Loss: 0.7408186197280884\n",
      "Iteration 19/1000, Loss: 0.7176265716552734\n",
      "Iteration 20/1000, Loss: 0.6962368488311768\n",
      "Iteration 21/1000, Loss: 0.676474392414093\n",
      "Iteration 22/1000, Loss: 0.658176064491272\n",
      "Iteration 23/1000, Loss: 0.6412015557289124\n",
      "Iteration 24/1000, Loss: 0.6254271864891052\n",
      "Iteration 25/1000, Loss: 0.6107404232025146\n",
      "Iteration 26/1000, Loss: 0.597036600112915\n",
      "Iteration 27/1000, Loss: 0.5842256546020508\n",
      "Iteration 28/1000, Loss: 0.572231650352478\n",
      "Iteration 29/1000, Loss: 0.560981273651123\n",
      "Iteration 30/1000, Loss: 0.5504072904586792\n",
      "Iteration 31/1000, Loss: 0.5404520034790039\n",
      "Iteration 32/1000, Loss: 0.5310660600662231\n",
      "Iteration 33/1000, Loss: 0.522202730178833\n",
      "Iteration 34/1000, Loss: 0.5138210654258728\n",
      "Iteration 35/1000, Loss: 0.5058827996253967\n",
      "Iteration 36/1000, Loss: 0.49835580587387085\n",
      "Iteration 37/1000, Loss: 0.491207480430603\n",
      "Iteration 38/1000, Loss: 0.4844094216823578\n",
      "Iteration 39/1000, Loss: 0.47793588042259216\n",
      "Iteration 40/1000, Loss: 0.47176384925842285\n",
      "Iteration 41/1000, Loss: 0.4658721387386322\n",
      "Iteration 42/1000, Loss: 0.46024131774902344\n",
      "Iteration 43/1000, Loss: 0.4548545777797699\n",
      "Iteration 44/1000, Loss: 0.44969692826271057\n",
      "Iteration 45/1000, Loss: 0.44475340843200684\n",
      "Iteration 46/1000, Loss: 0.4400108754634857\n",
      "Iteration 47/1000, Loss: 0.4354574978351593\n",
      "Iteration 48/1000, Loss: 0.4310808479785919\n",
      "Iteration 49/1000, Loss: 0.4268704354763031\n",
      "Iteration 50/1000, Loss: 0.42281728982925415\n",
      "Iteration 51/1000, Loss: 0.41891223192214966\n",
      "Iteration 52/1000, Loss: 0.4151470959186554\n",
      "Iteration 53/1000, Loss: 0.4115135371685028\n",
      "Iteration 54/1000, Loss: 0.40800413489341736\n",
      "Iteration 55/1000, Loss: 0.4046136140823364\n",
      "Iteration 56/1000, Loss: 0.4013356864452362\n",
      "Iteration 57/1000, Loss: 0.3981640636920929\n",
      "Iteration 58/1000, Loss: 0.3950934112071991\n",
      "Iteration 59/1000, Loss: 0.392118901014328\n",
      "Iteration 60/1000, Loss: 0.3892357349395752\n",
      "Iteration 61/1000, Loss: 0.3864397704601288\n",
      "Iteration 62/1000, Loss: 0.3837262988090515\n",
      "Iteration 63/1000, Loss: 0.38109153509140015\n",
      "Iteration 64/1000, Loss: 0.37853169441223145\n",
      "Iteration 65/1000, Loss: 0.3760436773300171\n",
      "Iteration 66/1000, Loss: 0.3736247420310974\n",
      "Iteration 67/1000, Loss: 0.3712712824344635\n",
      "Iteration 68/1000, Loss: 0.3689805865287781\n",
      "Iteration 69/1000, Loss: 0.36674970388412476\n",
      "Iteration 70/1000, Loss: 0.3645761013031006\n",
      "Iteration 71/1000, Loss: 0.3624577522277832\n",
      "Iteration 72/1000, Loss: 0.36039212346076965\n",
      "Iteration 73/1000, Loss: 0.35837748646736145\n",
      "Iteration 74/1000, Loss: 0.3564113974571228\n",
      "Iteration 75/1000, Loss: 0.3544917106628418\n",
      "Iteration 76/1000, Loss: 0.35261696577072144\n",
      "Iteration 77/1000, Loss: 0.350785493850708\n",
      "Iteration 78/1000, Loss: 0.3489952087402344\n",
      "Iteration 79/1000, Loss: 0.3472447991371155\n",
      "Iteration 80/1000, Loss: 0.3455328047275543\n",
      "Iteration 81/1000, Loss: 0.34385809302330017\n",
      "Iteration 82/1000, Loss: 0.3422190845012665\n",
      "Iteration 83/1000, Loss: 0.3406146764755249\n",
      "Iteration 84/1000, Loss: 0.33904317021369934\n",
      "Iteration 85/1000, Loss: 0.3375038206577301\n",
      "Iteration 86/1000, Loss: 0.33599573373794556\n",
      "Iteration 87/1000, Loss: 0.3345174491405487\n",
      "Iteration 88/1000, Loss: 0.33306801319122314\n",
      "Iteration 89/1000, Loss: 0.3316463530063629\n",
      "Iteration 90/1000, Loss: 0.3302517831325531\n",
      "Iteration 91/1000, Loss: 0.32888326048851013\n",
      "Iteration 92/1000, Loss: 0.32753992080688477\n",
      "Iteration 93/1000, Loss: 0.3262213170528412\n",
      "Iteration 94/1000, Loss: 0.324926495552063\n",
      "Iteration 95/1000, Loss: 0.32365456223487854\n",
      "Iteration 96/1000, Loss: 0.3224049508571625\n",
      "Iteration 97/1000, Loss: 0.3211771249771118\n",
      "Iteration 98/1000, Loss: 0.3199700713157654\n",
      "Iteration 99/1000, Loss: 0.3187836706638336\n",
      "Iteration 100/1000, Loss: 0.3176170587539673\n",
      "Iteration 101/1000, Loss: 0.31646981835365295\n",
      "Iteration 102/1000, Loss: 0.3153414726257324\n",
      "Iteration 103/1000, Loss: 0.31423136591911316\n",
      "Iteration 104/1000, Loss: 0.3131389319896698\n",
      "Iteration 105/1000, Loss: 0.3120637834072113\n",
      "Iteration 106/1000, Loss: 0.3110050857067108\n",
      "Iteration 107/1000, Loss: 0.30996233224868774\n",
      "Iteration 108/1000, Loss: 0.30893510580062866\n",
      "Iteration 109/1000, Loss: 0.3079228401184082\n",
      "Iteration 110/1000, Loss: 0.3069254457950592\n",
      "Iteration 111/1000, Loss: 0.305942565202713\n",
      "Iteration 112/1000, Loss: 0.3049737513065338\n",
      "Iteration 113/1000, Loss: 0.30401864647865295\n",
      "Iteration 114/1000, Loss: 0.30307668447494507\n",
      "Iteration 115/1000, Loss: 0.30214744806289673\n",
      "Iteration 116/1000, Loss: 0.30123066902160645\n",
      "Iteration 117/1000, Loss: 0.3003263473510742\n",
      "Iteration 118/1000, Loss: 0.2994341552257538\n",
      "Iteration 119/1000, Loss: 0.2985536754131317\n",
      "Iteration 120/1000, Loss: 0.2976847290992737\n",
      "Iteration 121/1000, Loss: 0.2968270182609558\n",
      "Iteration 122/1000, Loss: 0.295980304479599\n",
      "Iteration 123/1000, Loss: 0.2951441705226898\n",
      "Iteration 124/1000, Loss: 0.2943183481693268\n",
      "Iteration 125/1000, Loss: 0.2935028076171875\n",
      "Iteration 126/1000, Loss: 0.29269734025001526\n",
      "Iteration 127/1000, Loss: 0.29190149903297424\n",
      "Iteration 128/1000, Loss: 0.29111504554748535\n",
      "Iteration 129/1000, Loss: 0.2903379499912262\n",
      "Iteration 130/1000, Loss: 0.2895699739456177\n",
      "Iteration 131/1000, Loss: 0.28881072998046875\n",
      "Iteration 132/1000, Loss: 0.2880600392818451\n",
      "Iteration 133/1000, Loss: 0.2873177230358124\n",
      "Iteration 134/1000, Loss: 0.28658363223075867\n",
      "Iteration 135/1000, Loss: 0.28585782647132874\n",
      "Iteration 136/1000, Loss: 0.2851400375366211\n",
      "Iteration 137/1000, Loss: 0.28442996740341187\n",
      "Iteration 138/1000, Loss: 0.2837275266647339\n",
      "Iteration 139/1000, Loss: 0.2830325961112976\n",
      "Iteration 140/1000, Loss: 0.28234484791755676\n",
      "Iteration 141/1000, Loss: 0.2816642224788666\n",
      "Iteration 142/1000, Loss: 0.28099045157432556\n",
      "Iteration 143/1000, Loss: 0.28032350540161133\n",
      "Iteration 144/1000, Loss: 0.2796630263328552\n",
      "Iteration 145/1000, Loss: 0.2790091335773468\n",
      "Iteration 146/1000, Loss: 0.2783619165420532\n",
      "Iteration 147/1000, Loss: 0.2777210474014282\n",
      "Iteration 148/1000, Loss: 0.27708640694618225\n",
      "Iteration 149/1000, Loss: 0.27645793557167053\n",
      "Iteration 150/1000, Loss: 0.27583545446395874\n",
      "Iteration 151/1000, Loss: 0.27521875500679016\n",
      "Iteration 152/1000, Loss: 0.27460747957229614\n",
      "Iteration 153/1000, Loss: 0.27400195598602295\n",
      "Iteration 154/1000, Loss: 0.2734018564224243\n",
      "Iteration 155/1000, Loss: 0.2728072702884674\n",
      "Iteration 156/1000, Loss: 0.2722182273864746\n",
      "Iteration 157/1000, Loss: 0.2716345489025116\n",
      "Iteration 158/1000, Loss: 0.27105599641799927\n",
      "Iteration 159/1000, Loss: 0.2704826593399048\n",
      "Iteration 160/1000, Loss: 0.26991426944732666\n",
      "Iteration 161/1000, Loss: 0.2693505883216858\n",
      "Iteration 162/1000, Loss: 0.2687916159629822\n",
      "Iteration 163/1000, Loss: 0.2682372033596039\n",
      "Iteration 164/1000, Loss: 0.2676875591278076\n",
      "Iteration 165/1000, Loss: 0.2671424150466919\n",
      "Iteration 166/1000, Loss: 0.26660168170928955\n",
      "Iteration 167/1000, Loss: 0.266065388917923\n",
      "Iteration 168/1000, Loss: 0.2655334770679474\n",
      "Iteration 169/1000, Loss: 0.26500585675239563\n",
      "Iteration 170/1000, Loss: 0.26448240876197815\n",
      "Iteration 171/1000, Loss: 0.26396307349205017\n",
      "Iteration 172/1000, Loss: 0.26344776153564453\n",
      "Iteration 173/1000, Loss: 0.2629365026950836\n",
      "Iteration 174/1000, Loss: 0.2624293267726898\n",
      "Iteration 175/1000, Loss: 0.2619258463382721\n",
      "Iteration 176/1000, Loss: 0.26142618060112\n",
      "Iteration 177/1000, Loss: 0.2609303593635559\n",
      "Iteration 178/1000, Loss: 0.2604382038116455\n",
      "Iteration 179/1000, Loss: 0.25994962453842163\n",
      "Iteration 180/1000, Loss: 0.2594646215438843\n",
      "Iteration 181/1000, Loss: 0.2589828073978424\n",
      "Iteration 182/1000, Loss: 0.2585044205188751\n",
      "Iteration 183/1000, Loss: 0.25802943110466003\n",
      "Iteration 184/1000, Loss: 0.2575578987598419\n",
      "Iteration 185/1000, Loss: 0.25708967447280884\n",
      "Iteration 186/1000, Loss: 0.2566247284412384\n",
      "Iteration 187/1000, Loss: 0.2561630606651306\n",
      "Iteration 188/1000, Loss: 0.25570473074913025\n",
      "Iteration 189/1000, Loss: 0.2552495300769806\n",
      "Iteration 190/1000, Loss: 0.2547973394393921\n",
      "Iteration 191/1000, Loss: 0.25434809923171997\n",
      "Iteration 192/1000, Loss: 0.253901869058609\n",
      "Iteration 193/1000, Loss: 0.25345855951309204\n",
      "Iteration 194/1000, Loss: 0.2530181109905243\n",
      "Iteration 195/1000, Loss: 0.2525804042816162\n",
      "Iteration 196/1000, Loss: 0.2521454989910126\n",
      "Iteration 197/1000, Loss: 0.251713365316391\n",
      "Iteration 198/1000, Loss: 0.2512839138507843\n",
      "Iteration 199/1000, Loss: 0.2508571147918701\n",
      "Iteration 200/1000, Loss: 0.25043320655822754\n",
      "Iteration 201/1000, Loss: 0.2500119209289551\n",
      "Iteration 202/1000, Loss: 0.24959325790405273\n",
      "Iteration 203/1000, Loss: 0.24917718768119812\n",
      "Iteration 204/1000, Loss: 0.24876384437084198\n",
      "Iteration 205/1000, Loss: 0.24835309386253357\n",
      "Iteration 206/1000, Loss: 0.24794478714466095\n",
      "Iteration 207/1000, Loss: 0.2475389689207077\n",
      "Iteration 208/1000, Loss: 0.2471354454755783\n",
      "Iteration 209/1000, Loss: 0.24673421680927277\n",
      "Iteration 210/1000, Loss: 0.24633534252643585\n",
      "Iteration 211/1000, Loss: 0.24593867361545563\n",
      "Iteration 212/1000, Loss: 0.2455442100763321\n",
      "Iteration 213/1000, Loss: 0.24515199661254883\n",
      "Iteration 214/1000, Loss: 0.24476206302642822\n",
      "Iteration 215/1000, Loss: 0.2443743795156479\n",
      "Iteration 216/1000, Loss: 0.24398888647556305\n",
      "Iteration 217/1000, Loss: 0.2436055839061737\n",
      "Iteration 218/1000, Loss: 0.24322447180747986\n",
      "Iteration 219/1000, Loss: 0.2428455948829651\n",
      "Iteration 220/1000, Loss: 0.2424686998128891\n",
      "Iteration 221/1000, Loss: 0.24209395051002502\n",
      "Iteration 222/1000, Loss: 0.24172131717205048\n",
      "Iteration 223/1000, Loss: 0.24135084450244904\n",
      "Iteration 224/1000, Loss: 0.24098260700702667\n",
      "Iteration 225/1000, Loss: 0.24061642587184906\n",
      "Iteration 226/1000, Loss: 0.24025200307369232\n",
      "Iteration 227/1000, Loss: 0.23988938331604004\n",
      "Iteration 228/1000, Loss: 0.2395288199186325\n",
      "Iteration 229/1000, Loss: 0.23917017877101898\n",
      "Iteration 230/1000, Loss: 0.23881345987319946\n",
      "Iteration 231/1000, Loss: 0.23845863342285156\n",
      "Iteration 232/1000, Loss: 0.23810553550720215\n",
      "Iteration 233/1000, Loss: 0.23775416612625122\n",
      "Iteration 234/1000, Loss: 0.23740451037883759\n",
      "Iteration 235/1000, Loss: 0.23705671727657318\n",
      "Iteration 236/1000, Loss: 0.23671084642410278\n",
      "Iteration 237/1000, Loss: 0.23636670410633087\n",
      "Iteration 238/1000, Loss: 0.2360241860151291\n",
      "Iteration 239/1000, Loss: 0.23568347096443176\n",
      "Iteration 240/1000, Loss: 0.23534446954727173\n",
      "Iteration 241/1000, Loss: 0.2350071519613266\n",
      "Iteration 242/1000, Loss: 0.23467154800891876\n",
      "Iteration 243/1000, Loss: 0.23433764278888702\n",
      "Iteration 244/1000, Loss: 0.23400524258613586\n",
      "Iteration 245/1000, Loss: 0.2336745411157608\n",
      "Iteration 246/1000, Loss: 0.2333454042673111\n",
      "Iteration 247/1000, Loss: 0.23301777243614197\n",
      "Iteration 248/1000, Loss: 0.2326917052268982\n",
      "Iteration 249/1000, Loss: 0.23236703872680664\n",
      "Iteration 250/1000, Loss: 0.23204387724399567\n",
      "Iteration 251/1000, Loss: 0.23172210156917572\n",
      "Iteration 252/1000, Loss: 0.23140166699886322\n",
      "Iteration 253/1000, Loss: 0.23108261823654175\n",
      "Iteration 254/1000, Loss: 0.23076507449150085\n",
      "Iteration 255/1000, Loss: 0.2304491549730301\n",
      "Iteration 256/1000, Loss: 0.2301347702741623\n",
      "Iteration 257/1000, Loss: 0.22982177138328552\n",
      "Iteration 258/1000, Loss: 0.22951020300388336\n",
      "Iteration 259/1000, Loss: 0.22920012474060059\n",
      "Iteration 260/1000, Loss: 0.22889134287834167\n",
      "Iteration 261/1000, Loss: 0.22858399152755737\n",
      "Iteration 262/1000, Loss: 0.2282780259847641\n",
      "Iteration 263/1000, Loss: 0.2279733568429947\n",
      "Iteration 264/1000, Loss: 0.22766995429992676\n",
      "Iteration 265/1000, Loss: 0.22736796736717224\n",
      "Iteration 266/1000, Loss: 0.22706733644008636\n",
      "Iteration 267/1000, Loss: 0.22676800191402435\n",
      "Iteration 268/1000, Loss: 0.22647011280059814\n",
      "Iteration 269/1000, Loss: 0.22617356479167938\n",
      "Iteration 270/1000, Loss: 0.22587835788726807\n",
      "Iteration 271/1000, Loss: 0.22558438777923584\n",
      "Iteration 272/1000, Loss: 0.2252916395664215\n",
      "Iteration 273/1000, Loss: 0.22500000894069672\n",
      "Iteration 274/1000, Loss: 0.2247096747159958\n",
      "Iteration 275/1000, Loss: 0.2244204580783844\n",
      "Iteration 276/1000, Loss: 0.22413240373134613\n",
      "Iteration 277/1000, Loss: 0.22384558618068695\n",
      "Iteration 278/1000, Loss: 0.22356000542640686\n",
      "Iteration 279/1000, Loss: 0.22327561676502228\n",
      "Iteration 280/1000, Loss: 0.22299231588840485\n",
      "Iteration 281/1000, Loss: 0.22271019220352173\n",
      "Iteration 282/1000, Loss: 0.22242917120456696\n",
      "Iteration 283/1000, Loss: 0.22214925289154053\n",
      "Iteration 284/1000, Loss: 0.22187043726444244\n",
      "Iteration 285/1000, Loss: 0.22159279882907867\n",
      "Iteration 286/1000, Loss: 0.22131624817848206\n",
      "Iteration 287/1000, Loss: 0.22104069590568542\n",
      "Iteration 288/1000, Loss: 0.22076629102230072\n",
      "Iteration 289/1000, Loss: 0.22049298882484436\n",
      "Iteration 290/1000, Loss: 0.22022071480751038\n",
      "Iteration 291/1000, Loss: 0.21994942426681519\n",
      "Iteration 292/1000, Loss: 0.21967922151088715\n",
      "Iteration 293/1000, Loss: 0.21941007673740387\n",
      "Iteration 294/1000, Loss: 0.2191421240568161\n",
      "Iteration 295/1000, Loss: 0.21887527406215668\n",
      "Iteration 296/1000, Loss: 0.21860945224761963\n",
      "Iteration 297/1000, Loss: 0.21834465861320496\n",
      "Iteration 298/1000, Loss: 0.21808089315891266\n",
      "Iteration 299/1000, Loss: 0.21781803667545319\n",
      "Iteration 300/1000, Loss: 0.21755607426166534\n",
      "Iteration 301/1000, Loss: 0.21729500591754913\n",
      "Iteration 302/1000, Loss: 0.21703502535820007\n",
      "Iteration 303/1000, Loss: 0.21677610278129578\n",
      "Iteration 304/1000, Loss: 0.21651805937290192\n",
      "Iteration 305/1000, Loss: 0.21626093983650208\n",
      "Iteration 306/1000, Loss: 0.21600471436977386\n",
      "Iteration 307/1000, Loss: 0.21574941277503967\n",
      "Iteration 308/1000, Loss: 0.2154950052499771\n",
      "Iteration 309/1000, Loss: 0.21524150669574738\n",
      "Iteration 310/1000, Loss: 0.21498900651931763\n",
      "Iteration 311/1000, Loss: 0.21473735570907593\n",
      "Iteration 312/1000, Loss: 0.21448661386966705\n",
      "Iteration 313/1000, Loss: 0.21423672139644623\n",
      "Iteration 314/1000, Loss: 0.21398767828941345\n",
      "Iteration 315/1000, Loss: 0.2137395143508911\n",
      "Iteration 316/1000, Loss: 0.21349231898784637\n",
      "Iteration 317/1000, Loss: 0.21324586868286133\n",
      "Iteration 318/1000, Loss: 0.21300019323825836\n",
      "Iteration 319/1000, Loss: 0.2127554714679718\n",
      "Iteration 320/1000, Loss: 0.21251164376735687\n",
      "Iteration 321/1000, Loss: 0.21226869523525238\n",
      "Iteration 322/1000, Loss: 0.2120266854763031\n",
      "Iteration 323/1000, Loss: 0.2117854356765747\n",
      "Iteration 324/1000, Loss: 0.21154502034187317\n",
      "Iteration 325/1000, Loss: 0.2113054245710373\n",
      "Iteration 326/1000, Loss: 0.21106655895709991\n",
      "Iteration 327/1000, Loss: 0.2108285129070282\n",
      "Iteration 328/1000, Loss: 0.21059134602546692\n",
      "Iteration 329/1000, Loss: 0.21035504341125488\n",
      "Iteration 330/1000, Loss: 0.2101195752620697\n",
      "Iteration 331/1000, Loss: 0.20988498628139496\n",
      "Iteration 332/1000, Loss: 0.20965120196342468\n",
      "Iteration 333/1000, Loss: 0.20941820740699768\n",
      "Iteration 334/1000, Loss: 0.20918594300746918\n",
      "Iteration 335/1000, Loss: 0.20895428955554962\n",
      "Iteration 336/1000, Loss: 0.20872347056865692\n",
      "Iteration 337/1000, Loss: 0.2084934264421463\n",
      "Iteration 338/1000, Loss: 0.20826414227485657\n",
      "Iteration 339/1000, Loss: 0.20803554356098175\n",
      "Iteration 340/1000, Loss: 0.2078077644109726\n",
      "Iteration 341/1000, Loss: 0.20758076012134552\n",
      "Iteration 342/1000, Loss: 0.20735445618629456\n",
      "Iteration 343/1000, Loss: 0.20712894201278687\n",
      "Iteration 344/1000, Loss: 0.20690423250198364\n",
      "Iteration 345/1000, Loss: 0.2066803127527237\n",
      "Iteration 346/1000, Loss: 0.20645716786384583\n",
      "Iteration 347/1000, Loss: 0.20623469352722168\n",
      "Iteration 348/1000, Loss: 0.20601294934749603\n",
      "Iteration 349/1000, Loss: 0.20579196512699127\n",
      "Iteration 350/1000, Loss: 0.205571711063385\n",
      "Iteration 351/1000, Loss: 0.20535209774971008\n",
      "Iteration 352/1000, Loss: 0.20513315498828888\n",
      "Iteration 353/1000, Loss: 0.2049148976802826\n",
      "Iteration 354/1000, Loss: 0.2046973705291748\n",
      "Iteration 355/1000, Loss: 0.20448046922683716\n",
      "Iteration 356/1000, Loss: 0.20426425337791443\n",
      "Iteration 357/1000, Loss: 0.2040487676858902\n",
      "Iteration 358/1000, Loss: 0.2038339376449585\n",
      "Iteration 359/1000, Loss: 0.20361965894699097\n",
      "Iteration 360/1000, Loss: 0.20340600609779358\n",
      "Iteration 361/1000, Loss: 0.2031930387020111\n",
      "Iteration 362/1000, Loss: 0.20298072695732117\n",
      "Iteration 363/1000, Loss: 0.20276914536952972\n",
      "Iteration 364/1000, Loss: 0.202558234333992\n",
      "Iteration 365/1000, Loss: 0.20234797894954681\n",
      "Iteration 366/1000, Loss: 0.2021384835243225\n",
      "Iteration 367/1000, Loss: 0.20192958414554596\n",
      "Iteration 368/1000, Loss: 0.20172129571437836\n",
      "Iteration 369/1000, Loss: 0.20151367783546448\n",
      "Iteration 370/1000, Loss: 0.20130665600299835\n",
      "Iteration 371/1000, Loss: 0.20110027492046356\n",
      "Iteration 372/1000, Loss: 0.20089450478553772\n",
      "Iteration 373/1000, Loss: 0.20068934559822083\n",
      "Iteration 374/1000, Loss: 0.2004847377538681\n",
      "Iteration 375/1000, Loss: 0.20028075575828552\n",
      "Iteration 376/1000, Loss: 0.20007751882076263\n",
      "Iteration 377/1000, Loss: 0.1998748928308487\n",
      "Iteration 378/1000, Loss: 0.19967280328273773\n",
      "Iteration 379/1000, Loss: 0.19947129487991333\n",
      "Iteration 380/1000, Loss: 0.19927038252353668\n",
      "Iteration 381/1000, Loss: 0.19906997680664062\n",
      "Iteration 382/1000, Loss: 0.1988702118396759\n",
      "Iteration 383/1000, Loss: 0.1986711323261261\n",
      "Iteration 384/1000, Loss: 0.19847257435321808\n",
      "Iteration 385/1000, Loss: 0.1982746124267578\n",
      "Iteration 386/1000, Loss: 0.19807729125022888\n",
      "Iteration 387/1000, Loss: 0.1978805661201477\n",
      "Iteration 388/1000, Loss: 0.1976844221353531\n",
      "Iteration 389/1000, Loss: 0.19748884439468384\n",
      "Iteration 390/1000, Loss: 0.19729386270046234\n",
      "Iteration 391/1000, Loss: 0.19709941744804382\n",
      "Iteration 392/1000, Loss: 0.19690553843975067\n",
      "Iteration 393/1000, Loss: 0.1967121809720993\n",
      "Iteration 394/1000, Loss: 0.19651930034160614\n",
      "Iteration 395/1000, Loss: 0.19632698595523834\n",
      "Iteration 396/1000, Loss: 0.19613520801067352\n",
      "Iteration 397/1000, Loss: 0.1959439218044281\n",
      "Iteration 398/1000, Loss: 0.19575326144695282\n",
      "Iteration 399/1000, Loss: 0.1955631822347641\n",
      "Iteration 400/1000, Loss: 0.19537363946437836\n",
      "Iteration 401/1000, Loss: 0.195184588432312\n",
      "Iteration 402/1000, Loss: 0.19499605894088745\n",
      "Iteration 403/1000, Loss: 0.1948080211877823\n",
      "Iteration 404/1000, Loss: 0.19462047517299652\n",
      "Iteration 405/1000, Loss: 0.19443343579769135\n",
      "Iteration 406/1000, Loss: 0.19424685835838318\n",
      "Iteration 407/1000, Loss: 0.19406084716320038\n",
      "Iteration 408/1000, Loss: 0.19387532770633698\n",
      "Iteration 409/1000, Loss: 0.19369034469127655\n",
      "Iteration 410/1000, Loss: 0.1935058981180191\n",
      "Iteration 411/1000, Loss: 0.19332197308540344\n",
      "Iteration 412/1000, Loss: 0.19313855469226837\n",
      "Iteration 413/1000, Loss: 0.1929555982351303\n",
      "Iteration 414/1000, Loss: 0.19277320802211761\n",
      "Iteration 415/1000, Loss: 0.19259126484394073\n",
      "Iteration 416/1000, Loss: 0.19240979850292206\n",
      "Iteration 417/1000, Loss: 0.19222894310951233\n",
      "Iteration 418/1000, Loss: 0.1920485645532608\n",
      "Iteration 419/1000, Loss: 0.19186867773532867\n",
      "Iteration 420/1000, Loss: 0.1916893571615219\n",
      "Iteration 421/1000, Loss: 0.19151054322719574\n",
      "Iteration 422/1000, Loss: 0.19133226573467255\n",
      "Iteration 423/1000, Loss: 0.1911543905735016\n",
      "Iteration 424/1000, Loss: 0.19097699224948883\n",
      "Iteration 425/1000, Loss: 0.1908000260591507\n",
      "Iteration 426/1000, Loss: 0.19062350690364838\n",
      "Iteration 427/1000, Loss: 0.19044746458530426\n",
      "Iteration 428/1000, Loss: 0.19027183949947357\n",
      "Iteration 429/1000, Loss: 0.19009676575660706\n",
      "Iteration 430/1000, Loss: 0.18992218375205994\n",
      "Iteration 431/1000, Loss: 0.18974807858467102\n",
      "Iteration 432/1000, Loss: 0.1895744353532791\n",
      "Iteration 433/1000, Loss: 0.18940120935440063\n",
      "Iteration 434/1000, Loss: 0.18922841548919678\n",
      "Iteration 435/1000, Loss: 0.18905603885650635\n",
      "Iteration 436/1000, Loss: 0.18888412415981293\n",
      "Iteration 437/1000, Loss: 0.18871265649795532\n",
      "Iteration 438/1000, Loss: 0.18854163587093353\n",
      "Iteration 439/1000, Loss: 0.18837106227874756\n",
      "Iteration 440/1000, Loss: 0.1882009208202362\n",
      "Iteration 441/1000, Loss: 0.18803121149539948\n",
      "Iteration 442/1000, Loss: 0.18786194920539856\n",
      "Iteration 443/1000, Loss: 0.18769316375255585\n",
      "Iteration 444/1000, Loss: 0.18752475082874298\n",
      "Iteration 445/1000, Loss: 0.18735674023628235\n",
      "Iteration 446/1000, Loss: 0.18718911707401276\n",
      "Iteration 447/1000, Loss: 0.18702194094657898\n",
      "Iteration 448/1000, Loss: 0.18685518205165863\n",
      "Iteration 449/1000, Loss: 0.1866888403892517\n",
      "Iteration 450/1000, Loss: 0.18652285635471344\n",
      "Iteration 451/1000, Loss: 0.1863572895526886\n",
      "Iteration 452/1000, Loss: 0.18619218468666077\n",
      "Iteration 453/1000, Loss: 0.18602751195430756\n",
      "Iteration 454/1000, Loss: 0.185863196849823\n",
      "Iteration 455/1000, Loss: 0.18569926917552948\n",
      "Iteration 456/1000, Loss: 0.1855357587337494\n",
      "Iteration 457/1000, Loss: 0.18537263572216034\n",
      "Iteration 458/1000, Loss: 0.18520992994308472\n",
      "Iteration 459/1000, Loss: 0.18504765629768372\n",
      "Iteration 460/1000, Loss: 0.18488582968711853\n",
      "Iteration 461/1000, Loss: 0.18472439050674438\n",
      "Iteration 462/1000, Loss: 0.18456338346004486\n",
      "Iteration 463/1000, Loss: 0.18440276384353638\n",
      "Iteration 464/1000, Loss: 0.18424256145954132\n",
      "Iteration 465/1000, Loss: 0.18408270180225372\n",
      "Iteration 466/1000, Loss: 0.18392325937747955\n",
      "Iteration 467/1000, Loss: 0.18376420438289642\n",
      "Iteration 468/1000, Loss: 0.18360550701618195\n",
      "Iteration 469/1000, Loss: 0.1834472268819809\n",
      "Iteration 470/1000, Loss: 0.18328937888145447\n",
      "Iteration 471/1000, Loss: 0.18313190340995789\n",
      "Iteration 472/1000, Loss: 0.18297480046749115\n",
      "Iteration 473/1000, Loss: 0.18281807005405426\n",
      "Iteration 474/1000, Loss: 0.18266168236732483\n",
      "Iteration 475/1000, Loss: 0.18250569701194763\n",
      "Iteration 476/1000, Loss: 0.1823500692844391\n",
      "Iteration 477/1000, Loss: 0.18219473958015442\n",
      "Iteration 478/1000, Loss: 0.18203972280025482\n",
      "Iteration 479/1000, Loss: 0.18188509345054626\n",
      "Iteration 480/1000, Loss: 0.18173076212406158\n",
      "Iteration 481/1000, Loss: 0.18157680332660675\n",
      "Iteration 482/1000, Loss: 0.18142324686050415\n",
      "Iteration 483/1000, Loss: 0.18126998841762543\n",
      "Iteration 484/1000, Loss: 0.18111711740493774\n",
      "Iteration 485/1000, Loss: 0.1809646487236023\n",
      "Iteration 486/1000, Loss: 0.1808125525712967\n",
      "Iteration 487/1000, Loss: 0.18066084384918213\n",
      "Iteration 488/1000, Loss: 0.1805095672607422\n",
      "Iteration 489/1000, Loss: 0.18035855889320374\n",
      "Iteration 490/1000, Loss: 0.18020790815353394\n",
      "Iteration 491/1000, Loss: 0.1800576150417328\n",
      "Iteration 492/1000, Loss: 0.1799076497554779\n",
      "Iteration 493/1000, Loss: 0.17975808680057526\n",
      "Iteration 494/1000, Loss: 0.17960885167121887\n",
      "Iteration 495/1000, Loss: 0.17945994436740875\n",
      "Iteration 496/1000, Loss: 0.17931127548217773\n",
      "Iteration 497/1000, Loss: 0.1791629195213318\n",
      "Iteration 498/1000, Loss: 0.1790149062871933\n",
      "Iteration 499/1000, Loss: 0.17886723577976227\n",
      "Iteration 500/1000, Loss: 0.17871986329555511\n",
      "Iteration 501/1000, Loss: 0.17857281863689423\n",
      "Iteration 502/1000, Loss: 0.17842614650726318\n",
      "Iteration 503/1000, Loss: 0.1782797873020172\n",
      "Iteration 504/1000, Loss: 0.17813372611999512\n",
      "Iteration 505/1000, Loss: 0.1779879927635193\n",
      "Iteration 506/1000, Loss: 0.17784254252910614\n",
      "Iteration 507/1000, Loss: 0.17769739031791687\n",
      "Iteration 508/1000, Loss: 0.17755261063575745\n",
      "Iteration 509/1000, Loss: 0.1774081438779831\n",
      "Iteration 510/1000, Loss: 0.17726393043994904\n",
      "Iteration 511/1000, Loss: 0.17712004482746124\n",
      "Iteration 512/1000, Loss: 0.17697647213935852\n",
      "Iteration 513/1000, Loss: 0.17683318257331848\n",
      "Iteration 514/1000, Loss: 0.1766902357339859\n",
      "Iteration 515/1000, Loss: 0.1765476018190384\n",
      "Iteration 516/1000, Loss: 0.17640532553195953\n",
      "Iteration 517/1000, Loss: 0.17626336216926575\n",
      "Iteration 518/1000, Loss: 0.17612174153327942\n",
      "Iteration 519/1000, Loss: 0.17598041892051697\n",
      "Iteration 520/1000, Loss: 0.17583943903446198\n",
      "Iteration 521/1000, Loss: 0.17569875717163086\n",
      "Iteration 522/1000, Loss: 0.17555832862854004\n",
      "Iteration 523/1000, Loss: 0.1754181981086731\n",
      "Iteration 524/1000, Loss: 0.17527832090854645\n",
      "Iteration 525/1000, Loss: 0.1751386970281601\n",
      "Iteration 526/1000, Loss: 0.17499946057796478\n",
      "Iteration 527/1000, Loss: 0.17486053705215454\n",
      "Iteration 528/1000, Loss: 0.1747218817472458\n",
      "Iteration 529/1000, Loss: 0.1745835542678833\n",
      "Iteration 530/1000, Loss: 0.1744455099105835\n",
      "Iteration 531/1000, Loss: 0.1743076890707016\n",
      "Iteration 532/1000, Loss: 0.17417012155056\n",
      "Iteration 533/1000, Loss: 0.1740328073501587\n",
      "Iteration 534/1000, Loss: 0.17389585077762604\n",
      "Iteration 535/1000, Loss: 0.17375914752483368\n",
      "Iteration 536/1000, Loss: 0.1736227571964264\n",
      "Iteration 537/1000, Loss: 0.1734866350889206\n",
      "Iteration 538/1000, Loss: 0.17335082590579987\n",
      "Iteration 539/1000, Loss: 0.1732153296470642\n",
      "Iteration 540/1000, Loss: 0.17308011651039124\n",
      "Iteration 541/1000, Loss: 0.17294515669345856\n",
      "Iteration 542/1000, Loss: 0.17281043529510498\n",
      "Iteration 543/1000, Loss: 0.1726759672164917\n",
      "Iteration 544/1000, Loss: 0.1725417673587799\n",
      "Iteration 545/1000, Loss: 0.17240780591964722\n",
      "Iteration 546/1000, Loss: 0.1722741574048996\n",
      "Iteration 547/1000, Loss: 0.1721406877040863\n",
      "Iteration 548/1000, Loss: 0.17200754582881927\n",
      "Iteration 549/1000, Loss: 0.17187467217445374\n",
      "Iteration 550/1000, Loss: 0.1717420220375061\n",
      "Iteration 551/1000, Loss: 0.17160969972610474\n",
      "Iteration 552/1000, Loss: 0.17147767543792725\n",
      "Iteration 553/1000, Loss: 0.17134593427181244\n",
      "Iteration 554/1000, Loss: 0.1712144911289215\n",
      "Iteration 555/1000, Loss: 0.17108334600925446\n",
      "Iteration 556/1000, Loss: 0.1709524691104889\n",
      "Iteration 557/1000, Loss: 0.17082186043262482\n",
      "Iteration 558/1000, Loss: 0.17069147527217865\n",
      "Iteration 559/1000, Loss: 0.1705613136291504\n",
      "Iteration 560/1000, Loss: 0.17043134570121765\n",
      "Iteration 561/1000, Loss: 0.17030157148838043\n",
      "Iteration 562/1000, Loss: 0.1701720654964447\n",
      "Iteration 563/1000, Loss: 0.17004281282424927\n",
      "Iteration 564/1000, Loss: 0.16991379857063293\n",
      "Iteration 565/1000, Loss: 0.1697850078344345\n",
      "Iteration 566/1000, Loss: 0.16965651512145996\n",
      "Iteration 567/1000, Loss: 0.1695282757282257\n",
      "Iteration 568/1000, Loss: 0.16940027475357056\n",
      "Iteration 569/1000, Loss: 0.1692725121974945\n",
      "Iteration 570/1000, Loss: 0.16914500296115875\n",
      "Iteration 571/1000, Loss: 0.16901777684688568\n",
      "Iteration 572/1000, Loss: 0.16889077425003052\n",
      "Iteration 573/1000, Loss: 0.16876406967639923\n",
      "Iteration 574/1000, Loss: 0.16863760352134705\n",
      "Iteration 575/1000, Loss: 0.16851136088371277\n",
      "Iteration 576/1000, Loss: 0.16838543117046356\n",
      "Iteration 577/1000, Loss: 0.16825976967811584\n",
      "Iteration 578/1000, Loss: 0.16813434660434723\n",
      "Iteration 579/1000, Loss: 0.1680091768503189\n",
      "Iteration 580/1000, Loss: 0.1678842455148697\n",
      "Iteration 581/1000, Loss: 0.16775955259799957\n",
      "Iteration 582/1000, Loss: 0.16763511300086975\n",
      "Iteration 583/1000, Loss: 0.16751092672348022\n",
      "Iteration 584/1000, Loss: 0.1673870086669922\n",
      "Iteration 585/1000, Loss: 0.16726337373256683\n",
      "Iteration 586/1000, Loss: 0.16714000701904297\n",
      "Iteration 587/1000, Loss: 0.1670169085264206\n",
      "Iteration 588/1000, Loss: 0.16689404845237732\n",
      "Iteration 589/1000, Loss: 0.16677138209342957\n",
      "Iteration 590/1000, Loss: 0.16664887964725494\n",
      "Iteration 591/1000, Loss: 0.16652660071849823\n",
      "Iteration 592/1000, Loss: 0.1664045751094818\n",
      "Iteration 593/1000, Loss: 0.1662827730178833\n",
      "Iteration 594/1000, Loss: 0.16616122424602509\n",
      "Iteration 595/1000, Loss: 0.1660398691892624\n",
      "Iteration 596/1000, Loss: 0.16591876745224\n",
      "Iteration 597/1000, Loss: 0.16579782962799072\n",
      "Iteration 598/1000, Loss: 0.16567718982696533\n",
      "Iteration 599/1000, Loss: 0.16555678844451904\n",
      "Iteration 600/1000, Loss: 0.16543659567832947\n",
      "Iteration 601/1000, Loss: 0.16531658172607422\n",
      "Iteration 602/1000, Loss: 0.16519686579704285\n",
      "Iteration 603/1000, Loss: 0.1650772988796234\n",
      "Iteration 604/1000, Loss: 0.1649579554796219\n",
      "Iteration 605/1000, Loss: 0.16483883559703827\n",
      "Iteration 606/1000, Loss: 0.16471996903419495\n",
      "Iteration 607/1000, Loss: 0.16460128128528595\n",
      "Iteration 608/1000, Loss: 0.16448281705379486\n",
      "Iteration 609/1000, Loss: 0.1643645465373993\n",
      "Iteration 610/1000, Loss: 0.16424648463726044\n",
      "Iteration 611/1000, Loss: 0.1641286462545395\n",
      "Iteration 612/1000, Loss: 0.16401100158691406\n",
      "Iteration 613/1000, Loss: 0.16389358043670654\n",
      "Iteration 614/1000, Loss: 0.16377639770507812\n",
      "Iteration 615/1000, Loss: 0.16365946829319\n",
      "Iteration 616/1000, Loss: 0.1635427176952362\n",
      "Iteration 617/1000, Loss: 0.1634262055158615\n",
      "Iteration 618/1000, Loss: 0.1633099466562271\n",
      "Iteration 619/1000, Loss: 0.163193941116333\n",
      "Iteration 620/1000, Loss: 0.1630781590938568\n",
      "Iteration 621/1000, Loss: 0.16296260058879852\n",
      "Iteration 622/1000, Loss: 0.16284725069999695\n",
      "Iteration 623/1000, Loss: 0.1627320945262909\n",
      "Iteration 624/1000, Loss: 0.16261716187000275\n",
      "Iteration 625/1000, Loss: 0.16250240802764893\n",
      "Iteration 626/1000, Loss: 0.1623878926038742\n",
      "Iteration 627/1000, Loss: 0.16227354109287262\n",
      "Iteration 628/1000, Loss: 0.16215939819812775\n",
      "Iteration 629/1000, Loss: 0.1620454490184784\n",
      "Iteration 630/1000, Loss: 0.16193170845508575\n",
      "Iteration 631/1000, Loss: 0.16181813180446625\n",
      "Iteration 632/1000, Loss: 0.16170471906661987\n",
      "Iteration 633/1000, Loss: 0.1615915298461914\n",
      "Iteration 634/1000, Loss: 0.16147854924201965\n",
      "Iteration 635/1000, Loss: 0.16136571764945984\n",
      "Iteration 636/1000, Loss: 0.16125309467315674\n",
      "Iteration 637/1000, Loss: 0.16114063560962677\n",
      "Iteration 638/1000, Loss: 0.16102835536003113\n",
      "Iteration 639/1000, Loss: 0.1609162986278534\n",
      "Iteration 640/1000, Loss: 0.16080442070960999\n",
      "Iteration 641/1000, Loss: 0.16069281101226807\n",
      "Iteration 642/1000, Loss: 0.16058140993118286\n",
      "Iteration 643/1000, Loss: 0.16047021746635437\n",
      "Iteration 644/1000, Loss: 0.1603592187166214\n",
      "Iteration 645/1000, Loss: 0.16024838387966156\n",
      "Iteration 646/1000, Loss: 0.16013775765895844\n",
      "Iteration 647/1000, Loss: 0.16002734005451202\n",
      "Iteration 648/1000, Loss: 0.1599172055721283\n",
      "Iteration 649/1000, Loss: 0.1598072052001953\n",
      "Iteration 650/1000, Loss: 0.15969736874103546\n",
      "Iteration 651/1000, Loss: 0.15958775579929352\n",
      "Iteration 652/1000, Loss: 0.1594783514738083\n",
      "Iteration 653/1000, Loss: 0.15936917066574097\n",
      "Iteration 654/1000, Loss: 0.15926016867160797\n",
      "Iteration 655/1000, Loss: 0.1591513305902481\n",
      "Iteration 656/1000, Loss: 0.15904267132282257\n",
      "Iteration 657/1000, Loss: 0.15893417596817017\n",
      "Iteration 658/1000, Loss: 0.15882587432861328\n",
      "Iteration 659/1000, Loss: 0.15871784090995789\n",
      "Iteration 660/1000, Loss: 0.15860994160175323\n",
      "Iteration 661/1000, Loss: 0.1585022658109665\n",
      "Iteration 662/1000, Loss: 0.15839475393295288\n",
      "Iteration 663/1000, Loss: 0.1582874357700348\n",
      "Iteration 664/1000, Loss: 0.15818026661872864\n",
      "Iteration 665/1000, Loss: 0.1580733060836792\n",
      "Iteration 666/1000, Loss: 0.15796655416488647\n",
      "Iteration 667/1000, Loss: 0.15785996615886688\n",
      "Iteration 668/1000, Loss: 0.15775354206562042\n",
      "Iteration 669/1000, Loss: 0.1576472669839859\n",
      "Iteration 670/1000, Loss: 0.1575411856174469\n",
      "Iteration 671/1000, Loss: 0.15743528306484222\n",
      "Iteration 672/1000, Loss: 0.15732957422733307\n",
      "Iteration 673/1000, Loss: 0.15722405910491943\n",
      "Iteration 674/1000, Loss: 0.15711873769760132\n",
      "Iteration 675/1000, Loss: 0.15701361000537872\n",
      "Iteration 676/1000, Loss: 0.15690864622592926\n",
      "Iteration 677/1000, Loss: 0.15680384635925293\n",
      "Iteration 678/1000, Loss: 0.15669922530651093\n",
      "Iteration 679/1000, Loss: 0.15659475326538086\n",
      "Iteration 680/1000, Loss: 0.1564904749393463\n",
      "Iteration 681/1000, Loss: 0.1563863456249237\n",
      "Iteration 682/1000, Loss: 0.15628239512443542\n",
      "Iteration 683/1000, Loss: 0.15617862343788147\n",
      "Iteration 684/1000, Loss: 0.15607506036758423\n",
      "Iteration 685/1000, Loss: 0.1559716761112213\n",
      "Iteration 686/1000, Loss: 0.15586845576763153\n",
      "Iteration 687/1000, Loss: 0.15576539933681488\n",
      "Iteration 688/1000, Loss: 0.15566246211528778\n",
      "Iteration 689/1000, Loss: 0.15555976331233978\n",
      "Iteration 690/1000, Loss: 0.15545716881752014\n",
      "Iteration 691/1000, Loss: 0.15535472333431244\n",
      "Iteration 692/1000, Loss: 0.15525244176387787\n",
      "Iteration 693/1000, Loss: 0.15515035390853882\n",
      "Iteration 694/1000, Loss: 0.1550484001636505\n",
      "Iteration 695/1000, Loss: 0.15494662523269653\n",
      "Iteration 696/1000, Loss: 0.1548449993133545\n",
      "Iteration 697/1000, Loss: 0.1547435224056244\n",
      "Iteration 698/1000, Loss: 0.15464220941066742\n",
      "Iteration 699/1000, Loss: 0.154541015625\n",
      "Iteration 700/1000, Loss: 0.1544400006532669\n",
      "Iteration 701/1000, Loss: 0.15433914959430695\n",
      "Iteration 702/1000, Loss: 0.15423843264579773\n",
      "Iteration 703/1000, Loss: 0.15413786470890045\n",
      "Iteration 704/1000, Loss: 0.1540374606847763\n",
      "Iteration 705/1000, Loss: 0.1539372056722641\n",
      "Iteration 706/1000, Loss: 0.15383706986904144\n",
      "Iteration 707/1000, Loss: 0.1537371128797531\n",
      "Iteration 708/1000, Loss: 0.15363727509975433\n",
      "Iteration 709/1000, Loss: 0.15353763103485107\n",
      "Iteration 710/1000, Loss: 0.15343809127807617\n",
      "Iteration 711/1000, Loss: 0.15333868563175201\n",
      "Iteration 712/1000, Loss: 0.1532394140958786\n",
      "Iteration 713/1000, Loss: 0.1531403362751007\n",
      "Iteration 714/1000, Loss: 0.15304142236709595\n",
      "Iteration 715/1000, Loss: 0.15294264256954193\n",
      "Iteration 716/1000, Loss: 0.15284401178359985\n",
      "Iteration 717/1000, Loss: 0.1527455598115921\n",
      "Iteration 718/1000, Loss: 0.15264728665351868\n",
      "Iteration 719/1000, Loss: 0.15254910290241241\n",
      "Iteration 720/1000, Loss: 0.1524510681629181\n",
      "Iteration 721/1000, Loss: 0.1523531675338745\n",
      "Iteration 722/1000, Loss: 0.15225547552108765\n",
      "Iteration 723/1000, Loss: 0.15215787291526794\n",
      "Iteration 724/1000, Loss: 0.15206044912338257\n",
      "Iteration 725/1000, Loss: 0.15196314454078674\n",
      "Iteration 726/1000, Loss: 0.15186598896980286\n",
      "Iteration 727/1000, Loss: 0.1517690122127533\n",
      "Iteration 728/1000, Loss: 0.15167219936847687\n",
      "Iteration 729/1000, Loss: 0.15157555043697357\n",
      "Iteration 730/1000, Loss: 0.1514790803194046\n",
      "Iteration 731/1000, Loss: 0.15138277411460876\n",
      "Iteration 732/1000, Loss: 0.15128664672374725\n",
      "Iteration 733/1000, Loss: 0.1511906534433365\n",
      "Iteration 734/1000, Loss: 0.15109482407569885\n",
      "Iteration 735/1000, Loss: 0.15099918842315674\n",
      "Iteration 736/1000, Loss: 0.15090365707874298\n",
      "Iteration 737/1000, Loss: 0.15080831944942474\n",
      "Iteration 738/1000, Loss: 0.15071308612823486\n",
      "Iteration 739/1000, Loss: 0.15061800181865692\n",
      "Iteration 740/1000, Loss: 0.1505230814218521\n",
      "Iteration 741/1000, Loss: 0.15042835474014282\n",
      "Iteration 742/1000, Loss: 0.15033376216888428\n",
      "Iteration 743/1000, Loss: 0.15023928880691528\n",
      "Iteration 744/1000, Loss: 0.15014494955539703\n",
      "Iteration 745/1000, Loss: 0.15005074441432953\n",
      "Iteration 746/1000, Loss: 0.14995667338371277\n",
      "Iteration 747/1000, Loss: 0.14986276626586914\n",
      "Iteration 748/1000, Loss: 0.14976894855499268\n",
      "Iteration 749/1000, Loss: 0.14967529475688934\n",
      "Iteration 750/1000, Loss: 0.14958177506923676\n",
      "Iteration 751/1000, Loss: 0.1494884043931961\n",
      "Iteration 752/1000, Loss: 0.14939521253108978\n",
      "Iteration 753/1000, Loss: 0.1493021547794342\n",
      "Iteration 754/1000, Loss: 0.14920924603939056\n",
      "Iteration 755/1000, Loss: 0.14911647140979767\n",
      "Iteration 756/1000, Loss: 0.14902383089065552\n",
      "Iteration 757/1000, Loss: 0.14893129467964172\n",
      "Iteration 758/1000, Loss: 0.14883893728256226\n",
      "Iteration 759/1000, Loss: 0.14874668419361115\n",
      "Iteration 760/1000, Loss: 0.14865458011627197\n",
      "Iteration 761/1000, Loss: 0.14856259524822235\n",
      "Iteration 762/1000, Loss: 0.1484706848859787\n",
      "Iteration 763/1000, Loss: 0.14837893843650818\n",
      "Iteration 764/1000, Loss: 0.14828725159168243\n",
      "Iteration 765/1000, Loss: 0.14819569885730743\n",
      "Iteration 766/1000, Loss: 0.14810426533222198\n",
      "Iteration 767/1000, Loss: 0.14801296591758728\n",
      "Iteration 768/1000, Loss: 0.1479218304157257\n",
      "Iteration 769/1000, Loss: 0.1478308141231537\n",
      "Iteration 770/1000, Loss: 0.1477399468421936\n",
      "Iteration 771/1000, Loss: 0.14764918386936188\n",
      "Iteration 772/1000, Loss: 0.1475585550069809\n",
      "Iteration 773/1000, Loss: 0.14746813476085663\n",
      "Iteration 774/1000, Loss: 0.14737780392169952\n",
      "Iteration 775/1000, Loss: 0.14728759229183197\n",
      "Iteration 776/1000, Loss: 0.14719749987125397\n",
      "Iteration 777/1000, Loss: 0.14710752665996552\n",
      "Iteration 778/1000, Loss: 0.1470176875591278\n",
      "Iteration 779/1000, Loss: 0.14692801237106323\n",
      "Iteration 780/1000, Loss: 0.1468384563922882\n",
      "Iteration 781/1000, Loss: 0.1467490941286087\n",
      "Iteration 782/1000, Loss: 0.14665983617305756\n",
      "Iteration 783/1000, Loss: 0.14657069742679596\n",
      "Iteration 784/1000, Loss: 0.1464817374944687\n",
      "Iteration 785/1000, Loss: 0.14639289677143097\n",
      "Iteration 786/1000, Loss: 0.14630423486232758\n",
      "Iteration 787/1000, Loss: 0.14621566236019135\n",
      "Iteration 788/1000, Loss: 0.14612720906734467\n",
      "Iteration 789/1000, Loss: 0.14603890478610992\n",
      "Iteration 790/1000, Loss: 0.14595073461532593\n",
      "Iteration 791/1000, Loss: 0.1458626687526703\n",
      "Iteration 792/1000, Loss: 0.145774707198143\n",
      "Iteration 793/1000, Loss: 0.14568687975406647\n",
      "Iteration 794/1000, Loss: 0.14559918642044067\n",
      "Iteration 795/1000, Loss: 0.14551162719726562\n",
      "Iteration 796/1000, Loss: 0.14542415738105774\n",
      "Iteration 797/1000, Loss: 0.1453368216753006\n",
      "Iteration 798/1000, Loss: 0.145249605178833\n",
      "Iteration 799/1000, Loss: 0.14516249299049377\n",
      "Iteration 800/1000, Loss: 0.14507552981376648\n",
      "Iteration 801/1000, Loss: 0.14498870074748993\n",
      "Iteration 802/1000, Loss: 0.14490199089050293\n",
      "Iteration 803/1000, Loss: 0.14481540024280548\n",
      "Iteration 804/1000, Loss: 0.1447288990020752\n",
      "Iteration 805/1000, Loss: 0.14464248716831207\n",
      "Iteration 806/1000, Loss: 0.1445561796426773\n",
      "Iteration 807/1000, Loss: 0.14447002112865448\n",
      "Iteration 808/1000, Loss: 0.1443839818239212\n",
      "Iteration 809/1000, Loss: 0.14429804682731628\n",
      "Iteration 810/1000, Loss: 0.1442122459411621\n",
      "Iteration 811/1000, Loss: 0.14412656426429749\n",
      "Iteration 812/1000, Loss: 0.1440410315990448\n",
      "Iteration 813/1000, Loss: 0.14395560324192047\n",
      "Iteration 814/1000, Loss: 0.1438702791929245\n",
      "Iteration 815/1000, Loss: 0.14378505945205688\n",
      "Iteration 816/1000, Loss: 0.14369994401931763\n",
      "Iteration 817/1000, Loss: 0.14361490309238434\n",
      "Iteration 818/1000, Loss: 0.143530011177063\n",
      "Iteration 819/1000, Loss: 0.1434452086687088\n",
      "Iteration 820/1000, Loss: 0.14336058497428894\n",
      "Iteration 821/1000, Loss: 0.14327603578567505\n",
      "Iteration 822/1000, Loss: 0.1431916058063507\n",
      "Iteration 823/1000, Loss: 0.1431073248386383\n",
      "Iteration 824/1000, Loss: 0.14302314817905426\n",
      "Iteration 825/1000, Loss: 0.14293912053108215\n",
      "Iteration 826/1000, Loss: 0.1428551971912384\n",
      "Iteration 827/1000, Loss: 0.1427714079618454\n",
      "Iteration 828/1000, Loss: 0.14268773794174194\n",
      "Iteration 829/1000, Loss: 0.14260418713092804\n",
      "Iteration 830/1000, Loss: 0.1425206959247589\n",
      "Iteration 831/1000, Loss: 0.14243733882904053\n",
      "Iteration 832/1000, Loss: 0.1423540562391281\n",
      "Iteration 833/1000, Loss: 0.14227086305618286\n",
      "Iteration 834/1000, Loss: 0.14218778908252716\n",
      "Iteration 835/1000, Loss: 0.1421048492193222\n",
      "Iteration 836/1000, Loss: 0.1420219987630844\n",
      "Iteration 837/1000, Loss: 0.14193926751613617\n",
      "Iteration 838/1000, Loss: 0.14185665547847748\n",
      "Iteration 839/1000, Loss: 0.14177419245243073\n",
      "Iteration 840/1000, Loss: 0.14169183373451233\n",
      "Iteration 841/1000, Loss: 0.14160962402820587\n",
      "Iteration 842/1000, Loss: 0.141527459025383\n",
      "Iteration 843/1000, Loss: 0.14144542813301086\n",
      "Iteration 844/1000, Loss: 0.1413634866476059\n",
      "Iteration 845/1000, Loss: 0.14128167927265167\n",
      "Iteration 846/1000, Loss: 0.1412000060081482\n",
      "Iteration 847/1000, Loss: 0.14111845195293427\n",
      "Iteration 848/1000, Loss: 0.1410370022058487\n",
      "Iteration 849/1000, Loss: 0.14095565676689148\n",
      "Iteration 850/1000, Loss: 0.140874445438385\n",
      "Iteration 851/1000, Loss: 0.14079329371452332\n",
      "Iteration 852/1000, Loss: 0.14071226119995117\n",
      "Iteration 853/1000, Loss: 0.14063133299350739\n",
      "Iteration 854/1000, Loss: 0.14055052399635315\n",
      "Iteration 855/1000, Loss: 0.14046981930732727\n",
      "Iteration 856/1000, Loss: 0.14038921892642975\n",
      "Iteration 857/1000, Loss: 0.14030875265598297\n",
      "Iteration 858/1000, Loss: 0.14022839069366455\n",
      "Iteration 859/1000, Loss: 0.14014814794063568\n",
      "Iteration 860/1000, Loss: 0.14006799459457397\n",
      "Iteration 861/1000, Loss: 0.13998796045780182\n",
      "Iteration 862/1000, Loss: 0.1399080455303192\n",
      "Iteration 863/1000, Loss: 0.13982822000980377\n",
      "Iteration 864/1000, Loss: 0.13974851369857788\n",
      "Iteration 865/1000, Loss: 0.13966891169548035\n",
      "Iteration 866/1000, Loss: 0.13958941400051117\n",
      "Iteration 867/1000, Loss: 0.13951003551483154\n",
      "Iteration 868/1000, Loss: 0.13943077623844147\n",
      "Iteration 869/1000, Loss: 0.13935162127017975\n",
      "Iteration 870/1000, Loss: 0.13927258551120758\n",
      "Iteration 871/1000, Loss: 0.1391936093568802\n",
      "Iteration 872/1000, Loss: 0.13911475241184235\n",
      "Iteration 873/1000, Loss: 0.13903599977493286\n",
      "Iteration 874/1000, Loss: 0.13895735144615173\n",
      "Iteration 875/1000, Loss: 0.13887880742549896\n",
      "Iteration 876/1000, Loss: 0.13880032300949097\n",
      "Iteration 877/1000, Loss: 0.13872197270393372\n",
      "Iteration 878/1000, Loss: 0.13864368200302124\n",
      "Iteration 879/1000, Loss: 0.13856551051139832\n",
      "Iteration 880/1000, Loss: 0.13848739862442017\n",
      "Iteration 881/1000, Loss: 0.13840940594673157\n",
      "Iteration 882/1000, Loss: 0.13833147287368774\n",
      "Iteration 883/1000, Loss: 0.13825364410877228\n",
      "Iteration 884/1000, Loss: 0.13817590475082397\n",
      "Iteration 885/1000, Loss: 0.13809823989868164\n",
      "Iteration 886/1000, Loss: 0.13802066445350647\n",
      "Iteration 887/1000, Loss: 0.13794316351413727\n",
      "Iteration 888/1000, Loss: 0.13786575198173523\n",
      "Iteration 889/1000, Loss: 0.13778844475746155\n",
      "Iteration 890/1000, Loss: 0.13771125674247742\n",
      "Iteration 891/1000, Loss: 0.13763420283794403\n",
      "Iteration 892/1000, Loss: 0.1375572234392166\n",
      "Iteration 893/1000, Loss: 0.13748033344745636\n",
      "Iteration 894/1000, Loss: 0.13740354776382446\n",
      "Iteration 895/1000, Loss: 0.13732685148715973\n",
      "Iteration 896/1000, Loss: 0.13725028932094574\n",
      "Iteration 897/1000, Loss: 0.13717380166053772\n",
      "Iteration 898/1000, Loss: 0.13709738850593567\n",
      "Iteration 899/1000, Loss: 0.13702109456062317\n",
      "Iteration 900/1000, Loss: 0.13694491982460022\n",
      "Iteration 901/1000, Loss: 0.13686881959438324\n",
      "Iteration 902/1000, Loss: 0.13679277896881104\n",
      "Iteration 903/1000, Loss: 0.136716827750206\n",
      "Iteration 904/1000, Loss: 0.1366409808397293\n",
      "Iteration 905/1000, Loss: 0.13656523823738098\n",
      "Iteration 906/1000, Loss: 0.136489599943161\n",
      "Iteration 907/1000, Loss: 0.1364140510559082\n",
      "Iteration 908/1000, Loss: 0.13633856177330017\n",
      "Iteration 909/1000, Loss: 0.13626320660114288\n",
      "Iteration 910/1000, Loss: 0.13618792593479156\n",
      "Iteration 911/1000, Loss: 0.1361127644777298\n",
      "Iteration 912/1000, Loss: 0.1360376924276352\n",
      "Iteration 913/1000, Loss: 0.13596270978450775\n",
      "Iteration 914/1000, Loss: 0.1358877718448639\n",
      "Iteration 915/1000, Loss: 0.13581296801567078\n",
      "Iteration 916/1000, Loss: 0.13573823869228363\n",
      "Iteration 917/1000, Loss: 0.13566358387470245\n",
      "Iteration 918/1000, Loss: 0.13558904826641083\n",
      "Iteration 919/1000, Loss: 0.13551458716392517\n",
      "Iteration 920/1000, Loss: 0.13544024527072906\n",
      "Iteration 921/1000, Loss: 0.13536597788333893\n",
      "Iteration 922/1000, Loss: 0.13529181480407715\n",
      "Iteration 923/1000, Loss: 0.13521769642829895\n",
      "Iteration 924/1000, Loss: 0.13514366745948792\n",
      "Iteration 925/1000, Loss: 0.13506972789764404\n",
      "Iteration 926/1000, Loss: 0.13499587774276733\n",
      "Iteration 927/1000, Loss: 0.13492213189601898\n",
      "Iteration 928/1000, Loss: 0.1348484754562378\n",
      "Iteration 929/1000, Loss: 0.13477490842342377\n",
      "Iteration 930/1000, Loss: 0.13470140099525452\n",
      "Iteration 931/1000, Loss: 0.13462796807289124\n",
      "Iteration 932/1000, Loss: 0.13455462455749512\n",
      "Iteration 933/1000, Loss: 0.13448140025138855\n",
      "Iteration 934/1000, Loss: 0.13440826535224915\n",
      "Iteration 935/1000, Loss: 0.1343352198600769\n",
      "Iteration 936/1000, Loss: 0.13426224887371063\n",
      "Iteration 937/1000, Loss: 0.13418936729431152\n",
      "Iteration 938/1000, Loss: 0.13411657512187958\n",
      "Iteration 939/1000, Loss: 0.1340438574552536\n",
      "Iteration 940/1000, Loss: 0.13397127389907837\n",
      "Iteration 941/1000, Loss: 0.1338987797498703\n",
      "Iteration 942/1000, Loss: 0.1338263601064682\n",
      "Iteration 943/1000, Loss: 0.13375401496887207\n",
      "Iteration 944/1000, Loss: 0.1336817890405655\n",
      "Iteration 945/1000, Loss: 0.13360965251922607\n",
      "Iteration 946/1000, Loss: 0.13353759050369263\n",
      "Iteration 947/1000, Loss: 0.13346558809280396\n",
      "Iteration 948/1000, Loss: 0.13339367508888245\n",
      "Iteration 949/1000, Loss: 0.1333218514919281\n",
      "Iteration 950/1000, Loss: 0.1332501471042633\n",
      "Iteration 951/1000, Loss: 0.1331784874200821\n",
      "Iteration 952/1000, Loss: 0.13310690224170685\n",
      "Iteration 953/1000, Loss: 0.13303542137145996\n",
      "Iteration 954/1000, Loss: 0.13296401500701904\n",
      "Iteration 955/1000, Loss: 0.1328926980495453\n",
      "Iteration 956/1000, Loss: 0.1328214555978775\n",
      "Iteration 957/1000, Loss: 0.1327502727508545\n",
      "Iteration 958/1000, Loss: 0.13267916440963745\n",
      "Iteration 959/1000, Loss: 0.13260816037654877\n",
      "Iteration 960/1000, Loss: 0.13253721594810486\n",
      "Iteration 961/1000, Loss: 0.1324663758277893\n",
      "Iteration 962/1000, Loss: 0.13239561021327972\n",
      "Iteration 963/1000, Loss: 0.1323249340057373\n",
      "Iteration 964/1000, Loss: 0.13225436210632324\n",
      "Iteration 965/1000, Loss: 0.13218383491039276\n",
      "Iteration 966/1000, Loss: 0.13211341202259064\n",
      "Iteration 967/1000, Loss: 0.13204307854175568\n",
      "Iteration 968/1000, Loss: 0.13197283446788788\n",
      "Iteration 969/1000, Loss: 0.13190264999866486\n",
      "Iteration 970/1000, Loss: 0.1318325400352478\n",
      "Iteration 971/1000, Loss: 0.1317625343799591\n",
      "Iteration 972/1000, Loss: 0.13169260323047638\n",
      "Iteration 973/1000, Loss: 0.1316227912902832\n",
      "Iteration 974/1000, Loss: 0.1315530240535736\n",
      "Iteration 975/1000, Loss: 0.13148334622383118\n",
      "Iteration 976/1000, Loss: 0.1314137876033783\n",
      "Iteration 977/1000, Loss: 0.1313442885875702\n",
      "Iteration 978/1000, Loss: 0.13127486407756805\n",
      "Iteration 979/1000, Loss: 0.13120552897453308\n",
      "Iteration 980/1000, Loss: 0.13113626837730408\n",
      "Iteration 981/1000, Loss: 0.13106708228588104\n",
      "Iteration 982/1000, Loss: 0.13099798560142517\n",
      "Iteration 983/1000, Loss: 0.13092894852161407\n",
      "Iteration 984/1000, Loss: 0.13086001574993134\n",
      "Iteration 985/1000, Loss: 0.13079115748405457\n",
      "Iteration 986/1000, Loss: 0.13072237372398376\n",
      "Iteration 987/1000, Loss: 0.13065367937088013\n",
      "Iteration 988/1000, Loss: 0.13058504462242126\n",
      "Iteration 989/1000, Loss: 0.13051646947860718\n",
      "Iteration 990/1000, Loss: 0.13044798374176025\n",
      "Iteration 991/1000, Loss: 0.1303795576095581\n",
      "Iteration 992/1000, Loss: 0.13031119108200073\n",
      "Iteration 993/1000, Loss: 0.13024286925792694\n",
      "Iteration 994/1000, Loss: 0.13017460703849792\n",
      "Iteration 995/1000, Loss: 0.13010643422603607\n",
      "Iteration 996/1000, Loss: 0.1300383359193802\n",
      "Iteration 997/1000, Loss: 0.12997031211853027\n",
      "Iteration 998/1000, Loss: 0.12990236282348633\n",
      "Iteration 999/1000, Loss: 0.12983450293540955\n",
      "Iteration 1000/1000, Loss: 0.12976673245429993\n",
      "Pruning Step 4\n",
      "Iteration 1/1000, Loss: 1.5285005569458008\n",
      "Iteration 2/1000, Loss: 1.4174481630325317\n",
      "Iteration 3/1000, Loss: 1.3304760456085205\n",
      "Iteration 4/1000, Loss: 1.2532356977462769\n",
      "Iteration 5/1000, Loss: 1.1835172176361084\n",
      "Iteration 6/1000, Loss: 1.120300054550171\n",
      "Iteration 7/1000, Loss: 1.0628812313079834\n",
      "Iteration 8/1000, Loss: 1.0106335878372192\n",
      "Iteration 9/1000, Loss: 0.9630826115608215\n",
      "Iteration 10/1000, Loss: 0.9197725653648376\n",
      "Iteration 11/1000, Loss: 0.8803039193153381\n",
      "Iteration 12/1000, Loss: 0.8442891836166382\n",
      "Iteration 13/1000, Loss: 0.8113954663276672\n",
      "Iteration 14/1000, Loss: 0.7812883257865906\n",
      "Iteration 15/1000, Loss: 0.7536875009536743\n",
      "Iteration 16/1000, Loss: 0.7283357977867126\n",
      "Iteration 17/1000, Loss: 0.7050084471702576\n",
      "Iteration 18/1000, Loss: 0.6835026144981384\n",
      "Iteration 19/1000, Loss: 0.6636362075805664\n",
      "Iteration 20/1000, Loss: 0.6452423334121704\n",
      "Iteration 21/1000, Loss: 0.628182053565979\n",
      "Iteration 22/1000, Loss: 0.6123256683349609\n",
      "Iteration 23/1000, Loss: 0.5975574254989624\n",
      "Iteration 24/1000, Loss: 0.5837807655334473\n",
      "Iteration 25/1000, Loss: 0.5709024667739868\n",
      "Iteration 26/1000, Loss: 0.5588425397872925\n",
      "Iteration 27/1000, Loss: 0.5475282669067383\n",
      "Iteration 28/1000, Loss: 0.5368981957435608\n",
      "Iteration 29/1000, Loss: 0.5268962979316711\n",
      "Iteration 30/1000, Loss: 0.5174697637557983\n",
      "Iteration 31/1000, Loss: 0.5085722804069519\n",
      "Iteration 32/1000, Loss: 0.5001600384712219\n",
      "Iteration 33/1000, Loss: 0.492196649312973\n",
      "Iteration 34/1000, Loss: 0.4846472144126892\n",
      "Iteration 35/1000, Loss: 0.4774806499481201\n",
      "Iteration 36/1000, Loss: 0.4706699848175049\n",
      "Iteration 37/1000, Loss: 0.46418851613998413\n",
      "Iteration 38/1000, Loss: 0.4580135941505432\n",
      "Iteration 39/1000, Loss: 0.45212310552597046\n",
      "Iteration 40/1000, Loss: 0.4464966058731079\n",
      "Iteration 41/1000, Loss: 0.4411179721355438\n",
      "Iteration 42/1000, Loss: 0.4359695017337799\n",
      "Iteration 43/1000, Loss: 0.431037575006485\n",
      "Iteration 44/1000, Loss: 0.4263080954551697\n",
      "Iteration 45/1000, Loss: 0.4217692017555237\n",
      "Iteration 46/1000, Loss: 0.41740840673446655\n",
      "Iteration 47/1000, Loss: 0.4132159650325775\n",
      "Iteration 48/1000, Loss: 0.40918204188346863\n",
      "Iteration 49/1000, Loss: 0.4052981734275818\n",
      "Iteration 50/1000, Loss: 0.4015551209449768\n",
      "Iteration 51/1000, Loss: 0.39794522523880005\n",
      "Iteration 52/1000, Loss: 0.39446133375167847\n",
      "Iteration 53/1000, Loss: 0.39109739661216736\n",
      "Iteration 54/1000, Loss: 0.38784679770469666\n",
      "Iteration 55/1000, Loss: 0.3847030699253082\n",
      "Iteration 56/1000, Loss: 0.3816612660884857\n",
      "Iteration 57/1000, Loss: 0.37871575355529785\n",
      "Iteration 58/1000, Loss: 0.3758617043495178\n",
      "Iteration 59/1000, Loss: 0.3730941414833069\n",
      "Iteration 60/1000, Loss: 0.37040942907333374\n",
      "Iteration 61/1000, Loss: 0.36780357360839844\n",
      "Iteration 62/1000, Loss: 0.36527323722839355\n",
      "Iteration 63/1000, Loss: 0.36281469464302063\n",
      "Iteration 64/1000, Loss: 0.36042436957359314\n",
      "Iteration 65/1000, Loss: 0.35809987783432007\n",
      "Iteration 66/1000, Loss: 0.35583820939064026\n",
      "Iteration 67/1000, Loss: 0.35363683104515076\n",
      "Iteration 68/1000, Loss: 0.3514932692050934\n",
      "Iteration 69/1000, Loss: 0.3494051992893219\n",
      "Iteration 70/1000, Loss: 0.3473699688911438\n",
      "Iteration 71/1000, Loss: 0.3453851640224457\n",
      "Iteration 72/1000, Loss: 0.34344908595085144\n",
      "Iteration 73/1000, Loss: 0.3415599465370178\n",
      "Iteration 74/1000, Loss: 0.3397158980369568\n",
      "Iteration 75/1000, Loss: 0.3379151225090027\n",
      "Iteration 76/1000, Loss: 0.3361559510231018\n",
      "Iteration 77/1000, Loss: 0.33443644642829895\n",
      "Iteration 78/1000, Loss: 0.3327554166316986\n",
      "Iteration 79/1000, Loss: 0.3311115801334381\n",
      "Iteration 80/1000, Loss: 0.3295036852359772\n",
      "Iteration 81/1000, Loss: 0.32793018221855164\n",
      "Iteration 82/1000, Loss: 0.3263896405696869\n",
      "Iteration 83/1000, Loss: 0.32488080859184265\n",
      "Iteration 84/1000, Loss: 0.32340261340141296\n",
      "Iteration 85/1000, Loss: 0.32195404171943665\n",
      "Iteration 86/1000, Loss: 0.3205341696739197\n",
      "Iteration 87/1000, Loss: 0.31914180517196655\n",
      "Iteration 88/1000, Loss: 0.31777605414390564\n",
      "Iteration 89/1000, Loss: 0.31643617153167725\n",
      "Iteration 90/1000, Loss: 0.31512129306793213\n",
      "Iteration 91/1000, Loss: 0.31383103132247925\n",
      "Iteration 92/1000, Loss: 0.3125644624233246\n",
      "Iteration 93/1000, Loss: 0.31132084131240845\n",
      "Iteration 94/1000, Loss: 0.31009936332702637\n",
      "Iteration 95/1000, Loss: 0.3088993430137634\n",
      "Iteration 96/1000, Loss: 0.3077203035354614\n",
      "Iteration 97/1000, Loss: 0.30656126141548157\n",
      "Iteration 98/1000, Loss: 0.3054216504096985\n",
      "Iteration 99/1000, Loss: 0.3043011724948883\n",
      "Iteration 100/1000, Loss: 0.30319902300834656\n",
      "Iteration 101/1000, Loss: 0.302114874124527\n",
      "Iteration 102/1000, Loss: 0.30104830861091614\n",
      "Iteration 103/1000, Loss: 0.29999876022338867\n",
      "Iteration 104/1000, Loss: 0.2989656925201416\n",
      "Iteration 105/1000, Loss: 0.2979484498500824\n",
      "Iteration 106/1000, Loss: 0.29694664478302\n",
      "Iteration 107/1000, Loss: 0.2959601581096649\n",
      "Iteration 108/1000, Loss: 0.29498860239982605\n",
      "Iteration 109/1000, Loss: 0.29403144121170044\n",
      "Iteration 110/1000, Loss: 0.29308828711509705\n",
      "Iteration 111/1000, Loss: 0.2921586334705353\n",
      "Iteration 112/1000, Loss: 0.2912420630455017\n",
      "Iteration 113/1000, Loss: 0.29033827781677246\n",
      "Iteration 114/1000, Loss: 0.28944700956344604\n",
      "Iteration 115/1000, Loss: 0.28856807947158813\n",
      "Iteration 116/1000, Loss: 0.28770098090171814\n",
      "Iteration 117/1000, Loss: 0.28684571385383606\n",
      "Iteration 118/1000, Loss: 0.2860015630722046\n",
      "Iteration 119/1000, Loss: 0.28516849875450134\n",
      "Iteration 120/1000, Loss: 0.2843462824821472\n",
      "Iteration 121/1000, Loss: 0.28353455662727356\n",
      "Iteration 122/1000, Loss: 0.2827332019805908\n",
      "Iteration 123/1000, Loss: 0.2819420099258423\n",
      "Iteration 124/1000, Loss: 0.2811608910560608\n",
      "Iteration 125/1000, Loss: 0.2803891897201538\n",
      "Iteration 126/1000, Loss: 0.27962663769721985\n",
      "Iteration 127/1000, Loss: 0.2788732051849365\n",
      "Iteration 128/1000, Loss: 0.2781285345554352\n",
      "Iteration 129/1000, Loss: 0.27739277482032776\n",
      "Iteration 130/1000, Loss: 0.276665598154068\n",
      "Iteration 131/1000, Loss: 0.27594679594039917\n",
      "Iteration 132/1000, Loss: 0.27523618936538696\n",
      "Iteration 133/1000, Loss: 0.27453356981277466\n",
      "Iteration 134/1000, Loss: 0.27383893728256226\n",
      "Iteration 135/1000, Loss: 0.2731521427631378\n",
      "Iteration 136/1000, Loss: 0.27247318625450134\n",
      "Iteration 137/1000, Loss: 0.27180173993110657\n",
      "Iteration 138/1000, Loss: 0.27113717794418335\n",
      "Iteration 139/1000, Loss: 0.27047964930534363\n",
      "Iteration 140/1000, Loss: 0.26982903480529785\n",
      "Iteration 141/1000, Loss: 0.26918530464172363\n",
      "Iteration 142/1000, Loss: 0.2685483396053314\n",
      "Iteration 143/1000, Loss: 0.2679179906845093\n",
      "Iteration 144/1000, Loss: 0.2672940790653229\n",
      "Iteration 145/1000, Loss: 0.26667651534080505\n",
      "Iteration 146/1000, Loss: 0.2660653293132782\n",
      "Iteration 147/1000, Loss: 0.26546019315719604\n",
      "Iteration 148/1000, Loss: 0.264861136674881\n",
      "Iteration 149/1000, Loss: 0.2642677128314972\n",
      "Iteration 150/1000, Loss: 0.26367998123168945\n",
      "Iteration 151/1000, Loss: 0.26309770345687866\n",
      "Iteration 152/1000, Loss: 0.2625209391117096\n",
      "Iteration 153/1000, Loss: 0.261949747800827\n",
      "Iteration 154/1000, Loss: 0.26138371229171753\n",
      "Iteration 155/1000, Loss: 0.26082271337509155\n",
      "Iteration 156/1000, Loss: 0.26026663184165955\n",
      "Iteration 157/1000, Loss: 0.2597154974937439\n",
      "Iteration 158/1000, Loss: 0.25916919112205505\n",
      "Iteration 159/1000, Loss: 0.25862768292427063\n",
      "Iteration 160/1000, Loss: 0.25809088349342346\n",
      "Iteration 161/1000, Loss: 0.2575587332248688\n",
      "Iteration 162/1000, Loss: 0.25703126192092896\n",
      "Iteration 163/1000, Loss: 0.2565082609653473\n",
      "Iteration 164/1000, Loss: 0.2559896409511566\n",
      "Iteration 165/1000, Loss: 0.25547537207603455\n",
      "Iteration 166/1000, Loss: 0.2549656331539154\n",
      "Iteration 167/1000, Loss: 0.25446006655693054\n",
      "Iteration 168/1000, Loss: 0.2539586126804352\n",
      "Iteration 169/1000, Loss: 0.2534613311290741\n",
      "Iteration 170/1000, Loss: 0.25296810269355774\n",
      "Iteration 171/1000, Loss: 0.25247886776924133\n",
      "Iteration 172/1000, Loss: 0.25199347734451294\n",
      "Iteration 173/1000, Loss: 0.2515120208263397\n",
      "Iteration 174/1000, Loss: 0.2510342299938202\n",
      "Iteration 175/1000, Loss: 0.25056004524230957\n",
      "Iteration 176/1000, Loss: 0.25008952617645264\n",
      "Iteration 177/1000, Loss: 0.24962250888347626\n",
      "Iteration 178/1000, Loss: 0.24915897846221924\n",
      "Iteration 179/1000, Loss: 0.24869892001152039\n",
      "Iteration 180/1000, Loss: 0.24824218451976776\n",
      "Iteration 181/1000, Loss: 0.24778872728347778\n",
      "Iteration 182/1000, Loss: 0.24733857810497284\n",
      "Iteration 183/1000, Loss: 0.2468918263912201\n",
      "Iteration 184/1000, Loss: 0.2464483380317688\n",
      "Iteration 185/1000, Loss: 0.2460080087184906\n",
      "Iteration 186/1000, Loss: 0.24557089805603027\n",
      "Iteration 187/1000, Loss: 0.2451368123292923\n",
      "Iteration 188/1000, Loss: 0.24470576643943787\n",
      "Iteration 189/1000, Loss: 0.24427755177021027\n",
      "Iteration 190/1000, Loss: 0.2438523769378662\n",
      "Iteration 191/1000, Loss: 0.24343018233776093\n",
      "Iteration 192/1000, Loss: 0.24301090836524963\n",
      "Iteration 193/1000, Loss: 0.24259427189826965\n",
      "Iteration 194/1000, Loss: 0.24218016862869263\n",
      "Iteration 195/1000, Loss: 0.24176883697509766\n",
      "Iteration 196/1000, Loss: 0.2413601577281952\n",
      "Iteration 197/1000, Loss: 0.2409542351961136\n",
      "Iteration 198/1000, Loss: 0.24055087566375732\n",
      "Iteration 199/1000, Loss: 0.24015012383460999\n",
      "Iteration 200/1000, Loss: 0.23975209891796112\n",
      "Iteration 201/1000, Loss: 0.23935677111148834\n",
      "Iteration 202/1000, Loss: 0.23896396160125732\n",
      "Iteration 203/1000, Loss: 0.23857362568378448\n",
      "Iteration 204/1000, Loss: 0.23818570375442505\n",
      "Iteration 205/1000, Loss: 0.2378002554178238\n",
      "Iteration 206/1000, Loss: 0.2374172806739807\n",
      "Iteration 207/1000, Loss: 0.23703661561012268\n",
      "Iteration 208/1000, Loss: 0.23665837943553925\n",
      "Iteration 209/1000, Loss: 0.23628248274326324\n",
      "Iteration 210/1000, Loss: 0.23590892553329468\n",
      "Iteration 211/1000, Loss: 0.23553763329982758\n",
      "Iteration 212/1000, Loss: 0.23516857624053955\n",
      "Iteration 213/1000, Loss: 0.23480167984962463\n",
      "Iteration 214/1000, Loss: 0.23443683981895447\n",
      "Iteration 215/1000, Loss: 0.23407405614852905\n",
      "Iteration 216/1000, Loss: 0.23371337354183197\n",
      "Iteration 217/1000, Loss: 0.23335479199886322\n",
      "Iteration 218/1000, Loss: 0.2329983115196228\n",
      "Iteration 219/1000, Loss: 0.23264388740062714\n",
      "Iteration 220/1000, Loss: 0.2322915494441986\n",
      "Iteration 221/1000, Loss: 0.23194120824337006\n",
      "Iteration 222/1000, Loss: 0.23159277439117432\n",
      "Iteration 223/1000, Loss: 0.2312462031841278\n",
      "Iteration 224/1000, Loss: 0.23090146481990814\n",
      "Iteration 225/1000, Loss: 0.23055864870548248\n",
      "Iteration 226/1000, Loss: 0.23021773993968964\n",
      "Iteration 227/1000, Loss: 0.22987869381904602\n",
      "Iteration 228/1000, Loss: 0.22954146564006805\n",
      "Iteration 229/1000, Loss: 0.22920608520507812\n",
      "Iteration 230/1000, Loss: 0.22887252271175385\n",
      "Iteration 231/1000, Loss: 0.22854071855545044\n",
      "Iteration 232/1000, Loss: 0.22821061313152313\n",
      "Iteration 233/1000, Loss: 0.2278822958469391\n",
      "Iteration 234/1000, Loss: 0.22755569219589233\n",
      "Iteration 235/1000, Loss: 0.22723078727722168\n",
      "Iteration 236/1000, Loss: 0.22690756618976593\n",
      "Iteration 237/1000, Loss: 0.2265859842300415\n",
      "Iteration 238/1000, Loss: 0.2262660413980484\n",
      "Iteration 239/1000, Loss: 0.22594785690307617\n",
      "Iteration 240/1000, Loss: 0.22563128173351288\n",
      "Iteration 241/1000, Loss: 0.22531640529632568\n",
      "Iteration 242/1000, Loss: 0.22500303387641907\n",
      "Iteration 243/1000, Loss: 0.22469118237495422\n",
      "Iteration 244/1000, Loss: 0.22438089549541473\n",
      "Iteration 245/1000, Loss: 0.22407208383083344\n",
      "Iteration 246/1000, Loss: 0.22376474738121033\n",
      "Iteration 247/1000, Loss: 0.22345885634422302\n",
      "Iteration 248/1000, Loss: 0.2231544852256775\n",
      "Iteration 249/1000, Loss: 0.22285158932209015\n",
      "Iteration 250/1000, Loss: 0.22255006432533264\n",
      "Iteration 251/1000, Loss: 0.22224989533424377\n",
      "Iteration 252/1000, Loss: 0.2219511866569519\n",
      "Iteration 253/1000, Loss: 0.2216539978981018\n",
      "Iteration 254/1000, Loss: 0.22135820984840393\n",
      "Iteration 255/1000, Loss: 0.2210637629032135\n",
      "Iteration 256/1000, Loss: 0.2207706719636917\n",
      "Iteration 257/1000, Loss: 0.22047889232635498\n",
      "Iteration 258/1000, Loss: 0.22018839418888092\n",
      "Iteration 259/1000, Loss: 0.21989914774894714\n",
      "Iteration 260/1000, Loss: 0.2196113020181656\n",
      "Iteration 261/1000, Loss: 0.21932490170001984\n",
      "Iteration 262/1000, Loss: 0.21903979778289795\n",
      "Iteration 263/1000, Loss: 0.21875596046447754\n",
      "Iteration 264/1000, Loss: 0.2184733748435974\n",
      "Iteration 265/1000, Loss: 0.218191996216774\n",
      "Iteration 266/1000, Loss: 0.21791177988052368\n",
      "Iteration 267/1000, Loss: 0.21763284504413605\n",
      "Iteration 268/1000, Loss: 0.21735507249832153\n",
      "Iteration 269/1000, Loss: 0.21707847714424133\n",
      "Iteration 270/1000, Loss: 0.21680299937725067\n",
      "Iteration 271/1000, Loss: 0.21652862429618835\n",
      "Iteration 272/1000, Loss: 0.21625539660453796\n",
      "Iteration 273/1000, Loss: 0.21598342061042786\n",
      "Iteration 274/1000, Loss: 0.21571259200572968\n",
      "Iteration 275/1000, Loss: 0.21544285118579865\n",
      "Iteration 276/1000, Loss: 0.21517431735992432\n",
      "Iteration 277/1000, Loss: 0.21490693092346191\n",
      "Iteration 278/1000, Loss: 0.21464072167873383\n",
      "Iteration 279/1000, Loss: 0.21437548100948334\n",
      "Iteration 280/1000, Loss: 0.2141112983226776\n",
      "Iteration 281/1000, Loss: 0.21384814381599426\n",
      "Iteration 282/1000, Loss: 0.21358606219291687\n",
      "Iteration 283/1000, Loss: 0.21332502365112305\n",
      "Iteration 284/1000, Loss: 0.2130650132894516\n",
      "Iteration 285/1000, Loss: 0.21280601620674133\n",
      "Iteration 286/1000, Loss: 0.21254821121692657\n",
      "Iteration 287/1000, Loss: 0.21229155361652374\n",
      "Iteration 288/1000, Loss: 0.21203599870204926\n",
      "Iteration 289/1000, Loss: 0.21178151667118073\n",
      "Iteration 290/1000, Loss: 0.21152806282043457\n",
      "Iteration 291/1000, Loss: 0.2112755924463272\n",
      "Iteration 292/1000, Loss: 0.2110241800546646\n",
      "Iteration 293/1000, Loss: 0.2107737958431244\n",
      "Iteration 294/1000, Loss: 0.21052446961402893\n",
      "Iteration 295/1000, Loss: 0.2102760672569275\n",
      "Iteration 296/1000, Loss: 0.21002860367298126\n",
      "Iteration 297/1000, Loss: 0.20978204905986786\n",
      "Iteration 298/1000, Loss: 0.20953649282455444\n",
      "Iteration 299/1000, Loss: 0.20929181575775146\n",
      "Iteration 300/1000, Loss: 0.20904815196990967\n",
      "Iteration 301/1000, Loss: 0.20880544185638428\n",
      "Iteration 302/1000, Loss: 0.20856371521949768\n",
      "Iteration 303/1000, Loss: 0.20832288265228271\n",
      "Iteration 304/1000, Loss: 0.20808298885822296\n",
      "Iteration 305/1000, Loss: 0.20784394443035126\n",
      "Iteration 306/1000, Loss: 0.20760579407215118\n",
      "Iteration 307/1000, Loss: 0.20736850798130035\n",
      "Iteration 308/1000, Loss: 0.20713220536708832\n",
      "Iteration 309/1000, Loss: 0.2068968564271927\n",
      "Iteration 310/1000, Loss: 0.2066623717546463\n",
      "Iteration 311/1000, Loss: 0.20642870664596558\n",
      "Iteration 312/1000, Loss: 0.2061958611011505\n",
      "Iteration 313/1000, Loss: 0.20596393942832947\n",
      "Iteration 314/1000, Loss: 0.20573285222053528\n",
      "Iteration 315/1000, Loss: 0.20550262928009033\n",
      "Iteration 316/1000, Loss: 0.20527325570583344\n",
      "Iteration 317/1000, Loss: 0.20504476130008698\n",
      "Iteration 318/1000, Loss: 0.2048170417547226\n",
      "Iteration 319/1000, Loss: 0.20459017157554626\n",
      "Iteration 320/1000, Loss: 0.20436406135559082\n",
      "Iteration 321/1000, Loss: 0.20413874089717865\n",
      "Iteration 322/1000, Loss: 0.20391422510147095\n",
      "Iteration 323/1000, Loss: 0.20369049906730652\n",
      "Iteration 324/1000, Loss: 0.20346757769584656\n",
      "Iteration 325/1000, Loss: 0.20324546098709106\n",
      "Iteration 326/1000, Loss: 0.20302411913871765\n",
      "Iteration 327/1000, Loss: 0.2028036117553711\n",
      "Iteration 328/1000, Loss: 0.20258384943008423\n",
      "Iteration 329/1000, Loss: 0.20236483216285706\n",
      "Iteration 330/1000, Loss: 0.20214664936065674\n",
      "Iteration 331/1000, Loss: 0.2019292414188385\n",
      "Iteration 332/1000, Loss: 0.20171259343624115\n",
      "Iteration 333/1000, Loss: 0.2014966458082199\n",
      "Iteration 334/1000, Loss: 0.20128139853477478\n",
      "Iteration 335/1000, Loss: 0.20106688141822815\n",
      "Iteration 336/1000, Loss: 0.2008531093597412\n",
      "Iteration 337/1000, Loss: 0.20064005255699158\n",
      "Iteration 338/1000, Loss: 0.20042766630649567\n",
      "Iteration 339/1000, Loss: 0.20021601021289825\n",
      "Iteration 340/1000, Loss: 0.20000503957271576\n",
      "Iteration 341/1000, Loss: 0.19979485869407654\n",
      "Iteration 342/1000, Loss: 0.19958536326885223\n",
      "Iteration 343/1000, Loss: 0.19937662780284882\n",
      "Iteration 344/1000, Loss: 0.19916868209838867\n",
      "Iteration 345/1000, Loss: 0.19896143674850464\n",
      "Iteration 346/1000, Loss: 0.19875487685203552\n",
      "Iteration 347/1000, Loss: 0.19854898750782013\n",
      "Iteration 348/1000, Loss: 0.19834373891353607\n",
      "Iteration 349/1000, Loss: 0.19813916087150574\n",
      "Iteration 350/1000, Loss: 0.19793519377708435\n",
      "Iteration 351/1000, Loss: 0.19773180782794952\n",
      "Iteration 352/1000, Loss: 0.1975291222333908\n",
      "Iteration 353/1000, Loss: 0.19732704758644104\n",
      "Iteration 354/1000, Loss: 0.1971256136894226\n",
      "Iteration 355/1000, Loss: 0.19692477583885193\n",
      "Iteration 356/1000, Loss: 0.1967245489358902\n",
      "Iteration 357/1000, Loss: 0.1965249627828598\n",
      "Iteration 358/1000, Loss: 0.19632604718208313\n",
      "Iteration 359/1000, Loss: 0.19612786173820496\n",
      "Iteration 360/1000, Loss: 0.1959303766489029\n",
      "Iteration 361/1000, Loss: 0.1957334578037262\n",
      "Iteration 362/1000, Loss: 0.19553719460964203\n",
      "Iteration 363/1000, Loss: 0.1953415423631668\n",
      "Iteration 364/1000, Loss: 0.19514650106430054\n",
      "Iteration 365/1000, Loss: 0.1949520856142044\n",
      "Iteration 366/1000, Loss: 0.19475829601287842\n",
      "Iteration 367/1000, Loss: 0.19456513226032257\n",
      "Iteration 368/1000, Loss: 0.19437262415885925\n",
      "Iteration 369/1000, Loss: 0.19418075680732727\n",
      "Iteration 370/1000, Loss: 0.19398947060108185\n",
      "Iteration 371/1000, Loss: 0.19379879534244537\n",
      "Iteration 372/1000, Loss: 0.1936085969209671\n",
      "Iteration 373/1000, Loss: 0.19341899454593658\n",
      "Iteration 374/1000, Loss: 0.19322989881038666\n",
      "Iteration 375/1000, Loss: 0.19304129481315613\n",
      "Iteration 376/1000, Loss: 0.19285325706005096\n",
      "Iteration 377/1000, Loss: 0.19266581535339355\n",
      "Iteration 378/1000, Loss: 0.1924789547920227\n",
      "Iteration 379/1000, Loss: 0.19229264557361603\n",
      "Iteration 380/1000, Loss: 0.19210684299468994\n",
      "Iteration 381/1000, Loss: 0.19192157685756683\n",
      "Iteration 382/1000, Loss: 0.19173690676689148\n",
      "Iteration 383/1000, Loss: 0.1915527880191803\n",
      "Iteration 384/1000, Loss: 0.1913692206144333\n",
      "Iteration 385/1000, Loss: 0.19118623435497284\n",
      "Iteration 386/1000, Loss: 0.1910036951303482\n",
      "Iteration 387/1000, Loss: 0.19082172214984894\n",
      "Iteration 388/1000, Loss: 0.19064030051231384\n",
      "Iteration 389/1000, Loss: 0.19045932590961456\n",
      "Iteration 390/1000, Loss: 0.19027890264987946\n",
      "Iteration 391/1000, Loss: 0.19009898602962494\n",
      "Iteration 392/1000, Loss: 0.18991965055465698\n",
      "Iteration 393/1000, Loss: 0.18974082171916962\n",
      "Iteration 394/1000, Loss: 0.18956248462200165\n",
      "Iteration 395/1000, Loss: 0.18938469886779785\n",
      "Iteration 396/1000, Loss: 0.18920749425888062\n",
      "Iteration 397/1000, Loss: 0.18903082609176636\n",
      "Iteration 398/1000, Loss: 0.18885460495948792\n",
      "Iteration 399/1000, Loss: 0.18867893517017365\n",
      "Iteration 400/1000, Loss: 0.18850374221801758\n",
      "Iteration 401/1000, Loss: 0.1883290708065033\n",
      "Iteration 402/1000, Loss: 0.18815487623214722\n",
      "Iteration 403/1000, Loss: 0.18798115849494934\n",
      "Iteration 404/1000, Loss: 0.18780794739723206\n",
      "Iteration 405/1000, Loss: 0.18763522803783417\n",
      "Iteration 406/1000, Loss: 0.18746298551559448\n",
      "Iteration 407/1000, Loss: 0.18729117512702942\n",
      "Iteration 408/1000, Loss: 0.18711987137794495\n",
      "Iteration 409/1000, Loss: 0.18694905936717987\n",
      "Iteration 410/1000, Loss: 0.1867786943912506\n",
      "Iteration 411/1000, Loss: 0.18660882115364075\n",
      "Iteration 412/1000, Loss: 0.1864394247531891\n",
      "Iteration 413/1000, Loss: 0.18627052009105682\n",
      "Iteration 414/1000, Loss: 0.18610212206840515\n",
      "Iteration 415/1000, Loss: 0.18593421578407288\n",
      "Iteration 416/1000, Loss: 0.18576672673225403\n",
      "Iteration 417/1000, Loss: 0.18559974431991577\n",
      "Iteration 418/1000, Loss: 0.18543320894241333\n",
      "Iteration 419/1000, Loss: 0.1852671504020691\n",
      "Iteration 420/1000, Loss: 0.18510158360004425\n",
      "Iteration 421/1000, Loss: 0.1849364936351776\n",
      "Iteration 422/1000, Loss: 0.18477189540863037\n",
      "Iteration 423/1000, Loss: 0.18460772931575775\n",
      "Iteration 424/1000, Loss: 0.18444399535655975\n",
      "Iteration 425/1000, Loss: 0.18428069353103638\n",
      "Iteration 426/1000, Loss: 0.18411782383918762\n",
      "Iteration 427/1000, Loss: 0.1839553564786911\n",
      "Iteration 428/1000, Loss: 0.1837933212518692\n",
      "Iteration 429/1000, Loss: 0.18363173305988312\n",
      "Iteration 430/1000, Loss: 0.18347050249576569\n",
      "Iteration 431/1000, Loss: 0.18330970406532288\n",
      "Iteration 432/1000, Loss: 0.1831493228673935\n",
      "Iteration 433/1000, Loss: 0.18298937380313873\n",
      "Iteration 434/1000, Loss: 0.182829812169075\n",
      "Iteration 435/1000, Loss: 0.1826707124710083\n",
      "Iteration 436/1000, Loss: 0.18251201510429382\n",
      "Iteration 437/1000, Loss: 0.18235380947589874\n",
      "Iteration 438/1000, Loss: 0.18219605088233948\n",
      "Iteration 439/1000, Loss: 0.18203866481781006\n",
      "Iteration 440/1000, Loss: 0.18188172578811646\n",
      "Iteration 441/1000, Loss: 0.1817251294851303\n",
      "Iteration 442/1000, Loss: 0.18156887590885162\n",
      "Iteration 443/1000, Loss: 0.18141305446624756\n",
      "Iteration 444/1000, Loss: 0.18125760555267334\n",
      "Iteration 445/1000, Loss: 0.18110260367393494\n",
      "Iteration 446/1000, Loss: 0.18094803392887115\n",
      "Iteration 447/1000, Loss: 0.18079374730587006\n",
      "Iteration 448/1000, Loss: 0.1806398630142212\n",
      "Iteration 449/1000, Loss: 0.18048639595508575\n",
      "Iteration 450/1000, Loss: 0.18033330142498016\n",
      "Iteration 451/1000, Loss: 0.1801805943250656\n",
      "Iteration 452/1000, Loss: 0.1800282597541809\n",
      "Iteration 453/1000, Loss: 0.17987632751464844\n",
      "Iteration 454/1000, Loss: 0.17972472310066223\n",
      "Iteration 455/1000, Loss: 0.17957347631454468\n",
      "Iteration 456/1000, Loss: 0.17942261695861816\n",
      "Iteration 457/1000, Loss: 0.1792721450328827\n",
      "Iteration 458/1000, Loss: 0.17912200093269348\n",
      "Iteration 459/1000, Loss: 0.17897216975688934\n",
      "Iteration 460/1000, Loss: 0.17882271111011505\n",
      "Iteration 461/1000, Loss: 0.17867356538772583\n",
      "Iteration 462/1000, Loss: 0.17852480709552765\n",
      "Iteration 463/1000, Loss: 0.17837640643119812\n",
      "Iteration 464/1000, Loss: 0.17822834849357605\n",
      "Iteration 465/1000, Loss: 0.17808066308498383\n",
      "Iteration 466/1000, Loss: 0.17793333530426025\n",
      "Iteration 467/1000, Loss: 0.17778638005256653\n",
      "Iteration 468/1000, Loss: 0.17763981223106384\n",
      "Iteration 469/1000, Loss: 0.17749369144439697\n",
      "Iteration 470/1000, Loss: 0.17734791338443756\n",
      "Iteration 471/1000, Loss: 0.177202507853508\n",
      "Iteration 472/1000, Loss: 0.1770574450492859\n",
      "Iteration 473/1000, Loss: 0.17691275477409363\n",
      "Iteration 474/1000, Loss: 0.17676842212677002\n",
      "Iteration 475/1000, Loss: 0.17662444710731506\n",
      "Iteration 476/1000, Loss: 0.17648084461688995\n",
      "Iteration 477/1000, Loss: 0.1763375848531723\n",
      "Iteration 478/1000, Loss: 0.17619463801383972\n",
      "Iteration 479/1000, Loss: 0.1760520040988922\n",
      "Iteration 480/1000, Loss: 0.17590969800949097\n",
      "Iteration 481/1000, Loss: 0.17576773464679718\n",
      "Iteration 482/1000, Loss: 0.17562617361545563\n",
      "Iteration 483/1000, Loss: 0.17548489570617676\n",
      "Iteration 484/1000, Loss: 0.17534397542476654\n",
      "Iteration 485/1000, Loss: 0.1752033531665802\n",
      "Iteration 486/1000, Loss: 0.17506298422813416\n",
      "Iteration 487/1000, Loss: 0.1749228984117508\n",
      "Iteration 488/1000, Loss: 0.1747831255197525\n",
      "Iteration 489/1000, Loss: 0.1746436506509781\n",
      "Iteration 490/1000, Loss: 0.17450451850891113\n",
      "Iteration 491/1000, Loss: 0.17436565458774567\n",
      "Iteration 492/1000, Loss: 0.17422710359096527\n",
      "Iteration 493/1000, Loss: 0.17408880591392517\n",
      "Iteration 494/1000, Loss: 0.17395082116127014\n",
      "Iteration 495/1000, Loss: 0.173813134431839\n",
      "Iteration 496/1000, Loss: 0.1736757904291153\n",
      "Iteration 497/1000, Loss: 0.17353880405426025\n",
      "Iteration 498/1000, Loss: 0.1734021157026291\n",
      "Iteration 499/1000, Loss: 0.17326578497886658\n",
      "Iteration 500/1000, Loss: 0.17312976717948914\n",
      "Iteration 501/1000, Loss: 0.17299403250217438\n",
      "Iteration 502/1000, Loss: 0.1728585660457611\n",
      "Iteration 503/1000, Loss: 0.17272339761257172\n",
      "Iteration 504/1000, Loss: 0.1725885570049286\n",
      "Iteration 505/1000, Loss: 0.17245399951934814\n",
      "Iteration 506/1000, Loss: 0.17231976985931396\n",
      "Iteration 507/1000, Loss: 0.17218583822250366\n",
      "Iteration 508/1000, Loss: 0.17205221951007843\n",
      "Iteration 509/1000, Loss: 0.17191889882087708\n",
      "Iteration 510/1000, Loss: 0.1717858761548996\n",
      "Iteration 511/1000, Loss: 0.1716531366109848\n",
      "Iteration 512/1000, Loss: 0.17152073979377747\n",
      "Iteration 513/1000, Loss: 0.17138859629631042\n",
      "Iteration 514/1000, Loss: 0.17125673592090607\n",
      "Iteration 515/1000, Loss: 0.1711251437664032\n",
      "Iteration 516/1000, Loss: 0.1709938496351242\n",
      "Iteration 517/1000, Loss: 0.17086286842823029\n",
      "Iteration 518/1000, Loss: 0.17073221504688263\n",
      "Iteration 519/1000, Loss: 0.17060184478759766\n",
      "Iteration 520/1000, Loss: 0.17047175765037537\n",
      "Iteration 521/1000, Loss: 0.17034196853637695\n",
      "Iteration 522/1000, Loss: 0.17021244764328003\n",
      "Iteration 523/1000, Loss: 0.17008323967456818\n",
      "Iteration 524/1000, Loss: 0.1699542999267578\n",
      "Iteration 525/1000, Loss: 0.16982558369636536\n",
      "Iteration 526/1000, Loss: 0.169697105884552\n",
      "Iteration 527/1000, Loss: 0.16956888139247894\n",
      "Iteration 528/1000, Loss: 0.16944094002246857\n",
      "Iteration 529/1000, Loss: 0.1693132370710373\n",
      "Iteration 530/1000, Loss: 0.1691858470439911\n",
      "Iteration 531/1000, Loss: 0.16905875504016876\n",
      "Iteration 532/1000, Loss: 0.16893190145492554\n",
      "Iteration 533/1000, Loss: 0.16880527138710022\n",
      "Iteration 534/1000, Loss: 0.1686789095401764\n",
      "Iteration 535/1000, Loss: 0.16855283081531525\n",
      "Iteration 536/1000, Loss: 0.1684269905090332\n",
      "Iteration 537/1000, Loss: 0.16830144822597504\n",
      "Iteration 538/1000, Loss: 0.16817620396614075\n",
      "Iteration 539/1000, Loss: 0.16805119812488556\n",
      "Iteration 540/1000, Loss: 0.16792649030685425\n",
      "Iteration 541/1000, Loss: 0.16780205070972443\n",
      "Iteration 542/1000, Loss: 0.1676778644323349\n",
      "Iteration 543/1000, Loss: 0.16755399107933044\n",
      "Iteration 544/1000, Loss: 0.16743040084838867\n",
      "Iteration 545/1000, Loss: 0.1673070639371872\n",
      "Iteration 546/1000, Loss: 0.1671840399503708\n",
      "Iteration 547/1000, Loss: 0.16706128418445587\n",
      "Iteration 548/1000, Loss: 0.16693875193595886\n",
      "Iteration 549/1000, Loss: 0.16681650280952454\n",
      "Iteration 550/1000, Loss: 0.1666945517063141\n",
      "Iteration 551/1000, Loss: 0.16657285392284393\n",
      "Iteration 552/1000, Loss: 0.1664513796567917\n",
      "Iteration 553/1000, Loss: 0.1663302183151245\n",
      "Iteration 554/1000, Loss: 0.16620929539203644\n",
      "Iteration 555/1000, Loss: 0.16608864068984985\n",
      "Iteration 556/1000, Loss: 0.16596823930740356\n",
      "Iteration 557/1000, Loss: 0.16584810614585876\n",
      "Iteration 558/1000, Loss: 0.16572819650173187\n",
      "Iteration 559/1000, Loss: 0.1656085103750229\n",
      "Iteration 560/1000, Loss: 0.1654890924692154\n",
      "Iteration 561/1000, Loss: 0.1653698831796646\n",
      "Iteration 562/1000, Loss: 0.1652509719133377\n",
      "Iteration 563/1000, Loss: 0.16513225436210632\n",
      "Iteration 564/1000, Loss: 0.16501383483409882\n",
      "Iteration 565/1000, Loss: 0.16489560902118683\n",
      "Iteration 566/1000, Loss: 0.16477762162685394\n",
      "Iteration 567/1000, Loss: 0.16465987265110016\n",
      "Iteration 568/1000, Loss: 0.16454239189624786\n",
      "Iteration 569/1000, Loss: 0.16442513465881348\n",
      "Iteration 570/1000, Loss: 0.164308100938797\n",
      "Iteration 571/1000, Loss: 0.16419129073619843\n",
      "Iteration 572/1000, Loss: 0.16407467424869537\n",
      "Iteration 573/1000, Loss: 0.16395831108093262\n",
      "Iteration 574/1000, Loss: 0.16384217143058777\n",
      "Iteration 575/1000, Loss: 0.16372628509998322\n",
      "Iteration 576/1000, Loss: 0.16361062228679657\n",
      "Iteration 577/1000, Loss: 0.16349521279335022\n",
      "Iteration 578/1000, Loss: 0.16338001191616058\n",
      "Iteration 579/1000, Loss: 0.16326504945755005\n",
      "Iteration 580/1000, Loss: 0.16315032541751862\n",
      "Iteration 581/1000, Loss: 0.1630358099937439\n",
      "Iteration 582/1000, Loss: 0.1629214733839035\n",
      "Iteration 583/1000, Loss: 0.1628073751926422\n",
      "Iteration 584/1000, Loss: 0.16269348561763763\n",
      "Iteration 585/1000, Loss: 0.16257981956005096\n",
      "Iteration 586/1000, Loss: 0.16246634721755981\n",
      "Iteration 587/1000, Loss: 0.16235309839248657\n",
      "Iteration 588/1000, Loss: 0.16224005818367004\n",
      "Iteration 589/1000, Loss: 0.16212725639343262\n",
      "Iteration 590/1000, Loss: 0.1620146781206131\n",
      "Iteration 591/1000, Loss: 0.1619023233652115\n",
      "Iteration 592/1000, Loss: 0.16179019212722778\n",
      "Iteration 593/1000, Loss: 0.161678284406662\n",
      "Iteration 594/1000, Loss: 0.1615666151046753\n",
      "Iteration 595/1000, Loss: 0.16145513951778412\n",
      "Iteration 596/1000, Loss: 0.16134391725063324\n",
      "Iteration 597/1000, Loss: 0.1612328737974167\n",
      "Iteration 598/1000, Loss: 0.16112206876277924\n",
      "Iteration 599/1000, Loss: 0.16101141273975372\n",
      "Iteration 600/1000, Loss: 0.1609009951353073\n",
      "Iteration 601/1000, Loss: 0.16079075634479523\n",
      "Iteration 602/1000, Loss: 0.16068075597286224\n",
      "Iteration 603/1000, Loss: 0.16057096421718597\n",
      "Iteration 604/1000, Loss: 0.16046136617660522\n",
      "Iteration 605/1000, Loss: 0.16035199165344238\n",
      "Iteration 606/1000, Loss: 0.16024282574653625\n",
      "Iteration 607/1000, Loss: 0.16013383865356445\n",
      "Iteration 608/1000, Loss: 0.16002503037452698\n",
      "Iteration 609/1000, Loss: 0.15991640090942383\n",
      "Iteration 610/1000, Loss: 0.1598079651594162\n",
      "Iteration 611/1000, Loss: 0.1596996784210205\n",
      "Iteration 612/1000, Loss: 0.15959158539772034\n",
      "Iteration 613/1000, Loss: 0.15948368608951569\n",
      "Iteration 614/1000, Loss: 0.15937599539756775\n",
      "Iteration 615/1000, Loss: 0.15926845371723175\n",
      "Iteration 616/1000, Loss: 0.15916118025779724\n",
      "Iteration 617/1000, Loss: 0.15905410051345825\n",
      "Iteration 618/1000, Loss: 0.1589471995830536\n",
      "Iteration 619/1000, Loss: 0.15884052217006683\n",
      "Iteration 620/1000, Loss: 0.1587340533733368\n",
      "Iteration 621/1000, Loss: 0.15862776339054108\n",
      "Iteration 622/1000, Loss: 0.15852166712284088\n",
      "Iteration 623/1000, Loss: 0.15841573476791382\n",
      "Iteration 624/1000, Loss: 0.1583099663257599\n",
      "Iteration 625/1000, Loss: 0.15820439159870148\n",
      "Iteration 626/1000, Loss: 0.1580989956855774\n",
      "Iteration 627/1000, Loss: 0.15799373388290405\n",
      "Iteration 628/1000, Loss: 0.15788871049880981\n",
      "Iteration 629/1000, Loss: 0.1577838510274887\n",
      "Iteration 630/1000, Loss: 0.15767917037010193\n",
      "Iteration 631/1000, Loss: 0.1575746238231659\n",
      "Iteration 632/1000, Loss: 0.15747027099132538\n",
      "Iteration 633/1000, Loss: 0.15736611187458038\n",
      "Iteration 634/1000, Loss: 0.1572621911764145\n",
      "Iteration 635/1000, Loss: 0.15715843439102173\n",
      "Iteration 636/1000, Loss: 0.1570548564195633\n",
      "Iteration 637/1000, Loss: 0.15695136785507202\n",
      "Iteration 638/1000, Loss: 0.15684804320335388\n",
      "Iteration 639/1000, Loss: 0.15674488246440887\n",
      "Iteration 640/1000, Loss: 0.15664194524288177\n",
      "Iteration 641/1000, Loss: 0.1565391570329666\n",
      "Iteration 642/1000, Loss: 0.15643659234046936\n",
      "Iteration 643/1000, Loss: 0.15633414685726166\n",
      "Iteration 644/1000, Loss: 0.15623193979263306\n",
      "Iteration 645/1000, Loss: 0.1561298668384552\n",
      "Iteration 646/1000, Loss: 0.15602798759937286\n",
      "Iteration 647/1000, Loss: 0.15592627227306366\n",
      "Iteration 648/1000, Loss: 0.15582473576068878\n",
      "Iteration 649/1000, Loss: 0.15572337806224823\n",
      "Iteration 650/1000, Loss: 0.15562216937541962\n",
      "Iteration 651/1000, Loss: 0.15552110970020294\n",
      "Iteration 652/1000, Loss: 0.1554201990365982\n",
      "Iteration 653/1000, Loss: 0.1553194671869278\n",
      "Iteration 654/1000, Loss: 0.1552189141511917\n",
      "Iteration 655/1000, Loss: 0.15511849522590637\n",
      "Iteration 656/1000, Loss: 0.15501825511455536\n",
      "Iteration 657/1000, Loss: 0.15491822361946106\n",
      "Iteration 658/1000, Loss: 0.1548183262348175\n",
      "Iteration 659/1000, Loss: 0.15471863746643066\n",
      "Iteration 660/1000, Loss: 0.15461911261081696\n",
      "Iteration 661/1000, Loss: 0.15451975166797638\n",
      "Iteration 662/1000, Loss: 0.1544206291437149\n",
      "Iteration 663/1000, Loss: 0.1543216109275818\n",
      "Iteration 664/1000, Loss: 0.1542227566242218\n",
      "Iteration 665/1000, Loss: 0.15412405133247375\n",
      "Iteration 666/1000, Loss: 0.15402555465698242\n",
      "Iteration 667/1000, Loss: 0.15392714738845825\n",
      "Iteration 668/1000, Loss: 0.1538289338350296\n",
      "Iteration 669/1000, Loss: 0.15373089909553528\n",
      "Iteration 670/1000, Loss: 0.1536329984664917\n",
      "Iteration 671/1000, Loss: 0.15353521704673767\n",
      "Iteration 672/1000, Loss: 0.15343764424324036\n",
      "Iteration 673/1000, Loss: 0.15334023535251617\n",
      "Iteration 674/1000, Loss: 0.15324297547340393\n",
      "Iteration 675/1000, Loss: 0.15314584970474243\n",
      "Iteration 676/1000, Loss: 0.15304888784885406\n",
      "Iteration 677/1000, Loss: 0.15295204520225525\n",
      "Iteration 678/1000, Loss: 0.15285536646842957\n",
      "Iteration 679/1000, Loss: 0.15275885164737701\n",
      "Iteration 680/1000, Loss: 0.1526624858379364\n",
      "Iteration 681/1000, Loss: 0.15256625413894653\n",
      "Iteration 682/1000, Loss: 0.1524701863527298\n",
      "Iteration 683/1000, Loss: 0.1523742377758026\n",
      "Iteration 684/1000, Loss: 0.15227848291397095\n",
      "Iteration 685/1000, Loss: 0.15218287706375122\n",
      "Iteration 686/1000, Loss: 0.15208742022514343\n",
      "Iteration 687/1000, Loss: 0.15199214220046997\n",
      "Iteration 688/1000, Loss: 0.15189704298973083\n",
      "Iteration 689/1000, Loss: 0.15180206298828125\n",
      "Iteration 690/1000, Loss: 0.1517072319984436\n",
      "Iteration 691/1000, Loss: 0.15161250531673431\n",
      "Iteration 692/1000, Loss: 0.15151794254779816\n",
      "Iteration 693/1000, Loss: 0.15142355859279633\n",
      "Iteration 694/1000, Loss: 0.15132929384708405\n",
      "Iteration 695/1000, Loss: 0.1512351930141449\n",
      "Iteration 696/1000, Loss: 0.1511412113904953\n",
      "Iteration 697/1000, Loss: 0.15104739367961884\n",
      "Iteration 698/1000, Loss: 0.1509537696838379\n",
      "Iteration 699/1000, Loss: 0.15086032450199127\n",
      "Iteration 700/1000, Loss: 0.15076695382595062\n",
      "Iteration 701/1000, Loss: 0.1506737768650055\n",
      "Iteration 702/1000, Loss: 0.1505807340145111\n",
      "Iteration 703/1000, Loss: 0.15048781037330627\n",
      "Iteration 704/1000, Loss: 0.1503949910402298\n",
      "Iteration 705/1000, Loss: 0.15030235052108765\n",
      "Iteration 706/1000, Loss: 0.15020984411239624\n",
      "Iteration 707/1000, Loss: 0.15011748671531677\n",
      "Iteration 708/1000, Loss: 0.15002526342868805\n",
      "Iteration 709/1000, Loss: 0.14993315935134888\n",
      "Iteration 710/1000, Loss: 0.14984123408794403\n",
      "Iteration 711/1000, Loss: 0.14974944293498993\n",
      "Iteration 712/1000, Loss: 0.14965781569480896\n",
      "Iteration 713/1000, Loss: 0.14956630766391754\n",
      "Iteration 714/1000, Loss: 0.14947499334812164\n",
      "Iteration 715/1000, Loss: 0.14938384294509888\n",
      "Iteration 716/1000, Loss: 0.14929281175136566\n",
      "Iteration 717/1000, Loss: 0.14920195937156677\n",
      "Iteration 718/1000, Loss: 0.14911121129989624\n",
      "Iteration 719/1000, Loss: 0.14902064204216003\n",
      "Iteration 720/1000, Loss: 0.14893020689487457\n",
      "Iteration 721/1000, Loss: 0.14883989095687866\n",
      "Iteration 722/1000, Loss: 0.14874973893165588\n",
      "Iteration 723/1000, Loss: 0.14865969121456146\n",
      "Iteration 724/1000, Loss: 0.14856977760791779\n",
      "Iteration 725/1000, Loss: 0.14848002791404724\n",
      "Iteration 726/1000, Loss: 0.14839042723178864\n",
      "Iteration 727/1000, Loss: 0.14830094575881958\n",
      "Iteration 728/1000, Loss: 0.14821162819862366\n",
      "Iteration 729/1000, Loss: 0.14812244474887848\n",
      "Iteration 730/1000, Loss: 0.14803342521190643\n",
      "Iteration 731/1000, Loss: 0.14794456958770752\n",
      "Iteration 732/1000, Loss: 0.14785578846931458\n",
      "Iteration 733/1000, Loss: 0.14776715636253357\n",
      "Iteration 734/1000, Loss: 0.1476786732673645\n",
      "Iteration 735/1000, Loss: 0.1475902944803238\n",
      "Iteration 736/1000, Loss: 0.1475020796060562\n",
      "Iteration 737/1000, Loss: 0.1474139392375946\n",
      "Iteration 738/1000, Loss: 0.14732591807842255\n",
      "Iteration 739/1000, Loss: 0.14723806083202362\n",
      "Iteration 740/1000, Loss: 0.14715033769607544\n",
      "Iteration 741/1000, Loss: 0.1470627337694168\n",
      "Iteration 742/1000, Loss: 0.14697524905204773\n",
      "Iteration 743/1000, Loss: 0.1468878537416458\n",
      "Iteration 744/1000, Loss: 0.14680062234401703\n",
      "Iteration 745/1000, Loss: 0.1467135101556778\n",
      "Iteration 746/1000, Loss: 0.1466265469789505\n",
      "Iteration 747/1000, Loss: 0.14653974771499634\n",
      "Iteration 748/1000, Loss: 0.14645308256149292\n",
      "Iteration 749/1000, Loss: 0.14636656641960144\n",
      "Iteration 750/1000, Loss: 0.1462802141904831\n",
      "Iteration 751/1000, Loss: 0.1461939960718155\n",
      "Iteration 752/1000, Loss: 0.14610791206359863\n",
      "Iteration 753/1000, Loss: 0.1460219770669937\n",
      "Iteration 754/1000, Loss: 0.14593614637851715\n",
      "Iteration 755/1000, Loss: 0.14585044980049133\n",
      "Iteration 756/1000, Loss: 0.14576490223407745\n",
      "Iteration 757/1000, Loss: 0.14567945897579193\n",
      "Iteration 758/1000, Loss: 0.14559414982795715\n",
      "Iteration 759/1000, Loss: 0.14550894498825073\n",
      "Iteration 760/1000, Loss: 0.14542388916015625\n",
      "Iteration 761/1000, Loss: 0.1453389823436737\n",
      "Iteration 762/1000, Loss: 0.1452541947364807\n",
      "Iteration 763/1000, Loss: 0.14516957104206085\n",
      "Iteration 764/1000, Loss: 0.14508503675460815\n",
      "Iteration 765/1000, Loss: 0.145000621676445\n",
      "Iteration 766/1000, Loss: 0.14491631090641022\n",
      "Iteration 767/1000, Loss: 0.14483213424682617\n",
      "Iteration 768/1000, Loss: 0.14474807679653168\n",
      "Iteration 769/1000, Loss: 0.14466415345668793\n",
      "Iteration 770/1000, Loss: 0.14458030462265015\n",
      "Iteration 771/1000, Loss: 0.14449657499790192\n",
      "Iteration 772/1000, Loss: 0.14441294968128204\n",
      "Iteration 773/1000, Loss: 0.1443295031785965\n",
      "Iteration 774/1000, Loss: 0.1442461907863617\n",
      "Iteration 775/1000, Loss: 0.14416298270225525\n",
      "Iteration 776/1000, Loss: 0.14407987892627716\n",
      "Iteration 777/1000, Loss: 0.1439969390630722\n",
      "Iteration 778/1000, Loss: 0.1439141184091568\n",
      "Iteration 779/1000, Loss: 0.14383140206336975\n",
      "Iteration 780/1000, Loss: 0.14374879002571106\n",
      "Iteration 781/1000, Loss: 0.14366623759269714\n",
      "Iteration 782/1000, Loss: 0.14358381927013397\n",
      "Iteration 783/1000, Loss: 0.14350149035453796\n",
      "Iteration 784/1000, Loss: 0.14341923594474792\n",
      "Iteration 785/1000, Loss: 0.14333713054656982\n",
      "Iteration 786/1000, Loss: 0.1432551145553589\n",
      "Iteration 787/1000, Loss: 0.1431732475757599\n",
      "Iteration 788/1000, Loss: 0.14309147000312805\n",
      "Iteration 789/1000, Loss: 0.14300982654094696\n",
      "Iteration 790/1000, Loss: 0.1429283171892166\n",
      "Iteration 791/1000, Loss: 0.142846941947937\n",
      "Iteration 792/1000, Loss: 0.14276564121246338\n",
      "Iteration 793/1000, Loss: 0.1426844745874405\n",
      "Iteration 794/1000, Loss: 0.14260338246822357\n",
      "Iteration 795/1000, Loss: 0.1425224095582962\n",
      "Iteration 796/1000, Loss: 0.1424415409564972\n",
      "Iteration 797/1000, Loss: 0.14236077666282654\n",
      "Iteration 798/1000, Loss: 0.14228011667728424\n",
      "Iteration 799/1000, Loss: 0.1421995609998703\n",
      "Iteration 800/1000, Loss: 0.1421191394329071\n",
      "Iteration 801/1000, Loss: 0.14203883707523346\n",
      "Iteration 802/1000, Loss: 0.14195862412452698\n",
      "Iteration 803/1000, Loss: 0.14187850058078766\n",
      "Iteration 804/1000, Loss: 0.1417984962463379\n",
      "Iteration 805/1000, Loss: 0.14171862602233887\n",
      "Iteration 806/1000, Loss: 0.1416388303041458\n",
      "Iteration 807/1000, Loss: 0.1415591835975647\n",
      "Iteration 808/1000, Loss: 0.14147964119911194\n",
      "Iteration 809/1000, Loss: 0.14140018820762634\n",
      "Iteration 810/1000, Loss: 0.1413208693265915\n",
      "Iteration 811/1000, Loss: 0.1412416696548462\n",
      "Iteration 812/1000, Loss: 0.14116255939006805\n",
      "Iteration 813/1000, Loss: 0.14108358323574066\n",
      "Iteration 814/1000, Loss: 0.14100468158721924\n",
      "Iteration 815/1000, Loss: 0.14092589914798737\n",
      "Iteration 816/1000, Loss: 0.14084723591804504\n",
      "Iteration 817/1000, Loss: 0.14076866209506989\n",
      "Iteration 818/1000, Loss: 0.14069022238254547\n",
      "Iteration 819/1000, Loss: 0.14061184227466583\n",
      "Iteration 820/1000, Loss: 0.14053362607955933\n",
      "Iteration 821/1000, Loss: 0.1404554843902588\n",
      "Iteration 822/1000, Loss: 0.1403774470090866\n",
      "Iteration 823/1000, Loss: 0.1402994841337204\n",
      "Iteration 824/1000, Loss: 0.14022164046764374\n",
      "Iteration 825/1000, Loss: 0.14014388620853424\n",
      "Iteration 826/1000, Loss: 0.1400662213563919\n",
      "Iteration 827/1000, Loss: 0.13998869061470032\n",
      "Iteration 828/1000, Loss: 0.1399112343788147\n",
      "Iteration 829/1000, Loss: 0.13983388245105743\n",
      "Iteration 830/1000, Loss: 0.13975663483142853\n",
      "Iteration 831/1000, Loss: 0.13967952132225037\n",
      "Iteration 832/1000, Loss: 0.13960251212120056\n",
      "Iteration 833/1000, Loss: 0.13952559232711792\n",
      "Iteration 834/1000, Loss: 0.13944873213768005\n",
      "Iteration 835/1000, Loss: 0.13937200605869293\n",
      "Iteration 836/1000, Loss: 0.13929533958435059\n",
      "Iteration 837/1000, Loss: 0.13921883702278137\n",
      "Iteration 838/1000, Loss: 0.13914239406585693\n",
      "Iteration 839/1000, Loss: 0.13906608521938324\n",
      "Iteration 840/1000, Loss: 0.1389898806810379\n",
      "Iteration 841/1000, Loss: 0.13891375064849854\n",
      "Iteration 842/1000, Loss: 0.1388377547264099\n",
      "Iteration 843/1000, Loss: 0.13876187801361084\n",
      "Iteration 844/1000, Loss: 0.1386861354112625\n",
      "Iteration 845/1000, Loss: 0.13861045241355896\n",
      "Iteration 846/1000, Loss: 0.13853487372398376\n",
      "Iteration 847/1000, Loss: 0.13845941424369812\n",
      "Iteration 848/1000, Loss: 0.13838408887386322\n",
      "Iteration 849/1000, Loss: 0.13830885291099548\n",
      "Iteration 850/1000, Loss: 0.1382337063550949\n",
      "Iteration 851/1000, Loss: 0.1381586194038391\n",
      "Iteration 852/1000, Loss: 0.13808362185955048\n",
      "Iteration 853/1000, Loss: 0.13800875842571259\n",
      "Iteration 854/1000, Loss: 0.13793393969535828\n",
      "Iteration 855/1000, Loss: 0.13785921037197113\n",
      "Iteration 856/1000, Loss: 0.13778455555438995\n",
      "Iteration 857/1000, Loss: 0.13770999014377594\n",
      "Iteration 858/1000, Loss: 0.13763554394245148\n",
      "Iteration 859/1000, Loss: 0.13756120204925537\n",
      "Iteration 860/1000, Loss: 0.13748691976070404\n",
      "Iteration 861/1000, Loss: 0.13741269707679749\n",
      "Iteration 862/1000, Loss: 0.1373385787010193\n",
      "Iteration 863/1000, Loss: 0.13726451992988586\n",
      "Iteration 864/1000, Loss: 0.137190580368042\n",
      "Iteration 865/1000, Loss: 0.13711673021316528\n",
      "Iteration 866/1000, Loss: 0.13704298436641693\n",
      "Iteration 867/1000, Loss: 0.13696934282779694\n",
      "Iteration 868/1000, Loss: 0.13689573109149933\n",
      "Iteration 869/1000, Loss: 0.13682223856449127\n",
      "Iteration 870/1000, Loss: 0.1367487907409668\n",
      "Iteration 871/1000, Loss: 0.13667549192905426\n",
      "Iteration 872/1000, Loss: 0.1366022676229477\n",
      "Iteration 873/1000, Loss: 0.1365291327238083\n",
      "Iteration 874/1000, Loss: 0.13645607233047485\n",
      "Iteration 875/1000, Loss: 0.13638311624526978\n",
      "Iteration 876/1000, Loss: 0.13631024956703186\n",
      "Iteration 877/1000, Loss: 0.1362374871969223\n",
      "Iteration 878/1000, Loss: 0.1361648291349411\n",
      "Iteration 879/1000, Loss: 0.13609227538108826\n",
      "Iteration 880/1000, Loss: 0.13601981103420258\n",
      "Iteration 881/1000, Loss: 0.13594745099544525\n",
      "Iteration 882/1000, Loss: 0.1358751654624939\n",
      "Iteration 883/1000, Loss: 0.1358030140399933\n",
      "Iteration 884/1000, Loss: 0.13573092222213745\n",
      "Iteration 885/1000, Loss: 0.13565890491008759\n",
      "Iteration 886/1000, Loss: 0.13558699190616608\n",
      "Iteration 887/1000, Loss: 0.13551518321037292\n",
      "Iteration 888/1000, Loss: 0.13544347882270813\n",
      "Iteration 889/1000, Loss: 0.1353718787431717\n",
      "Iteration 890/1000, Loss: 0.13530036807060242\n",
      "Iteration 891/1000, Loss: 0.1352289766073227\n",
      "Iteration 892/1000, Loss: 0.13515762984752655\n",
      "Iteration 893/1000, Loss: 0.13508638739585876\n",
      "Iteration 894/1000, Loss: 0.13501523435115814\n",
      "Iteration 895/1000, Loss: 0.13494418561458588\n",
      "Iteration 896/1000, Loss: 0.1348731964826584\n",
      "Iteration 897/1000, Loss: 0.13480232656002045\n",
      "Iteration 898/1000, Loss: 0.13473153114318848\n",
      "Iteration 899/1000, Loss: 0.13466082513332367\n",
      "Iteration 900/1000, Loss: 0.13459022343158722\n",
      "Iteration 901/1000, Loss: 0.13451966643333435\n",
      "Iteration 902/1000, Loss: 0.13444921374320984\n",
      "Iteration 903/1000, Loss: 0.1343788504600525\n",
      "Iteration 904/1000, Loss: 0.13430854678153992\n",
      "Iteration 905/1000, Loss: 0.1342383325099945\n",
      "Iteration 906/1000, Loss: 0.13416819274425507\n",
      "Iteration 907/1000, Loss: 0.13409815728664398\n",
      "Iteration 908/1000, Loss: 0.13402816653251648\n",
      "Iteration 909/1000, Loss: 0.13395826518535614\n",
      "Iteration 910/1000, Loss: 0.13388843834400177\n",
      "Iteration 911/1000, Loss: 0.13381868600845337\n",
      "Iteration 912/1000, Loss: 0.13374900817871094\n",
      "Iteration 913/1000, Loss: 0.13367938995361328\n",
      "Iteration 914/1000, Loss: 0.1336098611354828\n",
      "Iteration 915/1000, Loss: 0.13354039192199707\n",
      "Iteration 916/1000, Loss: 0.1334710270166397\n",
      "Iteration 917/1000, Loss: 0.13340173661708832\n",
      "Iteration 918/1000, Loss: 0.1333325356245041\n",
      "Iteration 919/1000, Loss: 0.13326340913772583\n",
      "Iteration 920/1000, Loss: 0.13319435715675354\n",
      "Iteration 921/1000, Loss: 0.1331253945827484\n",
      "Iteration 922/1000, Loss: 0.13305650651454926\n",
      "Iteration 923/1000, Loss: 0.13298770785331726\n",
      "Iteration 924/1000, Loss: 0.13291898369789124\n",
      "Iteration 925/1000, Loss: 0.13285036385059357\n",
      "Iteration 926/1000, Loss: 0.13278181850910187\n",
      "Iteration 927/1000, Loss: 0.13271333277225494\n",
      "Iteration 928/1000, Loss: 0.1326449066400528\n",
      "Iteration 929/1000, Loss: 0.1325765699148178\n",
      "Iteration 930/1000, Loss: 0.1325082927942276\n",
      "Iteration 931/1000, Loss: 0.13244011998176575\n",
      "Iteration 932/1000, Loss: 0.13237202167510986\n",
      "Iteration 933/1000, Loss: 0.13230398297309875\n",
      "Iteration 934/1000, Loss: 0.1322360336780548\n",
      "Iteration 935/1000, Loss: 0.13216815888881683\n",
      "Iteration 936/1000, Loss: 0.13210037350654602\n",
      "Iteration 937/1000, Loss: 0.13203266263008118\n",
      "Iteration 938/1000, Loss: 0.1319650262594223\n",
      "Iteration 939/1000, Loss: 0.1318974643945694\n",
      "Iteration 940/1000, Loss: 0.13182999193668365\n",
      "Iteration 941/1000, Loss: 0.13176259398460388\n",
      "Iteration 942/1000, Loss: 0.13169528543949127\n",
      "Iteration 943/1000, Loss: 0.13162803649902344\n",
      "Iteration 944/1000, Loss: 0.13156087696552277\n",
      "Iteration 945/1000, Loss: 0.13149376213550568\n",
      "Iteration 946/1000, Loss: 0.13142675161361694\n",
      "Iteration 947/1000, Loss: 0.13135981559753418\n",
      "Iteration 948/1000, Loss: 0.13129298388957977\n",
      "Iteration 949/1000, Loss: 0.13122624158859253\n",
      "Iteration 950/1000, Loss: 0.13115957379341125\n",
      "Iteration 951/1000, Loss: 0.13109298050403595\n",
      "Iteration 952/1000, Loss: 0.1310264766216278\n",
      "Iteration 953/1000, Loss: 0.13096006214618683\n",
      "Iteration 954/1000, Loss: 0.13089372217655182\n",
      "Iteration 955/1000, Loss: 0.13082745671272278\n",
      "Iteration 956/1000, Loss: 0.1307612657546997\n",
      "Iteration 957/1000, Loss: 0.1306951642036438\n",
      "Iteration 958/1000, Loss: 0.13062915205955505\n",
      "Iteration 959/1000, Loss: 0.13056321442127228\n",
      "Iteration 960/1000, Loss: 0.13049733638763428\n",
      "Iteration 961/1000, Loss: 0.13043153285980225\n",
      "Iteration 962/1000, Loss: 0.1303657740354538\n",
      "Iteration 963/1000, Loss: 0.1303001046180725\n",
      "Iteration 964/1000, Loss: 0.1302345097064972\n",
      "Iteration 965/1000, Loss: 0.13016898930072784\n",
      "Iteration 966/1000, Loss: 0.13010354340076447\n",
      "Iteration 967/1000, Loss: 0.13003815710544586\n",
      "Iteration 968/1000, Loss: 0.12997281551361084\n",
      "Iteration 969/1000, Loss: 0.1299075484275818\n",
      "Iteration 970/1000, Loss: 0.1298423409461975\n",
      "Iteration 971/1000, Loss: 0.12977717816829681\n",
      "Iteration 972/1000, Loss: 0.1297120898962021\n",
      "Iteration 973/1000, Loss: 0.12964707612991333\n",
      "Iteration 974/1000, Loss: 0.12958212196826935\n",
      "Iteration 975/1000, Loss: 0.12951725721359253\n",
      "Iteration 976/1000, Loss: 0.1294524371623993\n",
      "Iteration 977/1000, Loss: 0.12938769161701202\n",
      "Iteration 978/1000, Loss: 0.12932299077510834\n",
      "Iteration 979/1000, Loss: 0.12925837934017181\n",
      "Iteration 980/1000, Loss: 0.12919385731220245\n",
      "Iteration 981/1000, Loss: 0.12912942469120026\n",
      "Iteration 982/1000, Loss: 0.12906505167484283\n",
      "Iteration 983/1000, Loss: 0.12900075316429138\n",
      "Iteration 984/1000, Loss: 0.1289365440607071\n",
      "Iteration 985/1000, Loss: 0.12887240946292877\n",
      "Iteration 986/1000, Loss: 0.12880833446979523\n",
      "Iteration 987/1000, Loss: 0.12874433398246765\n",
      "Iteration 988/1000, Loss: 0.12868040800094604\n",
      "Iteration 989/1000, Loss: 0.1286165565252304\n",
      "Iteration 990/1000, Loss: 0.12855276465415955\n",
      "Iteration 991/1000, Loss: 0.12848904728889465\n",
      "Iteration 992/1000, Loss: 0.12842541933059692\n",
      "Iteration 993/1000, Loss: 0.12836183607578278\n",
      "Iteration 994/1000, Loss: 0.1282983273267746\n",
      "Iteration 995/1000, Loss: 0.1282348781824112\n",
      "Iteration 996/1000, Loss: 0.12817151844501495\n",
      "Iteration 997/1000, Loss: 0.1281082183122635\n",
      "Iteration 998/1000, Loss: 0.128044992685318\n",
      "Iteration 999/1000, Loss: 0.12798181176185608\n",
      "Iteration 1000/1000, Loss: 0.12791872024536133\n",
      "Pruning Step 5\n",
      "Iteration 1/1000, Loss: 1.333052158355713\n",
      "Iteration 2/1000, Loss: 1.24921452999115\n",
      "Iteration 3/1000, Loss: 1.1784354448318481\n",
      "Iteration 4/1000, Loss: 1.1151710748672485\n",
      "Iteration 5/1000, Loss: 1.0579630136489868\n",
      "Iteration 6/1000, Loss: 1.0060466527938843\n",
      "Iteration 7/1000, Loss: 0.9588607549667358\n",
      "Iteration 8/1000, Loss: 0.9158855676651001\n",
      "Iteration 9/1000, Loss: 0.8766766786575317\n",
      "Iteration 10/1000, Loss: 0.8408430814743042\n",
      "Iteration 11/1000, Loss: 0.8080469965934753\n",
      "Iteration 12/1000, Loss: 0.7779872417449951\n",
      "Iteration 13/1000, Loss: 0.750376284122467\n",
      "Iteration 14/1000, Loss: 0.72496497631073\n",
      "Iteration 15/1000, Loss: 0.7015388011932373\n",
      "Iteration 16/1000, Loss: 0.6799063682556152\n",
      "Iteration 17/1000, Loss: 0.6598937511444092\n",
      "Iteration 18/1000, Loss: 0.6413456797599792\n",
      "Iteration 19/1000, Loss: 0.6241239309310913\n",
      "Iteration 20/1000, Loss: 0.6081016659736633\n",
      "Iteration 21/1000, Loss: 0.5931695103645325\n",
      "Iteration 22/1000, Loss: 0.5792294144630432\n",
      "Iteration 23/1000, Loss: 0.5661937594413757\n",
      "Iteration 24/1000, Loss: 0.5539820790290833\n",
      "Iteration 25/1000, Loss: 0.5425231456756592\n",
      "Iteration 26/1000, Loss: 0.5317544341087341\n",
      "Iteration 27/1000, Loss: 0.5216155648231506\n",
      "Iteration 28/1000, Loss: 0.512055516242981\n",
      "Iteration 29/1000, Loss: 0.5030285716056824\n",
      "Iteration 30/1000, Loss: 0.49449485540390015\n",
      "Iteration 31/1000, Loss: 0.4864148497581482\n",
      "Iteration 32/1000, Loss: 0.4787542223930359\n",
      "Iteration 33/1000, Loss: 0.4714830815792084\n",
      "Iteration 34/1000, Loss: 0.46457183361053467\n",
      "Iteration 35/1000, Loss: 0.45799532532691956\n",
      "Iteration 36/1000, Loss: 0.4517306685447693\n",
      "Iteration 37/1000, Loss: 0.4457542598247528\n",
      "Iteration 38/1000, Loss: 0.4400477111339569\n",
      "Iteration 39/1000, Loss: 0.43459394574165344\n",
      "Iteration 40/1000, Loss: 0.4293749928474426\n",
      "Iteration 41/1000, Loss: 0.42437663674354553\n",
      "Iteration 42/1000, Loss: 0.4195846915245056\n",
      "Iteration 43/1000, Loss: 0.4149866998195648\n",
      "Iteration 44/1000, Loss: 0.41057154536247253\n",
      "Iteration 45/1000, Loss: 0.4063289761543274\n",
      "Iteration 46/1000, Loss: 0.40224844217300415\n",
      "Iteration 47/1000, Loss: 0.39831972122192383\n",
      "Iteration 48/1000, Loss: 0.39453497529029846\n",
      "Iteration 49/1000, Loss: 0.3908855617046356\n",
      "Iteration 50/1000, Loss: 0.38736581802368164\n",
      "Iteration 51/1000, Loss: 0.38396844267845154\n",
      "Iteration 52/1000, Loss: 0.3806864619255066\n",
      "Iteration 53/1000, Loss: 0.3775140047073364\n",
      "Iteration 54/1000, Loss: 0.3744458556175232\n",
      "Iteration 55/1000, Loss: 0.3714764714241028\n",
      "Iteration 56/1000, Loss: 0.3686007857322693\n",
      "Iteration 57/1000, Loss: 0.3658140003681183\n",
      "Iteration 58/1000, Loss: 0.3631122410297394\n",
      "Iteration 59/1000, Loss: 0.36049172282218933\n",
      "Iteration 60/1000, Loss: 0.35794851183891296\n",
      "Iteration 61/1000, Loss: 0.35547885298728943\n",
      "Iteration 62/1000, Loss: 0.35307928919792175\n",
      "Iteration 63/1000, Loss: 0.35074687004089355\n",
      "Iteration 64/1000, Loss: 0.34847843647003174\n",
      "Iteration 65/1000, Loss: 0.346271276473999\n",
      "Iteration 66/1000, Loss: 0.3441227078437805\n",
      "Iteration 67/1000, Loss: 0.34202975034713745\n",
      "Iteration 68/1000, Loss: 0.3399905562400818\n",
      "Iteration 69/1000, Loss: 0.3380032181739807\n",
      "Iteration 70/1000, Loss: 0.3360653519630432\n",
      "Iteration 71/1000, Loss: 0.3341750502586365\n",
      "Iteration 72/1000, Loss: 0.33233052492141724\n",
      "Iteration 73/1000, Loss: 0.33053019642829895\n",
      "Iteration 74/1000, Loss: 0.3287721574306488\n",
      "Iteration 75/1000, Loss: 0.32705456018447876\n",
      "Iteration 76/1000, Loss: 0.3253759741783142\n",
      "Iteration 77/1000, Loss: 0.32373520731925964\n",
      "Iteration 78/1000, Loss: 0.3221309185028076\n",
      "Iteration 79/1000, Loss: 0.32056114077568054\n",
      "Iteration 80/1000, Loss: 0.3190251886844635\n",
      "Iteration 81/1000, Loss: 0.3175216615200043\n",
      "Iteration 82/1000, Loss: 0.3160494863986969\n",
      "Iteration 83/1000, Loss: 0.3146074116230011\n",
      "Iteration 84/1000, Loss: 0.31319460272789\n",
      "Iteration 85/1000, Loss: 0.31181010603904724\n",
      "Iteration 86/1000, Loss: 0.3104530870914459\n",
      "Iteration 87/1000, Loss: 0.30912259221076965\n",
      "Iteration 88/1000, Loss: 0.30781736969947815\n",
      "Iteration 89/1000, Loss: 0.30653679370880127\n",
      "Iteration 90/1000, Loss: 0.30527999997138977\n",
      "Iteration 91/1000, Loss: 0.3040466010570526\n",
      "Iteration 92/1000, Loss: 0.3028358221054077\n",
      "Iteration 93/1000, Loss: 0.3016469478607178\n",
      "Iteration 94/1000, Loss: 0.30047911405563354\n",
      "Iteration 95/1000, Loss: 0.2993316650390625\n",
      "Iteration 96/1000, Loss: 0.29820409417152405\n",
      "Iteration 97/1000, Loss: 0.29709556698799133\n",
      "Iteration 98/1000, Loss: 0.2960057556629181\n",
      "Iteration 99/1000, Loss: 0.2949341833591461\n",
      "Iteration 100/1000, Loss: 0.29388031363487244\n",
      "Iteration 101/1000, Loss: 0.29284346103668213\n",
      "Iteration 102/1000, Loss: 0.29182320833206177\n",
      "Iteration 103/1000, Loss: 0.2908191382884979\n",
      "Iteration 104/1000, Loss: 0.2898309528827667\n",
      "Iteration 105/1000, Loss: 0.2888582944869995\n",
      "Iteration 106/1000, Loss: 0.2879006564617157\n",
      "Iteration 107/1000, Loss: 0.28695762157440186\n",
      "Iteration 108/1000, Loss: 0.2860287129878998\n",
      "Iteration 109/1000, Loss: 0.2851135730743408\n",
      "Iteration 110/1000, Loss: 0.2842119038105011\n",
      "Iteration 111/1000, Loss: 0.28332316875457764\n",
      "Iteration 112/1000, Loss: 0.2824470102787018\n",
      "Iteration 113/1000, Loss: 0.28158313035964966\n",
      "Iteration 114/1000, Loss: 0.28073132038116455\n",
      "Iteration 115/1000, Loss: 0.27989134192466736\n",
      "Iteration 116/1000, Loss: 0.27906298637390137\n",
      "Iteration 117/1000, Loss: 0.2782456576824188\n",
      "Iteration 118/1000, Loss: 0.27743929624557495\n",
      "Iteration 119/1000, Loss: 0.27664363384246826\n",
      "Iteration 120/1000, Loss: 0.2758583426475525\n",
      "Iteration 121/1000, Loss: 0.2750832140445709\n",
      "Iteration 122/1000, Loss: 0.27431780099868774\n",
      "Iteration 123/1000, Loss: 0.27356210350990295\n",
      "Iteration 124/1000, Loss: 0.2728160321712494\n",
      "Iteration 125/1000, Loss: 0.27207937836647034\n",
      "Iteration 126/1000, Loss: 0.27135172486305237\n",
      "Iteration 127/1000, Loss: 0.2706327736377716\n",
      "Iteration 128/1000, Loss: 0.26992255449295044\n",
      "Iteration 129/1000, Loss: 0.26922091841697693\n",
      "Iteration 130/1000, Loss: 0.268527626991272\n",
      "Iteration 131/1000, Loss: 0.26784250140190125\n",
      "Iteration 132/1000, Loss: 0.26716527342796326\n",
      "Iteration 133/1000, Loss: 0.2664957344532013\n",
      "Iteration 134/1000, Loss: 0.26583370566368103\n",
      "Iteration 135/1000, Loss: 0.26517918705940247\n",
      "Iteration 136/1000, Loss: 0.2645319700241089\n",
      "Iteration 137/1000, Loss: 0.2638919949531555\n",
      "Iteration 138/1000, Loss: 0.26325902342796326\n",
      "Iteration 139/1000, Loss: 0.2626328766345978\n",
      "Iteration 140/1000, Loss: 0.26201343536376953\n",
      "Iteration 141/1000, Loss: 0.2614004909992218\n",
      "Iteration 142/1000, Loss: 0.26079389452934265\n",
      "Iteration 143/1000, Loss: 0.26019370555877686\n",
      "Iteration 144/1000, Loss: 0.2595996558666229\n",
      "Iteration 145/1000, Loss: 0.2590116262435913\n",
      "Iteration 146/1000, Loss: 0.25842949748039246\n",
      "Iteration 147/1000, Loss: 0.2578532099723816\n",
      "Iteration 148/1000, Loss: 0.2572826147079468\n",
      "Iteration 149/1000, Loss: 0.2567175030708313\n",
      "Iteration 150/1000, Loss: 0.256157785654068\n",
      "Iteration 151/1000, Loss: 0.2556034326553345\n",
      "Iteration 152/1000, Loss: 0.2550543546676636\n",
      "Iteration 153/1000, Loss: 0.2545104920864105\n",
      "Iteration 154/1000, Loss: 0.2539716958999634\n",
      "Iteration 155/1000, Loss: 0.253437876701355\n",
      "Iteration 156/1000, Loss: 0.25290894508361816\n",
      "Iteration 157/1000, Loss: 0.25238484144210815\n",
      "Iteration 158/1000, Loss: 0.25186553597450256\n",
      "Iteration 159/1000, Loss: 0.25135084986686707\n",
      "Iteration 160/1000, Loss: 0.2508407235145569\n",
      "Iteration 161/1000, Loss: 0.2503351867198944\n",
      "Iteration 162/1000, Loss: 0.24983417987823486\n",
      "Iteration 163/1000, Loss: 0.24933753907680511\n",
      "Iteration 164/1000, Loss: 0.248845174908638\n",
      "Iteration 165/1000, Loss: 0.2483569234609604\n",
      "Iteration 166/1000, Loss: 0.24787276983261108\n",
      "Iteration 167/1000, Loss: 0.2473926693201065\n",
      "Iteration 168/1000, Loss: 0.2469165325164795\n",
      "Iteration 169/1000, Loss: 0.2464442402124405\n",
      "Iteration 170/1000, Loss: 0.24597585201263428\n",
      "Iteration 171/1000, Loss: 0.24551136791706085\n",
      "Iteration 172/1000, Loss: 0.24505062401294708\n",
      "Iteration 173/1000, Loss: 0.2445935755968094\n",
      "Iteration 174/1000, Loss: 0.24414008855819702\n",
      "Iteration 175/1000, Loss: 0.24369005858898163\n",
      "Iteration 176/1000, Loss: 0.24324338138103485\n",
      "Iteration 177/1000, Loss: 0.24280013144016266\n",
      "Iteration 178/1000, Loss: 0.2423602044582367\n",
      "Iteration 179/1000, Loss: 0.24192360043525696\n",
      "Iteration 180/1000, Loss: 0.24149037897586823\n",
      "Iteration 181/1000, Loss: 0.2410605102777481\n",
      "Iteration 182/1000, Loss: 0.24063389003276825\n",
      "Iteration 183/1000, Loss: 0.2402104139328003\n",
      "Iteration 184/1000, Loss: 0.2397899478673935\n",
      "Iteration 185/1000, Loss: 0.23937246203422546\n",
      "Iteration 186/1000, Loss: 0.23895803093910217\n",
      "Iteration 187/1000, Loss: 0.23854665458202362\n",
      "Iteration 188/1000, Loss: 0.23813818395137787\n",
      "Iteration 189/1000, Loss: 0.23773258924484253\n",
      "Iteration 190/1000, Loss: 0.23732981085777283\n",
      "Iteration 191/1000, Loss: 0.236929789185524\n",
      "Iteration 192/1000, Loss: 0.23653239011764526\n",
      "Iteration 193/1000, Loss: 0.23613782227039337\n",
      "Iteration 194/1000, Loss: 0.2357458919286728\n",
      "Iteration 195/1000, Loss: 0.23535661399364471\n",
      "Iteration 196/1000, Loss: 0.23496995866298676\n",
      "Iteration 197/1000, Loss: 0.23458583652973175\n",
      "Iteration 198/1000, Loss: 0.23420420289039612\n",
      "Iteration 199/1000, Loss: 0.23382502794265747\n",
      "Iteration 200/1000, Loss: 0.2334483116865158\n",
      "Iteration 201/1000, Loss: 0.23307399451732635\n",
      "Iteration 202/1000, Loss: 0.2327018529176712\n",
      "Iteration 203/1000, Loss: 0.2323320209980011\n",
      "Iteration 204/1000, Loss: 0.23196454346179962\n",
      "Iteration 205/1000, Loss: 0.2315993756055832\n",
      "Iteration 206/1000, Loss: 0.2312365025281906\n",
      "Iteration 207/1000, Loss: 0.23087584972381592\n",
      "Iteration 208/1000, Loss: 0.2305174022912979\n",
      "Iteration 209/1000, Loss: 0.23016120493412018\n",
      "Iteration 210/1000, Loss: 0.22980716824531555\n",
      "Iteration 211/1000, Loss: 0.22945533692836761\n",
      "Iteration 212/1000, Loss: 0.22910559177398682\n",
      "Iteration 213/1000, Loss: 0.22875800728797913\n",
      "Iteration 214/1000, Loss: 0.22841253876686096\n",
      "Iteration 215/1000, Loss: 0.2280690222978592\n",
      "Iteration 216/1000, Loss: 0.22772757709026337\n",
      "Iteration 217/1000, Loss: 0.2273881584405899\n",
      "Iteration 218/1000, Loss: 0.22705064713954926\n",
      "Iteration 219/1000, Loss: 0.22671516239643097\n",
      "Iteration 220/1000, Loss: 0.2263815551996231\n",
      "Iteration 221/1000, Loss: 0.22604990005493164\n",
      "Iteration 222/1000, Loss: 0.22572007775306702\n",
      "Iteration 223/1000, Loss: 0.2253921926021576\n",
      "Iteration 224/1000, Loss: 0.22506611049175262\n",
      "Iteration 225/1000, Loss: 0.22474181652069092\n",
      "Iteration 226/1000, Loss: 0.22441935539245605\n",
      "Iteration 227/1000, Loss: 0.22409871220588684\n",
      "Iteration 228/1000, Loss: 0.22377976775169373\n",
      "Iteration 229/1000, Loss: 0.22346246242523193\n",
      "Iteration 230/1000, Loss: 0.22314684092998505\n",
      "Iteration 231/1000, Loss: 0.22283294796943665\n",
      "Iteration 232/1000, Loss: 0.2225208431482315\n",
      "Iteration 233/1000, Loss: 0.22221048176288605\n",
      "Iteration 234/1000, Loss: 0.2219017595052719\n",
      "Iteration 235/1000, Loss: 0.2215946912765503\n",
      "Iteration 236/1000, Loss: 0.22128929197788239\n",
      "Iteration 237/1000, Loss: 0.220985546708107\n",
      "Iteration 238/1000, Loss: 0.2206832617521286\n",
      "Iteration 239/1000, Loss: 0.22038254141807556\n",
      "Iteration 240/1000, Loss: 0.22008338570594788\n",
      "Iteration 241/1000, Loss: 0.21978577971458435\n",
      "Iteration 242/1000, Loss: 0.21948972344398499\n",
      "Iteration 243/1000, Loss: 0.21919511258602142\n",
      "Iteration 244/1000, Loss: 0.21890194714069366\n",
      "Iteration 245/1000, Loss: 0.21861034631729126\n",
      "Iteration 246/1000, Loss: 0.21832017600536346\n",
      "Iteration 247/1000, Loss: 0.21803151071071625\n",
      "Iteration 248/1000, Loss: 0.21774430572986603\n",
      "Iteration 249/1000, Loss: 0.2174585461616516\n",
      "Iteration 250/1000, Loss: 0.21717406809329987\n",
      "Iteration 251/1000, Loss: 0.21689100563526154\n",
      "Iteration 252/1000, Loss: 0.21660934388637543\n",
      "Iteration 253/1000, Loss: 0.21632906794548035\n",
      "Iteration 254/1000, Loss: 0.2160501480102539\n",
      "Iteration 255/1000, Loss: 0.21577250957489014\n",
      "Iteration 256/1000, Loss: 0.21549615263938904\n",
      "Iteration 257/1000, Loss: 0.21522115170955658\n",
      "Iteration 258/1000, Loss: 0.21494749188423157\n",
      "Iteration 259/1000, Loss: 0.2146751433610916\n",
      "Iteration 260/1000, Loss: 0.21440410614013672\n",
      "Iteration 261/1000, Loss: 0.2141343057155609\n",
      "Iteration 262/1000, Loss: 0.213865727186203\n",
      "Iteration 263/1000, Loss: 0.2135983407497406\n",
      "Iteration 264/1000, Loss: 0.2133321762084961\n",
      "Iteration 265/1000, Loss: 0.2130672186613083\n",
      "Iteration 266/1000, Loss: 0.21280351281166077\n",
      "Iteration 267/1000, Loss: 0.21254098415374756\n",
      "Iteration 268/1000, Loss: 0.21227960288524628\n",
      "Iteration 269/1000, Loss: 0.21201933920383453\n",
      "Iteration 270/1000, Loss: 0.21176022291183472\n",
      "Iteration 271/1000, Loss: 0.21150220930576324\n",
      "Iteration 272/1000, Loss: 0.21124528348445892\n",
      "Iteration 273/1000, Loss: 0.21098947525024414\n",
      "Iteration 274/1000, Loss: 0.21073473989963531\n",
      "Iteration 275/1000, Loss: 0.21048098802566528\n",
      "Iteration 276/1000, Loss: 0.21022821962833405\n",
      "Iteration 277/1000, Loss: 0.209976464509964\n",
      "Iteration 278/1000, Loss: 0.2097257524728775\n",
      "Iteration 279/1000, Loss: 0.209476038813591\n",
      "Iteration 280/1000, Loss: 0.20922741293907166\n",
      "Iteration 281/1000, Loss: 0.20897991955280304\n",
      "Iteration 282/1000, Loss: 0.2087334245443344\n",
      "Iteration 283/1000, Loss: 0.20848801732063293\n",
      "Iteration 284/1000, Loss: 0.2082436978816986\n",
      "Iteration 285/1000, Loss: 0.20800036191940308\n",
      "Iteration 286/1000, Loss: 0.2077580690383911\n",
      "Iteration 287/1000, Loss: 0.2075168341398239\n",
      "Iteration 288/1000, Loss: 0.20727656781673431\n",
      "Iteration 289/1000, Loss: 0.20703724026679993\n",
      "Iteration 290/1000, Loss: 0.20679882168769836\n",
      "Iteration 291/1000, Loss: 0.2065613716840744\n",
      "Iteration 292/1000, Loss: 0.20632489025592804\n",
      "Iteration 293/1000, Loss: 0.20608936250209808\n",
      "Iteration 294/1000, Loss: 0.20585478842258453\n",
      "Iteration 295/1000, Loss: 0.2056211680173874\n",
      "Iteration 296/1000, Loss: 0.20538845658302307\n",
      "Iteration 297/1000, Loss: 0.20515665411949158\n",
      "Iteration 298/1000, Loss: 0.2049257755279541\n",
      "Iteration 299/1000, Loss: 0.20469574630260468\n",
      "Iteration 300/1000, Loss: 0.20446661114692688\n",
      "Iteration 301/1000, Loss: 0.2042384147644043\n",
      "Iteration 302/1000, Loss: 0.20401106774806976\n",
      "Iteration 303/1000, Loss: 0.20378458499908447\n",
      "Iteration 304/1000, Loss: 0.20355895161628723\n",
      "Iteration 305/1000, Loss: 0.20333421230316162\n",
      "Iteration 306/1000, Loss: 0.20311029255390167\n",
      "Iteration 307/1000, Loss: 0.2028871774673462\n",
      "Iteration 308/1000, Loss: 0.20266489684581757\n",
      "Iteration 309/1000, Loss: 0.2024434506893158\n",
      "Iteration 310/1000, Loss: 0.2022228240966797\n",
      "Iteration 311/1000, Loss: 0.20200292766094208\n",
      "Iteration 312/1000, Loss: 0.20178382098674774\n",
      "Iteration 313/1000, Loss: 0.20156548917293549\n",
      "Iteration 314/1000, Loss: 0.2013479322195053\n",
      "Iteration 315/1000, Loss: 0.2011311799287796\n",
      "Iteration 316/1000, Loss: 0.20091523230075836\n",
      "Iteration 317/1000, Loss: 0.2007000595331192\n",
      "Iteration 318/1000, Loss: 0.20048567652702332\n",
      "Iteration 319/1000, Loss: 0.2002720683813095\n",
      "Iteration 320/1000, Loss: 0.20005924999713898\n",
      "Iteration 321/1000, Loss: 0.1998472660779953\n",
      "Iteration 322/1000, Loss: 0.19963596761226654\n",
      "Iteration 323/1000, Loss: 0.19942547380924225\n",
      "Iteration 324/1000, Loss: 0.19921572506427765\n",
      "Iteration 325/1000, Loss: 0.19900670647621155\n",
      "Iteration 326/1000, Loss: 0.19879837334156036\n",
      "Iteration 327/1000, Loss: 0.1985906958580017\n",
      "Iteration 328/1000, Loss: 0.19838371872901917\n",
      "Iteration 329/1000, Loss: 0.19817745685577393\n",
      "Iteration 330/1000, Loss: 0.197971910238266\n",
      "Iteration 331/1000, Loss: 0.19776709377765656\n",
      "Iteration 332/1000, Loss: 0.19756296277046204\n",
      "Iteration 333/1000, Loss: 0.19735953211784363\n",
      "Iteration 334/1000, Loss: 0.19715683162212372\n",
      "Iteration 335/1000, Loss: 0.19695481657981873\n",
      "Iteration 336/1000, Loss: 0.19675347208976746\n",
      "Iteration 337/1000, Loss: 0.19655291736125946\n",
      "Iteration 338/1000, Loss: 0.19635309278964996\n",
      "Iteration 339/1000, Loss: 0.19615398347377777\n",
      "Iteration 340/1000, Loss: 0.1959555596113205\n",
      "Iteration 341/1000, Loss: 0.19575777649879456\n",
      "Iteration 342/1000, Loss: 0.19556066393852234\n",
      "Iteration 343/1000, Loss: 0.19536419212818146\n",
      "Iteration 344/1000, Loss: 0.1951684206724167\n",
      "Iteration 345/1000, Loss: 0.19497330486774445\n",
      "Iteration 346/1000, Loss: 0.19477881491184235\n",
      "Iteration 347/1000, Loss: 0.1945849061012268\n",
      "Iteration 348/1000, Loss: 0.19439160823822021\n",
      "Iteration 349/1000, Loss: 0.19419889152050018\n",
      "Iteration 350/1000, Loss: 0.1940068006515503\n",
      "Iteration 351/1000, Loss: 0.19381539523601532\n",
      "Iteration 352/1000, Loss: 0.19362464547157288\n",
      "Iteration 353/1000, Loss: 0.19343450665473938\n",
      "Iteration 354/1000, Loss: 0.19324494898319244\n",
      "Iteration 355/1000, Loss: 0.19305601716041565\n",
      "Iteration 356/1000, Loss: 0.19286766648292542\n",
      "Iteration 357/1000, Loss: 0.19267989695072174\n",
      "Iteration 358/1000, Loss: 0.19249273836612701\n",
      "Iteration 359/1000, Loss: 0.19230623543262482\n",
      "Iteration 360/1000, Loss: 0.19212031364440918\n",
      "Iteration 361/1000, Loss: 0.1919349730014801\n",
      "Iteration 362/1000, Loss: 0.19175025820732117\n",
      "Iteration 363/1000, Loss: 0.19156616926193237\n",
      "Iteration 364/1000, Loss: 0.19138264656066895\n",
      "Iteration 365/1000, Loss: 0.19119969010353088\n",
      "Iteration 366/1000, Loss: 0.1910172402858734\n",
      "Iteration 367/1000, Loss: 0.19083534181118011\n",
      "Iteration 368/1000, Loss: 0.19065390527248383\n",
      "Iteration 369/1000, Loss: 0.1904730200767517\n",
      "Iteration 370/1000, Loss: 0.19029268622398376\n",
      "Iteration 371/1000, Loss: 0.19011302292346954\n",
      "Iteration 372/1000, Loss: 0.18993385136127472\n",
      "Iteration 373/1000, Loss: 0.18975520133972168\n",
      "Iteration 374/1000, Loss: 0.18957708775997162\n",
      "Iteration 375/1000, Loss: 0.18939945101737976\n",
      "Iteration 376/1000, Loss: 0.18922235071659088\n",
      "Iteration 377/1000, Loss: 0.18904586136341095\n",
      "Iteration 378/1000, Loss: 0.18886998295783997\n",
      "Iteration 379/1000, Loss: 0.18869462609291077\n",
      "Iteration 380/1000, Loss: 0.18851982057094574\n",
      "Iteration 381/1000, Loss: 0.18834547698497772\n",
      "Iteration 382/1000, Loss: 0.1881716102361679\n",
      "Iteration 383/1000, Loss: 0.1879982054233551\n",
      "Iteration 384/1000, Loss: 0.18782532215118408\n",
      "Iteration 385/1000, Loss: 0.18765302002429962\n",
      "Iteration 386/1000, Loss: 0.18748119473457336\n",
      "Iteration 387/1000, Loss: 0.1873098611831665\n",
      "Iteration 388/1000, Loss: 0.18713903427124023\n",
      "Iteration 389/1000, Loss: 0.18696869909763336\n",
      "Iteration 390/1000, Loss: 0.1867988556623459\n",
      "Iteration 391/1000, Loss: 0.186629518866539\n",
      "Iteration 392/1000, Loss: 0.1864606738090515\n",
      "Iteration 393/1000, Loss: 0.18629229068756104\n",
      "Iteration 394/1000, Loss: 0.18612435460090637\n",
      "Iteration 395/1000, Loss: 0.18595696985721588\n",
      "Iteration 396/1000, Loss: 0.1857900321483612\n",
      "Iteration 397/1000, Loss: 0.18562358617782593\n",
      "Iteration 398/1000, Loss: 0.18545755743980408\n",
      "Iteration 399/1000, Loss: 0.18529199063777924\n",
      "Iteration 400/1000, Loss: 0.1851269155740738\n",
      "Iteration 401/1000, Loss: 0.18496234714984894\n",
      "Iteration 402/1000, Loss: 0.18479828536510468\n",
      "Iteration 403/1000, Loss: 0.18463467061519623\n",
      "Iteration 404/1000, Loss: 0.1844714879989624\n",
      "Iteration 405/1000, Loss: 0.184308722615242\n",
      "Iteration 406/1000, Loss: 0.18414637446403503\n",
      "Iteration 407/1000, Loss: 0.1839844435453415\n",
      "Iteration 408/1000, Loss: 0.18382297456264496\n",
      "Iteration 409/1000, Loss: 0.18366196751594543\n",
      "Iteration 410/1000, Loss: 0.18350137770175934\n",
      "Iteration 411/1000, Loss: 0.18334124982357025\n",
      "Iteration 412/1000, Loss: 0.1831815093755722\n",
      "Iteration 413/1000, Loss: 0.18302223086357117\n",
      "Iteration 414/1000, Loss: 0.18286339938640594\n",
      "Iteration 415/1000, Loss: 0.18270498514175415\n",
      "Iteration 416/1000, Loss: 0.18254701793193817\n",
      "Iteration 417/1000, Loss: 0.182389497756958\n",
      "Iteration 418/1000, Loss: 0.1822323501110077\n",
      "Iteration 419/1000, Loss: 0.1820756047964096\n",
      "Iteration 420/1000, Loss: 0.18191929161548615\n",
      "Iteration 421/1000, Loss: 0.18176338076591492\n",
      "Iteration 422/1000, Loss: 0.18160784244537354\n",
      "Iteration 423/1000, Loss: 0.181452676653862\n",
      "Iteration 424/1000, Loss: 0.1812979280948639\n",
      "Iteration 425/1000, Loss: 0.18114355206489563\n",
      "Iteration 426/1000, Loss: 0.18098962306976318\n",
      "Iteration 427/1000, Loss: 0.1808360368013382\n",
      "Iteration 428/1000, Loss: 0.18068283796310425\n",
      "Iteration 429/1000, Loss: 0.18053007125854492\n",
      "Iteration 430/1000, Loss: 0.18037772178649902\n",
      "Iteration 431/1000, Loss: 0.18022581934928894\n",
      "Iteration 432/1000, Loss: 0.18007434904575348\n",
      "Iteration 433/1000, Loss: 0.17992320656776428\n",
      "Iteration 434/1000, Loss: 0.1797724813222885\n",
      "Iteration 435/1000, Loss: 0.17962217330932617\n",
      "Iteration 436/1000, Loss: 0.17947226762771606\n",
      "Iteration 437/1000, Loss: 0.179322749376297\n",
      "Iteration 438/1000, Loss: 0.17917363345623016\n",
      "Iteration 439/1000, Loss: 0.17902489006519318\n",
      "Iteration 440/1000, Loss: 0.17887656390666962\n",
      "Iteration 441/1000, Loss: 0.1787286400794983\n",
      "Iteration 442/1000, Loss: 0.1785810887813568\n",
      "Iteration 443/1000, Loss: 0.17843395471572876\n",
      "Iteration 444/1000, Loss: 0.17828716337680817\n",
      "Iteration 445/1000, Loss: 0.17814074456691742\n",
      "Iteration 446/1000, Loss: 0.1779947131872177\n",
      "Iteration 447/1000, Loss: 0.17784903943538666\n",
      "Iteration 448/1000, Loss: 0.17770370841026306\n",
      "Iteration 449/1000, Loss: 0.1775587499141693\n",
      "Iteration 450/1000, Loss: 0.1774141937494278\n",
      "Iteration 451/1000, Loss: 0.17726998031139374\n",
      "Iteration 452/1000, Loss: 0.17712616920471191\n",
      "Iteration 453/1000, Loss: 0.17698274552822113\n",
      "Iteration 454/1000, Loss: 0.17683960497379303\n",
      "Iteration 455/1000, Loss: 0.17669683694839478\n",
      "Iteration 456/1000, Loss: 0.17655448615550995\n",
      "Iteration 457/1000, Loss: 0.17641247808933258\n",
      "Iteration 458/1000, Loss: 0.17627082765102386\n",
      "Iteration 459/1000, Loss: 0.17612949013710022\n",
      "Iteration 460/1000, Loss: 0.17598852515220642\n",
      "Iteration 461/1000, Loss: 0.1758478730916977\n",
      "Iteration 462/1000, Loss: 0.17570756375789642\n",
      "Iteration 463/1000, Loss: 0.1755675971508026\n",
      "Iteration 464/1000, Loss: 0.17542795836925507\n",
      "Iteration 465/1000, Loss: 0.17528870701789856\n",
      "Iteration 466/1000, Loss: 0.17514976859092712\n",
      "Iteration 467/1000, Loss: 0.17501114308834076\n",
      "Iteration 468/1000, Loss: 0.17487281560897827\n",
      "Iteration 469/1000, Loss: 0.17473486065864563\n",
      "Iteration 470/1000, Loss: 0.17459717392921448\n",
      "Iteration 471/1000, Loss: 0.17445982992649078\n",
      "Iteration 472/1000, Loss: 0.17432281374931335\n",
      "Iteration 473/1000, Loss: 0.1741860806941986\n",
      "Iteration 474/1000, Loss: 0.17404967546463013\n",
      "Iteration 475/1000, Loss: 0.1739136129617691\n",
      "Iteration 476/1000, Loss: 0.17377789318561554\n",
      "Iteration 477/1000, Loss: 0.17364248633384705\n",
      "Iteration 478/1000, Loss: 0.17350737750530243\n",
      "Iteration 479/1000, Loss: 0.17337259650230408\n",
      "Iteration 480/1000, Loss: 0.17323808372020721\n",
      "Iteration 481/1000, Loss: 0.1731039136648178\n",
      "Iteration 482/1000, Loss: 0.17297008633613586\n",
      "Iteration 483/1000, Loss: 0.17283658683300018\n",
      "Iteration 484/1000, Loss: 0.17270338535308838\n",
      "Iteration 485/1000, Loss: 0.17257049679756165\n",
      "Iteration 486/1000, Loss: 0.17243792116641998\n",
      "Iteration 487/1000, Loss: 0.1723056137561798\n",
      "Iteration 488/1000, Loss: 0.17217357456684113\n",
      "Iteration 489/1000, Loss: 0.17204177379608154\n",
      "Iteration 490/1000, Loss: 0.17191027104854584\n",
      "Iteration 491/1000, Loss: 0.17177902162075043\n",
      "Iteration 492/1000, Loss: 0.17164810001850128\n",
      "Iteration 493/1000, Loss: 0.171517476439476\n",
      "Iteration 494/1000, Loss: 0.171387180685997\n",
      "Iteration 495/1000, Loss: 0.1712571680545807\n",
      "Iteration 496/1000, Loss: 0.17112740874290466\n",
      "Iteration 497/1000, Loss: 0.17099794745445251\n",
      "Iteration 498/1000, Loss: 0.17086881399154663\n",
      "Iteration 499/1000, Loss: 0.17073996365070343\n",
      "Iteration 500/1000, Loss: 0.1706114113330841\n",
      "Iteration 501/1000, Loss: 0.17048317193984985\n",
      "Iteration 502/1000, Loss: 0.1703551709651947\n",
      "Iteration 503/1000, Loss: 0.17022743821144104\n",
      "Iteration 504/1000, Loss: 0.17010000348091125\n",
      "Iteration 505/1000, Loss: 0.16997285187244415\n",
      "Iteration 506/1000, Loss: 0.16984599828720093\n",
      "Iteration 507/1000, Loss: 0.1697193682193756\n",
      "Iteration 508/1000, Loss: 0.16959300637245178\n",
      "Iteration 509/1000, Loss: 0.16946686804294586\n",
      "Iteration 510/1000, Loss: 0.16934099793434143\n",
      "Iteration 511/1000, Loss: 0.16921542584896088\n",
      "Iteration 512/1000, Loss: 0.1690901666879654\n",
      "Iteration 513/1000, Loss: 0.16896513104438782\n",
      "Iteration 514/1000, Loss: 0.16884039342403412\n",
      "Iteration 515/1000, Loss: 0.16871590912342072\n",
      "Iteration 516/1000, Loss: 0.16859173774719238\n",
      "Iteration 517/1000, Loss: 0.16846777498722076\n",
      "Iteration 518/1000, Loss: 0.16834408044815063\n",
      "Iteration 519/1000, Loss: 0.16822068393230438\n",
      "Iteration 520/1000, Loss: 0.16809755563735962\n",
      "Iteration 521/1000, Loss: 0.16797468066215515\n",
      "Iteration 522/1000, Loss: 0.16785204410552979\n",
      "Iteration 523/1000, Loss: 0.1677296757698059\n",
      "Iteration 524/1000, Loss: 0.16760759055614471\n",
      "Iteration 525/1000, Loss: 0.167485773563385\n",
      "Iteration 526/1000, Loss: 0.16736425459384918\n",
      "Iteration 527/1000, Loss: 0.16724298894405365\n",
      "Iteration 528/1000, Loss: 0.1671219766139984\n",
      "Iteration 529/1000, Loss: 0.1670011729001999\n",
      "Iteration 530/1000, Loss: 0.16688062250614166\n",
      "Iteration 531/1000, Loss: 0.16676028072834015\n",
      "Iteration 532/1000, Loss: 0.1666402369737625\n",
      "Iteration 533/1000, Loss: 0.1665203720331192\n",
      "Iteration 534/1000, Loss: 0.1664007157087326\n",
      "Iteration 535/1000, Loss: 0.1662813127040863\n",
      "Iteration 536/1000, Loss: 0.16616219282150269\n",
      "Iteration 537/1000, Loss: 0.16604332625865936\n",
      "Iteration 538/1000, Loss: 0.16592474281787872\n",
      "Iteration 539/1000, Loss: 0.1658063679933548\n",
      "Iteration 540/1000, Loss: 0.16568823158740997\n",
      "Iteration 541/1000, Loss: 0.16557036340236664\n",
      "Iteration 542/1000, Loss: 0.16545270383358002\n",
      "Iteration 543/1000, Loss: 0.1653352975845337\n",
      "Iteration 544/1000, Loss: 0.16521812975406647\n",
      "Iteration 545/1000, Loss: 0.16510120034217834\n",
      "Iteration 546/1000, Loss: 0.16498450934886932\n",
      "Iteration 547/1000, Loss: 0.1648680716753006\n",
      "Iteration 548/1000, Loss: 0.1647518277168274\n",
      "Iteration 549/1000, Loss: 0.1646358221769333\n",
      "Iteration 550/1000, Loss: 0.1645200550556183\n",
      "Iteration 551/1000, Loss: 0.16440452635288239\n",
      "Iteration 552/1000, Loss: 0.1642892062664032\n",
      "Iteration 553/1000, Loss: 0.1641741544008255\n",
      "Iteration 554/1000, Loss: 0.1640593260526657\n",
      "Iteration 555/1000, Loss: 0.16394473612308502\n",
      "Iteration 556/1000, Loss: 0.16383038461208344\n",
      "Iteration 557/1000, Loss: 0.16371622681617737\n",
      "Iteration 558/1000, Loss: 0.1636023223400116\n",
      "Iteration 559/1000, Loss: 0.16348864138126373\n",
      "Iteration 560/1000, Loss: 0.16337519884109497\n",
      "Iteration 561/1000, Loss: 0.16326196491718292\n",
      "Iteration 562/1000, Loss: 0.16314898431301117\n",
      "Iteration 563/1000, Loss: 0.16303624212741852\n",
      "Iteration 564/1000, Loss: 0.16292372345924377\n",
      "Iteration 565/1000, Loss: 0.16281144320964813\n",
      "Iteration 566/1000, Loss: 0.1626993864774704\n",
      "Iteration 567/1000, Loss: 0.16258755326271057\n",
      "Iteration 568/1000, Loss: 0.16247591376304626\n",
      "Iteration 569/1000, Loss: 0.16236449778079987\n",
      "Iteration 570/1000, Loss: 0.16225330531597137\n",
      "Iteration 571/1000, Loss: 0.16214224696159363\n",
      "Iteration 572/1000, Loss: 0.16203142702579498\n",
      "Iteration 573/1000, Loss: 0.16192081570625305\n",
      "Iteration 574/1000, Loss: 0.16181041300296783\n",
      "Iteration 575/1000, Loss: 0.16170021891593933\n",
      "Iteration 576/1000, Loss: 0.16159021854400635\n",
      "Iteration 577/1000, Loss: 0.16148047149181366\n",
      "Iteration 578/1000, Loss: 0.1613709032535553\n",
      "Iteration 579/1000, Loss: 0.16126155853271484\n",
      "Iteration 580/1000, Loss: 0.1611524075269699\n",
      "Iteration 581/1000, Loss: 0.16104352474212646\n",
      "Iteration 582/1000, Loss: 0.16093482077121735\n",
      "Iteration 583/1000, Loss: 0.16082634031772614\n",
      "Iteration 584/1000, Loss: 0.16071806848049164\n",
      "Iteration 585/1000, Loss: 0.16061002016067505\n",
      "Iteration 586/1000, Loss: 0.16050218045711517\n",
      "Iteration 587/1000, Loss: 0.16039451956748962\n",
      "Iteration 588/1000, Loss: 0.1602870225906372\n",
      "Iteration 589/1000, Loss: 0.1601797640323639\n",
      "Iteration 590/1000, Loss: 0.1600727140903473\n",
      "Iteration 591/1000, Loss: 0.1599659025669098\n",
      "Iteration 592/1000, Loss: 0.159859299659729\n",
      "Iteration 593/1000, Loss: 0.15975289046764374\n",
      "Iteration 594/1000, Loss: 0.15964668989181519\n",
      "Iteration 595/1000, Loss: 0.15954066812992096\n",
      "Iteration 596/1000, Loss: 0.15943484008312225\n",
      "Iteration 597/1000, Loss: 0.15932925045490265\n",
      "Iteration 598/1000, Loss: 0.1592237949371338\n",
      "Iteration 599/1000, Loss: 0.15911859273910522\n",
      "Iteration 600/1000, Loss: 0.159013569355011\n",
      "Iteration 601/1000, Loss: 0.15890876948833466\n",
      "Iteration 602/1000, Loss: 0.15880416333675385\n",
      "Iteration 603/1000, Loss: 0.15869976580142975\n",
      "Iteration 604/1000, Loss: 0.15859554708003998\n",
      "Iteration 605/1000, Loss: 0.15849147737026215\n",
      "Iteration 606/1000, Loss: 0.15838763117790222\n",
      "Iteration 607/1000, Loss: 0.15828397870063782\n",
      "Iteration 608/1000, Loss: 0.15818051993846893\n",
      "Iteration 609/1000, Loss: 0.1580771952867508\n",
      "Iteration 610/1000, Loss: 0.15797404944896698\n",
      "Iteration 611/1000, Loss: 0.1578710973262787\n",
      "Iteration 612/1000, Loss: 0.15776832401752472\n",
      "Iteration 613/1000, Loss: 0.15766578912734985\n",
      "Iteration 614/1000, Loss: 0.15756337344646454\n",
      "Iteration 615/1000, Loss: 0.15746116638183594\n",
      "Iteration 616/1000, Loss: 0.15735915303230286\n",
      "Iteration 617/1000, Loss: 0.1572573184967041\n",
      "Iteration 618/1000, Loss: 0.15715566277503967\n",
      "Iteration 619/1000, Loss: 0.15705420076847076\n",
      "Iteration 620/1000, Loss: 0.156952902674675\n",
      "Iteration 621/1000, Loss: 0.15685181319713593\n",
      "Iteration 622/1000, Loss: 0.15675091743469238\n",
      "Iteration 623/1000, Loss: 0.15665015578269958\n",
      "Iteration 624/1000, Loss: 0.1565495878458023\n",
      "Iteration 625/1000, Loss: 0.15644918382167816\n",
      "Iteration 626/1000, Loss: 0.15634895861148834\n",
      "Iteration 627/1000, Loss: 0.15624889731407166\n",
      "Iteration 628/1000, Loss: 0.1561489850282669\n",
      "Iteration 629/1000, Loss: 0.15604925155639648\n",
      "Iteration 630/1000, Loss: 0.155949667096138\n",
      "Iteration 631/1000, Loss: 0.15585027635097504\n",
      "Iteration 632/1000, Loss: 0.155751034617424\n",
      "Iteration 633/1000, Loss: 0.15565195679664612\n",
      "Iteration 634/1000, Loss: 0.15555302798748016\n",
      "Iteration 635/1000, Loss: 0.15545423328876495\n",
      "Iteration 636/1000, Loss: 0.15535563230514526\n",
      "Iteration 637/1000, Loss: 0.1552572101354599\n",
      "Iteration 638/1000, Loss: 0.15515896677970886\n",
      "Iteration 639/1000, Loss: 0.15506093204021454\n",
      "Iteration 640/1000, Loss: 0.15496309101581573\n",
      "Iteration 641/1000, Loss: 0.15486542880535126\n",
      "Iteration 642/1000, Loss: 0.15476787090301514\n",
      "Iteration 643/1000, Loss: 0.15467050671577454\n",
      "Iteration 644/1000, Loss: 0.15457327663898468\n",
      "Iteration 645/1000, Loss: 0.15447621047496796\n",
      "Iteration 646/1000, Loss: 0.15437926352024078\n",
      "Iteration 647/1000, Loss: 0.15428248047828674\n",
      "Iteration 648/1000, Loss: 0.15418584644794464\n",
      "Iteration 649/1000, Loss: 0.15408936142921448\n",
      "Iteration 650/1000, Loss: 0.15399298071861267\n",
      "Iteration 651/1000, Loss: 0.15389679372310638\n",
      "Iteration 652/1000, Loss: 0.15380075573921204\n",
      "Iteration 653/1000, Loss: 0.15370486676692963\n",
      "Iteration 654/1000, Loss: 0.15360911190509796\n",
      "Iteration 655/1000, Loss: 0.15351353585720062\n",
      "Iteration 656/1000, Loss: 0.15341809391975403\n",
      "Iteration 657/1000, Loss: 0.15332278609275818\n",
      "Iteration 658/1000, Loss: 0.15322764217853546\n",
      "Iteration 659/1000, Loss: 0.1531326323747635\n",
      "Iteration 660/1000, Loss: 0.15303783118724823\n",
      "Iteration 661/1000, Loss: 0.15294316411018372\n",
      "Iteration 662/1000, Loss: 0.15284863114356995\n",
      "Iteration 663/1000, Loss: 0.15275423228740692\n",
      "Iteration 664/1000, Loss: 0.15265999734401703\n",
      "Iteration 665/1000, Loss: 0.1525658518075943\n",
      "Iteration 666/1000, Loss: 0.1524718701839447\n",
      "Iteration 667/1000, Loss: 0.15237800776958466\n",
      "Iteration 668/1000, Loss: 0.15228432416915894\n",
      "Iteration 669/1000, Loss: 0.15219078958034515\n",
      "Iteration 670/1000, Loss: 0.1520974338054657\n",
      "Iteration 671/1000, Loss: 0.15200427174568176\n",
      "Iteration 672/1000, Loss: 0.15191125869750977\n",
      "Iteration 673/1000, Loss: 0.1518184095621109\n",
      "Iteration 674/1000, Loss: 0.15172570943832397\n",
      "Iteration 675/1000, Loss: 0.15163320302963257\n",
      "Iteration 676/1000, Loss: 0.1515408158302307\n",
      "Iteration 677/1000, Loss: 0.15144862234592438\n",
      "Iteration 678/1000, Loss: 0.15135657787322998\n",
      "Iteration 679/1000, Loss: 0.15126465260982513\n",
      "Iteration 680/1000, Loss: 0.15117284655570984\n",
      "Iteration 681/1000, Loss: 0.15108121931552887\n",
      "Iteration 682/1000, Loss: 0.15098971128463745\n",
      "Iteration 683/1000, Loss: 0.15089835226535797\n",
      "Iteration 684/1000, Loss: 0.15080715715885162\n",
      "Iteration 685/1000, Loss: 0.1507161408662796\n",
      "Iteration 686/1000, Loss: 0.15062525868415833\n",
      "Iteration 687/1000, Loss: 0.1505344957113266\n",
      "Iteration 688/1000, Loss: 0.150443896651268\n",
      "Iteration 689/1000, Loss: 0.15035341680049896\n",
      "Iteration 690/1000, Loss: 0.15026307106018066\n",
      "Iteration 691/1000, Loss: 0.1501728743314743\n",
      "Iteration 692/1000, Loss: 0.1500828117132187\n",
      "Iteration 693/1000, Loss: 0.14999288320541382\n",
      "Iteration 694/1000, Loss: 0.1499030590057373\n",
      "Iteration 695/1000, Loss: 0.14981341361999512\n",
      "Iteration 696/1000, Loss: 0.14972393214702606\n",
      "Iteration 697/1000, Loss: 0.14963458478450775\n",
      "Iteration 698/1000, Loss: 0.14954543113708496\n",
      "Iteration 699/1000, Loss: 0.14945638179779053\n",
      "Iteration 700/1000, Loss: 0.14936748147010803\n",
      "Iteration 701/1000, Loss: 0.1492786556482315\n",
      "Iteration 702/1000, Loss: 0.14918997883796692\n",
      "Iteration 703/1000, Loss: 0.14910143613815308\n",
      "Iteration 704/1000, Loss: 0.14901308715343475\n",
      "Iteration 705/1000, Loss: 0.1489248424768448\n",
      "Iteration 706/1000, Loss: 0.14883674681186676\n",
      "Iteration 707/1000, Loss: 0.14874882996082306\n",
      "Iteration 708/1000, Loss: 0.1486610323190689\n",
      "Iteration 709/1000, Loss: 0.1485733985900879\n",
      "Iteration 710/1000, Loss: 0.14848589897155762\n",
      "Iteration 711/1000, Loss: 0.1483985334634781\n",
      "Iteration 712/1000, Loss: 0.1483113318681717\n",
      "Iteration 713/1000, Loss: 0.14822427928447723\n",
      "Iteration 714/1000, Loss: 0.14813733100891113\n",
      "Iteration 715/1000, Loss: 0.14805054664611816\n",
      "Iteration 716/1000, Loss: 0.14796389639377594\n",
      "Iteration 717/1000, Loss: 0.14787738025188446\n",
      "Iteration 718/1000, Loss: 0.14779096841812134\n",
      "Iteration 719/1000, Loss: 0.14770467579364777\n",
      "Iteration 720/1000, Loss: 0.14761851727962494\n",
      "Iteration 721/1000, Loss: 0.14753252267837524\n",
      "Iteration 722/1000, Loss: 0.1474466174840927\n",
      "Iteration 723/1000, Loss: 0.14736084640026093\n",
      "Iteration 724/1000, Loss: 0.1472751945257187\n",
      "Iteration 725/1000, Loss: 0.14718970656394958\n",
      "Iteration 726/1000, Loss: 0.14710429310798645\n",
      "Iteration 727/1000, Loss: 0.14701901376247406\n",
      "Iteration 728/1000, Loss: 0.14693385362625122\n",
      "Iteration 729/1000, Loss: 0.14684882760047913\n",
      "Iteration 730/1000, Loss: 0.14676399528980255\n",
      "Iteration 731/1000, Loss: 0.14667929708957672\n",
      "Iteration 732/1000, Loss: 0.14659468829631805\n",
      "Iteration 733/1000, Loss: 0.1465102583169937\n",
      "Iteration 734/1000, Loss: 0.14642593264579773\n",
      "Iteration 735/1000, Loss: 0.1463417261838913\n",
      "Iteration 736/1000, Loss: 0.1462576538324356\n",
      "Iteration 737/1000, Loss: 0.14617368578910828\n",
      "Iteration 738/1000, Loss: 0.14608988165855408\n",
      "Iteration 739/1000, Loss: 0.14600618183612823\n",
      "Iteration 740/1000, Loss: 0.14592263102531433\n",
      "Iteration 741/1000, Loss: 0.14583921432495117\n",
      "Iteration 742/1000, Loss: 0.14575593173503876\n",
      "Iteration 743/1000, Loss: 0.1456727534532547\n",
      "Iteration 744/1000, Loss: 0.14558972418308258\n",
      "Iteration 745/1000, Loss: 0.1455068588256836\n",
      "Iteration 746/1000, Loss: 0.14542408287525177\n",
      "Iteration 747/1000, Loss: 0.1453414112329483\n",
      "Iteration 748/1000, Loss: 0.1452588587999344\n",
      "Iteration 749/1000, Loss: 0.14517642557621002\n",
      "Iteration 750/1000, Loss: 0.1450941115617752\n",
      "Iteration 751/1000, Loss: 0.14501190185546875\n",
      "Iteration 752/1000, Loss: 0.14492981135845184\n",
      "Iteration 753/1000, Loss: 0.14484785497188568\n",
      "Iteration 754/1000, Loss: 0.14476603269577026\n",
      "Iteration 755/1000, Loss: 0.1446843147277832\n",
      "Iteration 756/1000, Loss: 0.1446027159690857\n",
      "Iteration 757/1000, Loss: 0.14452123641967773\n",
      "Iteration 758/1000, Loss: 0.1444399058818817\n",
      "Iteration 759/1000, Loss: 0.14435872435569763\n",
      "Iteration 760/1000, Loss: 0.1442776322364807\n",
      "Iteration 761/1000, Loss: 0.14419661462306976\n",
      "Iteration 762/1000, Loss: 0.14411574602127075\n",
      "Iteration 763/1000, Loss: 0.14403502643108368\n",
      "Iteration 764/1000, Loss: 0.14395439624786377\n",
      "Iteration 765/1000, Loss: 0.1438739001750946\n",
      "Iteration 766/1000, Loss: 0.1437934935092926\n",
      "Iteration 767/1000, Loss: 0.14371317625045776\n",
      "Iteration 768/1000, Loss: 0.14363297820091248\n",
      "Iteration 769/1000, Loss: 0.14355291426181793\n",
      "Iteration 770/1000, Loss: 0.14347296953201294\n",
      "Iteration 771/1000, Loss: 0.1433931291103363\n",
      "Iteration 772/1000, Loss: 0.14331340789794922\n",
      "Iteration 773/1000, Loss: 0.1432337909936905\n",
      "Iteration 774/1000, Loss: 0.14315424859523773\n",
      "Iteration 775/1000, Loss: 0.14307484030723572\n",
      "Iteration 776/1000, Loss: 0.14299553632736206\n",
      "Iteration 777/1000, Loss: 0.14291635155677795\n",
      "Iteration 778/1000, Loss: 0.1428373008966446\n",
      "Iteration 779/1000, Loss: 0.1427583545446396\n",
      "Iteration 780/1000, Loss: 0.14267952740192413\n",
      "Iteration 781/1000, Loss: 0.14260083436965942\n",
      "Iteration 782/1000, Loss: 0.14252223074436188\n",
      "Iteration 783/1000, Loss: 0.14244377613067627\n",
      "Iteration 784/1000, Loss: 0.14236542582511902\n",
      "Iteration 785/1000, Loss: 0.14228716492652893\n",
      "Iteration 786/1000, Loss: 0.1422089785337448\n",
      "Iteration 787/1000, Loss: 0.14213092625141144\n",
      "Iteration 788/1000, Loss: 0.14205296337604523\n",
      "Iteration 789/1000, Loss: 0.14197513461112976\n",
      "Iteration 790/1000, Loss: 0.14189738035202026\n",
      "Iteration 791/1000, Loss: 0.1418197602033615\n",
      "Iteration 792/1000, Loss: 0.1417422741651535\n",
      "Iteration 793/1000, Loss: 0.14166490733623505\n",
      "Iteration 794/1000, Loss: 0.14158762991428375\n",
      "Iteration 795/1000, Loss: 0.1415104866027832\n",
      "Iteration 796/1000, Loss: 0.14143343269824982\n",
      "Iteration 797/1000, Loss: 0.14135651290416718\n",
      "Iteration 798/1000, Loss: 0.14127972722053528\n",
      "Iteration 799/1000, Loss: 0.14120303094387054\n",
      "Iteration 800/1000, Loss: 0.14112645387649536\n",
      "Iteration 801/1000, Loss: 0.14104996621608734\n",
      "Iteration 802/1000, Loss: 0.14097359776496887\n",
      "Iteration 803/1000, Loss: 0.14089733362197876\n",
      "Iteration 804/1000, Loss: 0.14082114398479462\n",
      "Iteration 805/1000, Loss: 0.14074508845806122\n",
      "Iteration 806/1000, Loss: 0.14066912233829498\n",
      "Iteration 807/1000, Loss: 0.1405932903289795\n",
      "Iteration 808/1000, Loss: 0.14051754772663116\n",
      "Iteration 809/1000, Loss: 0.14044189453125\n",
      "Iteration 810/1000, Loss: 0.1403663605451584\n",
      "Iteration 811/1000, Loss: 0.14029094576835632\n",
      "Iteration 812/1000, Loss: 0.14021562039852142\n",
      "Iteration 813/1000, Loss: 0.1401403844356537\n",
      "Iteration 814/1000, Loss: 0.1400652378797531\n",
      "Iteration 815/1000, Loss: 0.1399901956319809\n",
      "Iteration 816/1000, Loss: 0.13991527259349823\n",
      "Iteration 817/1000, Loss: 0.13984039425849915\n",
      "Iteration 818/1000, Loss: 0.1397656351327896\n",
      "Iteration 819/1000, Loss: 0.13969098031520844\n",
      "Iteration 820/1000, Loss: 0.1396164447069168\n",
      "Iteration 821/1000, Loss: 0.13954199850559235\n",
      "Iteration 822/1000, Loss: 0.13946764171123505\n",
      "Iteration 823/1000, Loss: 0.1393933743238449\n",
      "Iteration 824/1000, Loss: 0.13931921124458313\n",
      "Iteration 825/1000, Loss: 0.13924512267112732\n",
      "Iteration 826/1000, Loss: 0.13917113840579987\n",
      "Iteration 827/1000, Loss: 0.13909724354743958\n",
      "Iteration 828/1000, Loss: 0.13902345299720764\n",
      "Iteration 829/1000, Loss: 0.13894975185394287\n",
      "Iteration 830/1000, Loss: 0.13887616991996765\n",
      "Iteration 831/1000, Loss: 0.1388026624917984\n",
      "Iteration 832/1000, Loss: 0.13872922956943512\n",
      "Iteration 833/1000, Loss: 0.13865593075752258\n",
      "Iteration 834/1000, Loss: 0.1385827511548996\n",
      "Iteration 835/1000, Loss: 0.1385096311569214\n",
      "Iteration 836/1000, Loss: 0.13843661546707153\n",
      "Iteration 837/1000, Loss: 0.13836370408535004\n",
      "Iteration 838/1000, Loss: 0.1382908821105957\n",
      "Iteration 839/1000, Loss: 0.13821814954280853\n",
      "Iteration 840/1000, Loss: 0.13814547657966614\n",
      "Iteration 841/1000, Loss: 0.1380729377269745\n",
      "Iteration 842/1000, Loss: 0.1380004733800888\n",
      "Iteration 843/1000, Loss: 0.13792811334133148\n",
      "Iteration 844/1000, Loss: 0.13785585761070251\n",
      "Iteration 845/1000, Loss: 0.1377836912870407\n",
      "Iteration 846/1000, Loss: 0.13771164417266846\n",
      "Iteration 847/1000, Loss: 0.13763964176177979\n",
      "Iteration 848/1000, Loss: 0.13756777346134186\n",
      "Iteration 849/1000, Loss: 0.1374960094690323\n",
      "Iteration 850/1000, Loss: 0.13742434978485107\n",
      "Iteration 851/1000, Loss: 0.13735279440879822\n",
      "Iteration 852/1000, Loss: 0.13728132843971252\n",
      "Iteration 853/1000, Loss: 0.137209951877594\n",
      "Iteration 854/1000, Loss: 0.13713863492012024\n",
      "Iteration 855/1000, Loss: 0.13706742227077484\n",
      "Iteration 856/1000, Loss: 0.1369962990283966\n",
      "Iteration 857/1000, Loss: 0.13692526519298553\n",
      "Iteration 858/1000, Loss: 0.13685433566570282\n",
      "Iteration 859/1000, Loss: 0.13678348064422607\n",
      "Iteration 860/1000, Loss: 0.1367127150297165\n",
      "Iteration 861/1000, Loss: 0.13664202392101288\n",
      "Iteration 862/1000, Loss: 0.13657145202159882\n",
      "Iteration 863/1000, Loss: 0.13650096952915192\n",
      "Iteration 864/1000, Loss: 0.13643057644367218\n",
      "Iteration 865/1000, Loss: 0.1363602578639984\n",
      "Iteration 866/1000, Loss: 0.1362900286912918\n",
      "Iteration 867/1000, Loss: 0.13621987402439117\n",
      "Iteration 868/1000, Loss: 0.1361498087644577\n",
      "Iteration 869/1000, Loss: 0.1360798329114914\n",
      "Iteration 870/1000, Loss: 0.13600993156433105\n",
      "Iteration 871/1000, Loss: 0.13594011962413788\n",
      "Iteration 872/1000, Loss: 0.13587039709091187\n",
      "Iteration 873/1000, Loss: 0.13580076396465302\n",
      "Iteration 874/1000, Loss: 0.13573122024536133\n",
      "Iteration 875/1000, Loss: 0.1356617659330368\n",
      "Iteration 876/1000, Loss: 0.13559243083000183\n",
      "Iteration 877/1000, Loss: 0.13552317023277283\n",
      "Iteration 878/1000, Loss: 0.135453999042511\n",
      "Iteration 879/1000, Loss: 0.1353849470615387\n",
      "Iteration 880/1000, Loss: 0.13531596958637238\n",
      "Iteration 881/1000, Loss: 0.1352470964193344\n",
      "Iteration 882/1000, Loss: 0.13517829775810242\n",
      "Iteration 883/1000, Loss: 0.1351095587015152\n",
      "Iteration 884/1000, Loss: 0.13504092395305634\n",
      "Iteration 885/1000, Loss: 0.13497233390808105\n",
      "Iteration 886/1000, Loss: 0.13490383327007294\n",
      "Iteration 887/1000, Loss: 0.13483543694019318\n",
      "Iteration 888/1000, Loss: 0.1347671002149582\n",
      "Iteration 889/1000, Loss: 0.13469886779785156\n",
      "Iteration 890/1000, Loss: 0.1346306949853897\n",
      "Iteration 891/1000, Loss: 0.13456258177757263\n",
      "Iteration 892/1000, Loss: 0.13449454307556152\n",
      "Iteration 893/1000, Loss: 0.13442659378051758\n",
      "Iteration 894/1000, Loss: 0.13435876369476318\n",
      "Iteration 895/1000, Loss: 0.13429097831249237\n",
      "Iteration 896/1000, Loss: 0.13422326743602753\n",
      "Iteration 897/1000, Loss: 0.13415566086769104\n",
      "Iteration 898/1000, Loss: 0.13408812880516052\n",
      "Iteration 899/1000, Loss: 0.13402067124843597\n",
      "Iteration 900/1000, Loss: 0.1339532881975174\n",
      "Iteration 901/1000, Loss: 0.13388600945472717\n",
      "Iteration 902/1000, Loss: 0.1338188350200653\n",
      "Iteration 903/1000, Loss: 0.13375172019004822\n",
      "Iteration 904/1000, Loss: 0.13368470966815948\n",
      "Iteration 905/1000, Loss: 0.13361777365207672\n",
      "Iteration 906/1000, Loss: 0.13355091214179993\n",
      "Iteration 907/1000, Loss: 0.1334841251373291\n",
      "Iteration 908/1000, Loss: 0.13341742753982544\n",
      "Iteration 909/1000, Loss: 0.13335083425045013\n",
      "Iteration 910/1000, Loss: 0.1332843154668808\n",
      "Iteration 911/1000, Loss: 0.13321787118911743\n",
      "Iteration 912/1000, Loss: 0.13315150141716003\n",
      "Iteration 913/1000, Loss: 0.1330852210521698\n",
      "Iteration 914/1000, Loss: 0.13301903009414673\n",
      "Iteration 915/1000, Loss: 0.13295292854309082\n",
      "Iteration 916/1000, Loss: 0.13288690149784088\n",
      "Iteration 917/1000, Loss: 0.13282093405723572\n",
      "Iteration 918/1000, Loss: 0.13275504112243652\n",
      "Iteration 919/1000, Loss: 0.1326892226934433\n",
      "Iteration 920/1000, Loss: 0.13262344896793365\n",
      "Iteration 921/1000, Loss: 0.13255774974822998\n",
      "Iteration 922/1000, Loss: 0.13249212503433228\n",
      "Iteration 923/1000, Loss: 0.13242657482624054\n",
      "Iteration 924/1000, Loss: 0.13236109912395477\n",
      "Iteration 925/1000, Loss: 0.13229569792747498\n",
      "Iteration 926/1000, Loss: 0.13223035633563995\n",
      "Iteration 927/1000, Loss: 0.1321650892496109\n",
      "Iteration 928/1000, Loss: 0.132099911570549\n",
      "Iteration 929/1000, Loss: 0.1320348083972931\n",
      "Iteration 930/1000, Loss: 0.13196979463100433\n",
      "Iteration 931/1000, Loss: 0.13190487027168274\n",
      "Iteration 932/1000, Loss: 0.1318400353193283\n",
      "Iteration 933/1000, Loss: 0.13177528977394104\n",
      "Iteration 934/1000, Loss: 0.13171063363552094\n",
      "Iteration 935/1000, Loss: 0.1316460371017456\n",
      "Iteration 936/1000, Loss: 0.13158152997493744\n",
      "Iteration 937/1000, Loss: 0.13151711225509644\n",
      "Iteration 938/1000, Loss: 0.131452739238739\n",
      "Iteration 939/1000, Loss: 0.13138847053050995\n",
      "Iteration 940/1000, Loss: 0.13132424652576447\n",
      "Iteration 941/1000, Loss: 0.13126011192798615\n",
      "Iteration 942/1000, Loss: 0.1311960369348526\n",
      "Iteration 943/1000, Loss: 0.13113205134868622\n",
      "Iteration 944/1000, Loss: 0.131068155169487\n",
      "Iteration 945/1000, Loss: 0.13100430369377136\n",
      "Iteration 946/1000, Loss: 0.1309404969215393\n",
      "Iteration 947/1000, Loss: 0.1308767944574356\n",
      "Iteration 948/1000, Loss: 0.13081316649913788\n",
      "Iteration 949/1000, Loss: 0.13074961304664612\n",
      "Iteration 950/1000, Loss: 0.13068611919879913\n",
      "Iteration 951/1000, Loss: 0.13062269985675812\n",
      "Iteration 952/1000, Loss: 0.13055934011936188\n",
      "Iteration 953/1000, Loss: 0.1304960399866104\n",
      "Iteration 954/1000, Loss: 0.1304328292608261\n",
      "Iteration 955/1000, Loss: 0.13036970794200897\n",
      "Iteration 956/1000, Loss: 0.1303066611289978\n",
      "Iteration 957/1000, Loss: 0.1302437037229538\n",
      "Iteration 958/1000, Loss: 0.13018080592155457\n",
      "Iteration 959/1000, Loss: 0.1301179975271225\n",
      "Iteration 960/1000, Loss: 0.1300552487373352\n",
      "Iteration 961/1000, Loss: 0.1299925595521927\n",
      "Iteration 962/1000, Loss: 0.12992994487285614\n",
      "Iteration 963/1000, Loss: 0.12986740469932556\n",
      "Iteration 964/1000, Loss: 0.12980490922927856\n",
      "Iteration 965/1000, Loss: 0.12974250316619873\n",
      "Iteration 966/1000, Loss: 0.12968015670776367\n",
      "Iteration 967/1000, Loss: 0.1296178698539734\n",
      "Iteration 968/1000, Loss: 0.12955564260482788\n",
      "Iteration 969/1000, Loss: 0.12949350476264954\n",
      "Iteration 970/1000, Loss: 0.12943141162395477\n",
      "Iteration 971/1000, Loss: 0.12936942279338837\n",
      "Iteration 972/1000, Loss: 0.12930746376514435\n",
      "Iteration 973/1000, Loss: 0.1292456090450287\n",
      "Iteration 974/1000, Loss: 0.129183828830719\n",
      "Iteration 975/1000, Loss: 0.12912210822105408\n",
      "Iteration 976/1000, Loss: 0.12906044721603394\n",
      "Iteration 977/1000, Loss: 0.12899884581565857\n",
      "Iteration 978/1000, Loss: 0.12893731892108917\n",
      "Iteration 979/1000, Loss: 0.12887586653232574\n",
      "Iteration 980/1000, Loss: 0.1288144886493683\n",
      "Iteration 981/1000, Loss: 0.1287531703710556\n",
      "Iteration 982/1000, Loss: 0.1286919116973877\n",
      "Iteration 983/1000, Loss: 0.12863072752952576\n",
      "Iteration 984/1000, Loss: 0.1285696029663086\n",
      "Iteration 985/1000, Loss: 0.12850850820541382\n",
      "Iteration 986/1000, Loss: 0.1284475028514862\n",
      "Iteration 987/1000, Loss: 0.12838655710220337\n",
      "Iteration 988/1000, Loss: 0.1283256858587265\n",
      "Iteration 989/1000, Loss: 0.1282648742198944\n",
      "Iteration 990/1000, Loss: 0.1282041370868683\n",
      "Iteration 991/1000, Loss: 0.12814348936080933\n",
      "Iteration 992/1000, Loss: 0.12808290123939514\n",
      "Iteration 993/1000, Loss: 0.12802235782146454\n",
      "Iteration 994/1000, Loss: 0.1279618889093399\n",
      "Iteration 995/1000, Loss: 0.12790147960186005\n",
      "Iteration 996/1000, Loss: 0.12784111499786377\n",
      "Iteration 997/1000, Loss: 0.12778083980083466\n",
      "Iteration 998/1000, Loss: 0.12772060930728912\n",
      "Iteration 999/1000, Loss: 0.12766043841838837\n",
      "Iteration 1000/1000, Loss: 0.12760034203529358\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    \"\"\"\n",
    "    Function used to run the full lottery ticket experiment.\n",
    "    \"\"\"\n",
    "    make_dataset: callable = load_and_process_mnist\n",
    "    train_model: callable = functools.partial(train, iterations=C.TRAINING_ITERATIONS)\n",
    "    prune_masks: callable = functools.partial(prune_by_percent, C.PRUNING_PERCENTS)\n",
    "\n",
    "    for i in range(C.NUM_MODELS):\n",
    "        make_model: callable = functools.partial(LeNet300, i)\n",
    "        experiment(make_dataset, make_model, train_model, prune_masks, C.TEST_PRUNING_STEPS)\n",
    "\n",
    "run_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
