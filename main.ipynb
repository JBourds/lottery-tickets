{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_experiments' from 'src.harness.experiment' (/Users/jordan/Projects/lottery-tickets/src/harness/experiment.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mharness\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants \u001b[38;5;28;01mas\u001b[39;00m C\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# from src.harness.dataset import download_data, load_and_process_mnist\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mharness\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExperimentSummary, ExperimentData, run_experiments\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'run_experiments' from 'src.harness.experiment' (/Users/jordan/Projects/lottery-tickets/src/harness/experiment.py)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "main.ipynb\n",
    "\n",
    "Main file for recreating lottery ticket experiments done in randomly initialized dense neural networks.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.harness import constants as C\n",
    "# from src.harness.dataset import download_data, load_and_process_mnist\n",
    "from src.harness.experiment import ExperimentSummary, ExperimentData, run_experiments\n",
    "# from src.harness.model import create_model, LeNet300, load_model\n",
    "# from src.harness.pruning import prune_by_percent\n",
    "# from src.harness.training import train\n",
    "# from src.lottery_ticket.foundations import paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Step 0\n",
      "Iteration 1/100, Loss: 2.3527424335479736\n",
      "Iteration 2/100, Loss: 2.2643282413482666\n",
      "Iteration 3/100, Loss: 2.1938443183898926\n",
      "Iteration 4/100, Loss: 2.129702568054199\n",
      "Iteration 5/100, Loss: 2.067338705062866\n",
      "Iteration 6/100, Loss: 2.0048041343688965\n",
      "Iteration 7/100, Loss: 1.9412062168121338\n",
      "Iteration 8/100, Loss: 1.876131534576416\n",
      "Iteration 9/100, Loss: 1.8094804286956787\n",
      "Iteration 10/100, Loss: 1.7416300773620605\n",
      "Iteration 11/100, Loss: 1.673186182975769\n",
      "Iteration 12/100, Loss: 1.6049703359603882\n",
      "Iteration 13/100, Loss: 1.537709355354309\n",
      "Iteration 14/100, Loss: 1.4719213247299194\n",
      "Iteration 15/100, Loss: 1.4081066846847534\n",
      "Iteration 16/100, Loss: 1.3467572927474976\n",
      "Iteration 17/100, Loss: 1.2881991863250732\n",
      "Iteration 18/100, Loss: 1.2326465845108032\n",
      "Iteration 19/100, Loss: 1.1802276372909546\n",
      "Iteration 20/100, Loss: 1.131006121635437\n",
      "Iteration 21/100, Loss: 1.0849409103393555\n",
      "Iteration 22/100, Loss: 1.0419408082962036\n",
      "Iteration 23/100, Loss: 1.0019105672836304\n",
      "Iteration 24/100, Loss: 0.9647563695907593\n",
      "Iteration 25/100, Loss: 0.9303327202796936\n",
      "Iteration 26/100, Loss: 0.8984748721122742\n",
      "Iteration 27/100, Loss: 0.8690072298049927\n",
      "Iteration 28/100, Loss: 0.8417454957962036\n",
      "Iteration 29/100, Loss: 0.8165230751037598\n",
      "Iteration 30/100, Loss: 0.7931435108184814\n",
      "Iteration 31/100, Loss: 0.7714393734931946\n",
      "Iteration 32/100, Loss: 0.7512730360031128\n",
      "Iteration 33/100, Loss: 0.732506513595581\n",
      "Iteration 34/100, Loss: 0.7150156497955322\n",
      "Iteration 35/100, Loss: 0.6986902356147766\n",
      "Iteration 36/100, Loss: 0.6834225654602051\n",
      "Iteration 37/100, Loss: 0.6691274046897888\n",
      "Iteration 38/100, Loss: 0.6557150483131409\n",
      "Iteration 39/100, Loss: 0.6431124210357666\n",
      "Iteration 40/100, Loss: 0.6312538981437683\n",
      "Iteration 41/100, Loss: 0.6200771927833557\n",
      "Iteration 42/100, Loss: 0.6095278859138489\n",
      "Iteration 43/100, Loss: 0.5995569229125977\n",
      "Iteration 44/100, Loss: 0.5901191234588623\n",
      "Iteration 45/100, Loss: 0.5811737179756165\n",
      "Iteration 46/100, Loss: 0.5726860761642456\n",
      "Iteration 47/100, Loss: 0.5646217465400696\n",
      "Iteration 48/100, Loss: 0.5569495558738708\n",
      "Iteration 49/100, Loss: 0.5496412515640259\n",
      "Iteration 50/100, Loss: 0.542673647403717\n",
      "Iteration 51/100, Loss: 0.5360227823257446\n",
      "Iteration 52/100, Loss: 0.5296670198440552\n",
      "Iteration 53/100, Loss: 0.523588240146637\n",
      "Iteration 54/100, Loss: 0.5177682638168335\n",
      "Iteration 55/100, Loss: 0.5121914148330688\n",
      "Iteration 56/100, Loss: 0.5068413019180298\n",
      "Iteration 57/100, Loss: 0.5017041563987732\n",
      "Iteration 58/100, Loss: 0.4967685639858246\n",
      "Iteration 59/100, Loss: 0.49202099442481995\n",
      "Iteration 60/100, Loss: 0.48745131492614746\n",
      "Iteration 61/100, Loss: 0.48304876685142517\n",
      "Iteration 62/100, Loss: 0.478805273771286\n",
      "Iteration 63/100, Loss: 0.4747123718261719\n",
      "Iteration 64/100, Loss: 0.47076159715652466\n",
      "Iteration 65/100, Loss: 0.4669451117515564\n",
      "Iteration 66/100, Loss: 0.4632544219493866\n",
      "Iteration 67/100, Loss: 0.45968419313430786\n",
      "Iteration 68/100, Loss: 0.4562290906906128\n",
      "Iteration 69/100, Loss: 0.4528826177120209\n",
      "Iteration 70/100, Loss: 0.44963952898979187\n",
      "Iteration 71/100, Loss: 0.44649484753608704\n",
      "Iteration 72/100, Loss: 0.4434443414211273\n",
      "Iteration 73/100, Loss: 0.4404820203781128\n",
      "Iteration 74/100, Loss: 0.4376033842563629\n",
      "Iteration 75/100, Loss: 0.4348062574863434\n",
      "Iteration 76/100, Loss: 0.43208643794059753\n",
      "Iteration 77/100, Loss: 0.4294417202472687\n",
      "Iteration 78/100, Loss: 0.42686763405799866\n",
      "Iteration 79/100, Loss: 0.42436203360557556\n",
      "Iteration 80/100, Loss: 0.42192158102989197\n",
      "Iteration 81/100, Loss: 0.419542521238327\n",
      "Iteration 82/100, Loss: 0.41722363233566284\n",
      "Iteration 83/100, Loss: 0.414962500333786\n",
      "Iteration 84/100, Loss: 0.41275671124458313\n",
      "Iteration 85/100, Loss: 0.4106033146381378\n",
      "Iteration 86/100, Loss: 0.4085007309913635\n",
      "Iteration 87/100, Loss: 0.40644708275794983\n",
      "Iteration 88/100, Loss: 0.4044402539730072\n",
      "Iteration 89/100, Loss: 0.4024789333343506\n",
      "Iteration 90/100, Loss: 0.40056097507476807\n",
      "Iteration 91/100, Loss: 0.39868462085723877\n",
      "Iteration 92/100, Loss: 0.39684855937957764\n",
      "Iteration 93/100, Loss: 0.39505141973495483\n",
      "Iteration 94/100, Loss: 0.39329153299331665\n",
      "Iteration 95/100, Loss: 0.3915669918060303\n",
      "Iteration 96/100, Loss: 0.3898766338825226\n",
      "Iteration 97/100, Loss: 0.3882189989089966\n",
      "Iteration 98/100, Loss: 0.386593759059906\n",
      "Iteration 99/100, Loss: 0.38499948382377625\n",
      "Iteration 100/100, Loss: 0.3834352195262909\n",
      "Pruning Step 1\n",
      "Iteration 1/100, Loss: 2.155895471572876\n",
      "Iteration 2/100, Loss: 2.0569987297058105\n",
      "Iteration 3/100, Loss: 1.9710819721221924\n",
      "Iteration 4/100, Loss: 1.8897364139556885\n",
      "Iteration 5/100, Loss: 1.8106821775436401\n",
      "Iteration 6/100, Loss: 1.7331866025924683\n",
      "Iteration 7/100, Loss: 1.6572171449661255\n",
      "Iteration 8/100, Loss: 1.583172082901001\n",
      "Iteration 9/100, Loss: 1.511461615562439\n",
      "Iteration 10/100, Loss: 1.4425002336502075\n",
      "Iteration 11/100, Loss: 1.376602053642273\n",
      "Iteration 12/100, Loss: 1.313982605934143\n",
      "Iteration 13/100, Loss: 1.2548669576644897\n",
      "Iteration 14/100, Loss: 1.1993354558944702\n",
      "Iteration 15/100, Loss: 1.1474441289901733\n",
      "Iteration 16/100, Loss: 1.0991727113723755\n",
      "Iteration 17/100, Loss: 1.054413080215454\n",
      "Iteration 18/100, Loss: 1.0130224227905273\n",
      "Iteration 19/100, Loss: 0.9747627377510071\n",
      "Iteration 20/100, Loss: 0.9394233226776123\n",
      "Iteration 21/100, Loss: 0.9067980647087097\n",
      "Iteration 22/100, Loss: 0.876673698425293\n",
      "Iteration 23/100, Loss: 0.8488315343856812\n",
      "Iteration 24/100, Loss: 0.8230875730514526\n",
      "Iteration 25/100, Loss: 0.7992566227912903\n",
      "Iteration 26/100, Loss: 0.7771598100662231\n",
      "Iteration 27/100, Loss: 0.7566407918930054\n",
      "Iteration 28/100, Loss: 0.737560510635376\n",
      "Iteration 29/100, Loss: 0.7197970747947693\n",
      "Iteration 30/100, Loss: 0.703231155872345\n",
      "Iteration 31/100, Loss: 0.6877583265304565\n",
      "Iteration 32/100, Loss: 0.6732833981513977\n",
      "Iteration 33/100, Loss: 0.6597172021865845\n",
      "Iteration 34/100, Loss: 0.6469786167144775\n",
      "Iteration 35/100, Loss: 0.6349986791610718\n",
      "Iteration 36/100, Loss: 0.6237185001373291\n",
      "Iteration 37/100, Loss: 0.6130810976028442\n",
      "Iteration 38/100, Loss: 0.6030356884002686\n",
      "Iteration 39/100, Loss: 0.5935325026512146\n",
      "Iteration 40/100, Loss: 0.5845307111740112\n",
      "Iteration 41/100, Loss: 0.5759943127632141\n",
      "Iteration 42/100, Loss: 0.5678892135620117\n",
      "Iteration 43/100, Loss: 0.5601840615272522\n",
      "Iteration 44/100, Loss: 0.5528472661972046\n",
      "Iteration 45/100, Loss: 0.545853316783905\n",
      "Iteration 46/100, Loss: 0.5391796827316284\n",
      "Iteration 47/100, Loss: 0.5328044295310974\n",
      "Iteration 48/100, Loss: 0.5267090201377869\n",
      "Iteration 49/100, Loss: 0.5208755731582642\n",
      "Iteration 50/100, Loss: 0.5152860879898071\n",
      "Iteration 51/100, Loss: 0.5099254846572876\n",
      "Iteration 52/100, Loss: 0.504780113697052\n",
      "Iteration 53/100, Loss: 0.49983757734298706\n",
      "Iteration 54/100, Loss: 0.4950871765613556\n",
      "Iteration 55/100, Loss: 0.4905157685279846\n",
      "Iteration 56/100, Loss: 0.4861132800579071\n",
      "Iteration 57/100, Loss: 0.4818718731403351\n",
      "Iteration 58/100, Loss: 0.4777810275554657\n",
      "Iteration 59/100, Loss: 0.47383177280426025\n",
      "Iteration 60/100, Loss: 0.47001808881759644\n",
      "Iteration 61/100, Loss: 0.4663326144218445\n",
      "Iteration 62/100, Loss: 0.46276840567588806\n",
      "Iteration 63/100, Loss: 0.45931991934776306\n",
      "Iteration 64/100, Loss: 0.4559815227985382\n",
      "Iteration 65/100, Loss: 0.4527473449707031\n",
      "Iteration 66/100, Loss: 0.44961217045783997\n",
      "Iteration 67/100, Loss: 0.44657102227211\n",
      "Iteration 68/100, Loss: 0.4436191916465759\n",
      "Iteration 69/100, Loss: 0.44075319170951843\n",
      "Iteration 70/100, Loss: 0.4379686117172241\n",
      "Iteration 71/100, Loss: 0.43526196479797363\n",
      "Iteration 72/100, Loss: 0.4326295554637909\n",
      "Iteration 73/100, Loss: 0.430067241191864\n",
      "Iteration 74/100, Loss: 0.4275728464126587\n",
      "Iteration 75/100, Loss: 0.4251440465450287\n",
      "Iteration 76/100, Loss: 0.42277806997299194\n",
      "Iteration 77/100, Loss: 0.4204716682434082\n",
      "Iteration 78/100, Loss: 0.41822248697280884\n",
      "Iteration 79/100, Loss: 0.4160279631614685\n",
      "Iteration 80/100, Loss: 0.4138864576816559\n",
      "Iteration 81/100, Loss: 0.41179585456848145\n",
      "Iteration 82/100, Loss: 0.4097534120082855\n",
      "Iteration 83/100, Loss: 0.40775761008262634\n",
      "Iteration 84/100, Loss: 0.4058069586753845\n",
      "Iteration 85/100, Loss: 0.40389931201934814\n",
      "Iteration 86/100, Loss: 0.40203315019607544\n",
      "Iteration 87/100, Loss: 0.40020737051963806\n",
      "Iteration 88/100, Loss: 0.39842015504837036\n",
      "Iteration 89/100, Loss: 0.3966705799102783\n",
      "Iteration 90/100, Loss: 0.39495649933815\n",
      "Iteration 91/100, Loss: 0.3932769298553467\n",
      "Iteration 92/100, Loss: 0.39163026213645935\n",
      "Iteration 93/100, Loss: 0.39001595973968506\n",
      "Iteration 94/100, Loss: 0.3884331285953522\n",
      "Iteration 95/100, Loss: 0.38688039779663086\n",
      "Iteration 96/100, Loss: 0.3853565454483032\n",
      "Iteration 97/100, Loss: 0.3838611841201782\n",
      "Iteration 98/100, Loss: 0.38239291310310364\n",
      "Iteration 99/100, Loss: 0.3809513747692108\n",
      "Iteration 100/100, Loss: 0.37953534722328186\n",
      "Pruning Step 2\n",
      "Iteration 1/100, Loss: 1.9659929275512695\n",
      "Iteration 2/100, Loss: 1.8555617332458496\n",
      "Iteration 3/100, Loss: 1.7619080543518066\n",
      "Iteration 4/100, Loss: 1.6752872467041016\n",
      "Iteration 5/100, Loss: 1.5933808088302612\n",
      "Iteration 6/100, Loss: 1.515670657157898\n",
      "Iteration 7/100, Loss: 1.4421634674072266\n",
      "Iteration 8/100, Loss: 1.3728848695755005\n",
      "Iteration 9/100, Loss: 1.3079190254211426\n",
      "Iteration 10/100, Loss: 1.247305154800415\n",
      "Iteration 11/100, Loss: 1.1909542083740234\n",
      "Iteration 12/100, Loss: 1.138757348060608\n",
      "Iteration 13/100, Loss: 1.090527057647705\n",
      "Iteration 14/100, Loss: 1.046029806137085\n",
      "Iteration 15/100, Loss: 1.0050160884857178\n",
      "Iteration 16/100, Loss: 0.9672286510467529\n",
      "Iteration 17/100, Loss: 0.9324312210083008\n",
      "Iteration 18/100, Loss: 0.9003661274909973\n",
      "Iteration 19/100, Loss: 0.8707988262176514\n",
      "Iteration 20/100, Loss: 0.8435150980949402\n",
      "Iteration 21/100, Loss: 0.8183053135871887\n",
      "Iteration 22/100, Loss: 0.7949728965759277\n",
      "Iteration 23/100, Loss: 0.77335125207901\n",
      "Iteration 24/100, Loss: 0.7532753944396973\n",
      "Iteration 25/100, Loss: 0.7346118688583374\n",
      "Iteration 26/100, Loss: 0.7172278761863708\n",
      "Iteration 27/100, Loss: 0.7010117173194885\n",
      "Iteration 28/100, Loss: 0.6858615875244141\n",
      "Iteration 29/100, Loss: 0.6716819405555725\n",
      "Iteration 30/100, Loss: 0.6583877801895142\n",
      "Iteration 31/100, Loss: 0.645905613899231\n",
      "Iteration 32/100, Loss: 0.6341691613197327\n",
      "Iteration 33/100, Loss: 0.623115062713623\n",
      "Iteration 34/100, Loss: 0.6126870512962341\n",
      "Iteration 35/100, Loss: 0.6028366088867188\n",
      "Iteration 36/100, Loss: 0.593518078327179\n",
      "Iteration 37/100, Loss: 0.5846915245056152\n",
      "Iteration 38/100, Loss: 0.57631915807724\n",
      "Iteration 39/100, Loss: 0.5683680176734924\n",
      "Iteration 40/100, Loss: 0.560806930065155\n",
      "Iteration 41/100, Loss: 0.5536069273948669\n",
      "Iteration 42/100, Loss: 0.5467437505722046\n",
      "Iteration 43/100, Loss: 0.5401946902275085\n",
      "Iteration 44/100, Loss: 0.5339402556419373\n",
      "Iteration 45/100, Loss: 0.5279577970504761\n",
      "Iteration 46/100, Loss: 0.522232174873352\n",
      "Iteration 47/100, Loss: 0.5167456269264221\n",
      "Iteration 48/100, Loss: 0.5114847421646118\n",
      "Iteration 49/100, Loss: 0.5064343214035034\n",
      "Iteration 50/100, Loss: 0.5015817284584045\n",
      "Iteration 51/100, Loss: 0.4969155788421631\n",
      "Iteration 52/100, Loss: 0.4924241900444031\n",
      "Iteration 53/100, Loss: 0.4880993068218231\n",
      "Iteration 54/100, Loss: 0.48392996191978455\n",
      "Iteration 55/100, Loss: 0.4799080789089203\n",
      "Iteration 56/100, Loss: 0.4760255813598633\n",
      "Iteration 57/100, Loss: 0.4722754657268524\n",
      "Iteration 58/100, Loss: 0.46865037083625793\n",
      "Iteration 59/100, Loss: 0.46514368057250977\n",
      "Iteration 60/100, Loss: 0.46175020933151245\n",
      "Iteration 61/100, Loss: 0.4584643244743347\n",
      "Iteration 62/100, Loss: 0.45528021454811096\n",
      "Iteration 63/100, Loss: 0.4521927833557129\n",
      "Iteration 64/100, Loss: 0.44919803738594055\n",
      "Iteration 65/100, Loss: 0.4462912678718567\n",
      "Iteration 66/100, Loss: 0.4434686303138733\n",
      "Iteration 67/100, Loss: 0.4407261610031128\n",
      "Iteration 68/100, Loss: 0.43806010484695435\n",
      "Iteration 69/100, Loss: 0.4354669153690338\n",
      "Iteration 70/100, Loss: 0.43294376134872437\n",
      "Iteration 71/100, Loss: 0.4304882287979126\n",
      "Iteration 72/100, Loss: 0.428096741437912\n",
      "Iteration 73/100, Loss: 0.42576658725738525\n",
      "Iteration 74/100, Loss: 0.42349499464035034\n",
      "Iteration 75/100, Loss: 0.4212800860404968\n",
      "Iteration 76/100, Loss: 0.4191194772720337\n",
      "Iteration 77/100, Loss: 0.4170103371143341\n",
      "Iteration 78/100, Loss: 0.4149504005908966\n",
      "Iteration 79/100, Loss: 0.4129377603530884\n",
      "Iteration 80/100, Loss: 0.4109712541103363\n",
      "Iteration 81/100, Loss: 0.40904849767684937\n",
      "Iteration 82/100, Loss: 0.40716803073883057\n",
      "Iteration 83/100, Loss: 0.4053287208080292\n",
      "Iteration 84/100, Loss: 0.40352851152420044\n",
      "Iteration 85/100, Loss: 0.40176624059677124\n",
      "Iteration 86/100, Loss: 0.4000405967235565\n",
      "Iteration 87/100, Loss: 0.39835038781166077\n",
      "Iteration 88/100, Loss: 0.3966940641403198\n",
      "Iteration 89/100, Loss: 0.39507097005844116\n",
      "Iteration 90/100, Loss: 0.3934795558452606\n",
      "Iteration 91/100, Loss: 0.3919190764427185\n",
      "Iteration 92/100, Loss: 0.39038828015327454\n",
      "Iteration 93/100, Loss: 0.3888862729072571\n",
      "Iteration 94/100, Loss: 0.38741230964660645\n",
      "Iteration 95/100, Loss: 0.3859652280807495\n",
      "Iteration 96/100, Loss: 0.38454383611679077\n",
      "Iteration 97/100, Loss: 0.38314729928970337\n",
      "Iteration 98/100, Loss: 0.38177475333213806\n",
      "Iteration 99/100, Loss: 0.3804257810115814\n",
      "Iteration 100/100, Loss: 0.3790993094444275\n",
      "Pruning Step 3\n",
      "Iteration 1/100, Loss: 1.7713515758514404\n",
      "Iteration 2/100, Loss: 1.6633814573287964\n",
      "Iteration 3/100, Loss: 1.574923038482666\n",
      "Iteration 4/100, Loss: 1.495078444480896\n",
      "Iteration 5/100, Loss: 1.4212698936462402\n",
      "Iteration 6/100, Loss: 1.3526415824890137\n",
      "Iteration 7/100, Loss: 1.288864016532898\n",
      "Iteration 8/100, Loss: 1.2297619581222534\n",
      "Iteration 9/100, Loss: 1.1751320362091064\n",
      "Iteration 10/100, Loss: 1.1247100830078125\n",
      "Iteration 11/100, Loss: 1.0782150030136108\n",
      "Iteration 12/100, Loss: 1.0353959798812866\n",
      "Iteration 13/100, Loss: 0.995959997177124\n",
      "Iteration 14/100, Loss: 0.9596384167671204\n",
      "Iteration 15/100, Loss: 0.9261763095855713\n",
      "Iteration 16/100, Loss: 0.895327627658844\n",
      "Iteration 17/100, Loss: 0.8668388724327087\n",
      "Iteration 18/100, Loss: 0.8405130505561829\n",
      "Iteration 19/100, Loss: 0.8161497116088867\n",
      "Iteration 20/100, Loss: 0.7935725450515747\n",
      "Iteration 21/100, Loss: 0.7726143002510071\n",
      "Iteration 22/100, Loss: 0.7531248331069946\n",
      "Iteration 23/100, Loss: 0.7349718809127808\n",
      "Iteration 24/100, Loss: 0.7180359363555908\n",
      "Iteration 25/100, Loss: 0.7022156119346619\n",
      "Iteration 26/100, Loss: 0.6874098777770996\n",
      "Iteration 27/100, Loss: 0.6735327243804932\n",
      "Iteration 28/100, Loss: 0.6605077981948853\n",
      "Iteration 29/100, Loss: 0.6482611894607544\n",
      "Iteration 30/100, Loss: 0.6367273330688477\n",
      "Iteration 31/100, Loss: 0.6258497834205627\n",
      "Iteration 32/100, Loss: 0.6155759692192078\n",
      "Iteration 33/100, Loss: 0.605858564376831\n",
      "Iteration 34/100, Loss: 0.5966575741767883\n",
      "Iteration 35/100, Loss: 0.5879324078559875\n",
      "Iteration 36/100, Loss: 0.5796462297439575\n",
      "Iteration 37/100, Loss: 0.5717687010765076\n",
      "Iteration 38/100, Loss: 0.564271092414856\n",
      "Iteration 39/100, Loss: 0.557128369808197\n",
      "Iteration 40/100, Loss: 0.5503143072128296\n",
      "Iteration 41/100, Loss: 0.5438071489334106\n",
      "Iteration 42/100, Loss: 0.5375871658325195\n",
      "Iteration 43/100, Loss: 0.5316354632377625\n",
      "Iteration 44/100, Loss: 0.5259354710578918\n",
      "Iteration 45/100, Loss: 0.5204716920852661\n",
      "Iteration 46/100, Loss: 0.5152299404144287\n",
      "Iteration 47/100, Loss: 0.5101972222328186\n",
      "Iteration 48/100, Loss: 0.5053617358207703\n",
      "Iteration 49/100, Loss: 0.5007114410400391\n",
      "Iteration 50/100, Loss: 0.4962359666824341\n",
      "Iteration 51/100, Loss: 0.4919254183769226\n",
      "Iteration 52/100, Loss: 0.4877704083919525\n",
      "Iteration 53/100, Loss: 0.4837625026702881\n",
      "Iteration 54/100, Loss: 0.47989290952682495\n",
      "Iteration 55/100, Loss: 0.47615450620651245\n",
      "Iteration 56/100, Loss: 0.47254082560539246\n",
      "Iteration 57/100, Loss: 0.4690452814102173\n",
      "Iteration 58/100, Loss: 0.4656623303890228\n",
      "Iteration 59/100, Loss: 0.46238553524017334\n",
      "Iteration 60/100, Loss: 0.4592105448246002\n",
      "Iteration 61/100, Loss: 0.456132173538208\n",
      "Iteration 62/100, Loss: 0.4531460702419281\n",
      "Iteration 63/100, Loss: 0.4502476453781128\n",
      "Iteration 64/100, Loss: 0.44743260741233826\n",
      "Iteration 65/100, Loss: 0.44469812512397766\n",
      "Iteration 66/100, Loss: 0.44204089045524597\n",
      "Iteration 67/100, Loss: 0.4394570291042328\n",
      "Iteration 68/100, Loss: 0.4369429051876068\n",
      "Iteration 69/100, Loss: 0.4344956576824188\n",
      "Iteration 70/100, Loss: 0.43211209774017334\n",
      "Iteration 71/100, Loss: 0.42978939414024353\n",
      "Iteration 72/100, Loss: 0.4275255799293518\n",
      "Iteration 73/100, Loss: 0.42531803250312805\n",
      "Iteration 74/100, Loss: 0.4231642782688141\n",
      "Iteration 75/100, Loss: 0.42106229066848755\n",
      "Iteration 76/100, Loss: 0.41901007294654846\n",
      "Iteration 77/100, Loss: 0.4170058071613312\n",
      "Iteration 78/100, Loss: 0.4150480329990387\n",
      "Iteration 79/100, Loss: 0.41313478350639343\n",
      "Iteration 80/100, Loss: 0.4112647771835327\n",
      "Iteration 81/100, Loss: 0.4094362258911133\n",
      "Iteration 82/100, Loss: 0.4076477289199829\n",
      "Iteration 83/100, Loss: 0.40589743852615356\n",
      "Iteration 84/100, Loss: 0.4041842520236969\n",
      "Iteration 85/100, Loss: 0.40250664949417114\n",
      "Iteration 86/100, Loss: 0.4008631408214569\n",
      "Iteration 87/100, Loss: 0.3992527425289154\n",
      "Iteration 88/100, Loss: 0.3976741433143616\n",
      "Iteration 89/100, Loss: 0.39612624049186707\n",
      "Iteration 90/100, Loss: 0.3946080803871155\n",
      "Iteration 91/100, Loss: 0.39311841130256653\n",
      "Iteration 92/100, Loss: 0.39165645837783813\n",
      "Iteration 93/100, Loss: 0.39022159576416016\n",
      "Iteration 94/100, Loss: 0.3888125419616699\n",
      "Iteration 95/100, Loss: 0.38742879033088684\n",
      "Iteration 96/100, Loss: 0.3860698640346527\n",
      "Iteration 97/100, Loss: 0.38473501801490784\n",
      "Iteration 98/100, Loss: 0.3834236264228821\n",
      "Iteration 99/100, Loss: 0.38213449716567993\n",
      "Iteration 100/100, Loss: 0.3808670938014984\n",
      "Pruning Step 4\n",
      "Iteration 1/100, Loss: 1.6168279647827148\n",
      "Iteration 2/100, Loss: 1.510752558708191\n",
      "Iteration 3/100, Loss: 1.429283618927002\n",
      "Iteration 4/100, Loss: 1.3580121994018555\n",
      "Iteration 5/100, Loss: 1.2935463190078735\n",
      "Iteration 6/100, Loss: 1.2345607280731201\n",
      "Iteration 7/100, Loss: 1.1804372072219849\n",
      "Iteration 8/100, Loss: 1.1306815147399902\n",
      "Iteration 9/100, Loss: 1.0849298238754272\n",
      "Iteration 10/100, Loss: 1.042839765548706\n",
      "Iteration 11/100, Loss: 1.0040721893310547\n",
      "Iteration 12/100, Loss: 0.9683485627174377\n",
      "Iteration 13/100, Loss: 0.9354164600372314\n",
      "Iteration 14/100, Loss: 0.9050067663192749\n",
      "Iteration 15/100, Loss: 0.8768978118896484\n",
      "Iteration 16/100, Loss: 0.850875198841095\n",
      "Iteration 17/100, Loss: 0.826749324798584\n",
      "Iteration 18/100, Loss: 0.8043478727340698\n",
      "Iteration 19/100, Loss: 0.7835140824317932\n",
      "Iteration 20/100, Loss: 0.7641078233718872\n",
      "Iteration 21/100, Loss: 0.7460079193115234\n",
      "Iteration 22/100, Loss: 0.7290993332862854\n",
      "Iteration 23/100, Loss: 0.7132784724235535\n",
      "Iteration 24/100, Loss: 0.6984475255012512\n",
      "Iteration 25/100, Loss: 0.684528112411499\n",
      "Iteration 26/100, Loss: 0.6714445352554321\n",
      "Iteration 27/100, Loss: 0.6591269373893738\n",
      "Iteration 28/100, Loss: 0.6475132703781128\n",
      "Iteration 29/100, Loss: 0.6365479826927185\n",
      "Iteration 30/100, Loss: 0.626183032989502\n",
      "Iteration 31/100, Loss: 0.6163733601570129\n",
      "Iteration 32/100, Loss: 0.6070775985717773\n",
      "Iteration 33/100, Loss: 0.5982595086097717\n",
      "Iteration 34/100, Loss: 0.5898827314376831\n",
      "Iteration 35/100, Loss: 0.5819148421287537\n",
      "Iteration 36/100, Loss: 0.5743274092674255\n",
      "Iteration 37/100, Loss: 0.5670948028564453\n",
      "Iteration 38/100, Loss: 0.5601921081542969\n",
      "Iteration 39/100, Loss: 0.5535984635353088\n",
      "Iteration 40/100, Loss: 0.5472936034202576\n",
      "Iteration 41/100, Loss: 0.5412593483924866\n",
      "Iteration 42/100, Loss: 0.5354787707328796\n",
      "Iteration 43/100, Loss: 0.5299368500709534\n",
      "Iteration 44/100, Loss: 0.5246192812919617\n",
      "Iteration 45/100, Loss: 0.5195121169090271\n",
      "Iteration 46/100, Loss: 0.5146039128303528\n",
      "Iteration 47/100, Loss: 0.5098828077316284\n",
      "Iteration 48/100, Loss: 0.5053384900093079\n",
      "Iteration 49/100, Loss: 0.500960648059845\n",
      "Iteration 50/100, Loss: 0.49674004316329956\n",
      "Iteration 51/100, Loss: 0.492668092250824\n",
      "Iteration 52/100, Loss: 0.4887370467185974\n",
      "Iteration 53/100, Loss: 0.48493990302085876\n",
      "Iteration 54/100, Loss: 0.4812702536582947\n",
      "Iteration 55/100, Loss: 0.47772088646888733\n",
      "Iteration 56/100, Loss: 0.47428542375564575\n",
      "Iteration 57/100, Loss: 0.47095805406570435\n",
      "Iteration 58/100, Loss: 0.46773380041122437\n",
      "Iteration 59/100, Loss: 0.46460801362991333\n",
      "Iteration 60/100, Loss: 0.46157634258270264\n",
      "Iteration 61/100, Loss: 0.4586343765258789\n",
      "Iteration 62/100, Loss: 0.45577824115753174\n",
      "Iteration 63/100, Loss: 0.4530040919780731\n",
      "Iteration 64/100, Loss: 0.45030730962753296\n",
      "Iteration 65/100, Loss: 0.44768473505973816\n",
      "Iteration 66/100, Loss: 0.445133239030838\n",
      "Iteration 67/100, Loss: 0.4426499605178833\n",
      "Iteration 68/100, Loss: 0.44023242592811584\n",
      "Iteration 69/100, Loss: 0.43787750601768494\n",
      "Iteration 70/100, Loss: 0.4355826675891876\n",
      "Iteration 71/100, Loss: 0.43334558606147766\n",
      "Iteration 72/100, Loss: 0.431164026260376\n",
      "Iteration 73/100, Loss: 0.4290355145931244\n",
      "Iteration 74/100, Loss: 0.4269578754901886\n",
      "Iteration 75/100, Loss: 0.42492926120758057\n",
      "Iteration 76/100, Loss: 0.4229481816291809\n",
      "Iteration 77/100, Loss: 0.42101243138313293\n",
      "Iteration 78/100, Loss: 0.4191204607486725\n",
      "Iteration 79/100, Loss: 0.41727063059806824\n",
      "Iteration 80/100, Loss: 0.4154616892337799\n",
      "Iteration 81/100, Loss: 0.41369199752807617\n",
      "Iteration 82/100, Loss: 0.41196003556251526\n",
      "Iteration 83/100, Loss: 0.4102645814418793\n",
      "Iteration 84/100, Loss: 0.4086046516895294\n",
      "Iteration 85/100, Loss: 0.4069788455963135\n",
      "Iteration 86/100, Loss: 0.40538594126701355\n",
      "Iteration 87/100, Loss: 0.4038245379924774\n",
      "Iteration 88/100, Loss: 0.40229332447052\n",
      "Iteration 89/100, Loss: 0.40079134702682495\n",
      "Iteration 90/100, Loss: 0.3993179500102997\n",
      "Iteration 91/100, Loss: 0.39787203073501587\n",
      "Iteration 92/100, Loss: 0.3964526355266571\n",
      "Iteration 93/100, Loss: 0.3950593173503876\n",
      "Iteration 94/100, Loss: 0.39369088411331177\n",
      "Iteration 95/100, Loss: 0.3923465609550476\n",
      "Iteration 96/100, Loss: 0.39102548360824585\n",
      "Iteration 97/100, Loss: 0.3897272050380707\n",
      "Iteration 98/100, Loss: 0.3884509205818176\n",
      "Iteration 99/100, Loss: 0.3871960937976837\n",
      "Iteration 100/100, Loss: 0.3859621286392212\n",
      "Pruning Step 5\n",
      "Iteration 1/100, Loss: 1.4671812057495117\n",
      "Iteration 2/100, Loss: 1.377597689628601\n",
      "Iteration 3/100, Loss: 1.3104453086853027\n",
      "Iteration 4/100, Loss: 1.251520037651062\n",
      "Iteration 5/100, Loss: 1.1980047225952148\n",
      "Iteration 6/100, Loss: 1.1488990783691406\n",
      "Iteration 7/100, Loss: 1.103656530380249\n",
      "Iteration 8/100, Loss: 1.061931848526001\n",
      "Iteration 9/100, Loss: 1.0234252214431763\n",
      "Iteration 10/100, Loss: 0.9878438115119934\n",
      "Iteration 11/100, Loss: 0.954924464225769\n",
      "Iteration 12/100, Loss: 0.9244405031204224\n",
      "Iteration 13/100, Loss: 0.8961830139160156\n",
      "Iteration 14/100, Loss: 0.8699625134468079\n",
      "Iteration 15/100, Loss: 0.8455954790115356\n",
      "Iteration 16/100, Loss: 0.8229181170463562\n",
      "Iteration 17/100, Loss: 0.801784336566925\n",
      "Iteration 18/100, Loss: 0.7820634841918945\n",
      "Iteration 19/100, Loss: 0.7636340260505676\n",
      "Iteration 20/100, Loss: 0.7463852763175964\n",
      "Iteration 21/100, Loss: 0.7302186489105225\n",
      "Iteration 22/100, Loss: 0.7150443196296692\n",
      "Iteration 23/100, Loss: 0.7007842659950256\n",
      "Iteration 24/100, Loss: 0.6873652338981628\n",
      "Iteration 25/100, Loss: 0.6747178435325623\n",
      "Iteration 26/100, Loss: 0.6627828478813171\n",
      "Iteration 27/100, Loss: 0.6515069007873535\n",
      "Iteration 28/100, Loss: 0.6408401727676392\n",
      "Iteration 29/100, Loss: 0.6307374238967896\n",
      "Iteration 30/100, Loss: 0.6211572885513306\n",
      "Iteration 31/100, Loss: 0.6120616793632507\n",
      "Iteration 32/100, Loss: 0.6034154891967773\n",
      "Iteration 33/100, Loss: 0.5951874256134033\n",
      "Iteration 34/100, Loss: 0.5873488187789917\n",
      "Iteration 35/100, Loss: 0.5798737406730652\n",
      "Iteration 36/100, Loss: 0.5727382898330688\n",
      "Iteration 37/100, Loss: 0.5659202337265015\n",
      "Iteration 38/100, Loss: 0.5593987107276917\n",
      "Iteration 39/100, Loss: 0.553155779838562\n",
      "Iteration 40/100, Loss: 0.5471752882003784\n",
      "Iteration 41/100, Loss: 0.5414406657218933\n",
      "Iteration 42/100, Loss: 0.5359375476837158\n",
      "Iteration 43/100, Loss: 0.5306527614593506\n",
      "Iteration 44/100, Loss: 0.5255733728408813\n",
      "Iteration 45/100, Loss: 0.5206872820854187\n",
      "Iteration 46/100, Loss: 0.5159841179847717\n",
      "Iteration 47/100, Loss: 0.5114532709121704\n",
      "Iteration 48/100, Loss: 0.5070854425430298\n",
      "Iteration 49/100, Loss: 0.50287264585495\n",
      "Iteration 50/100, Loss: 0.49880632758140564\n",
      "Iteration 51/100, Loss: 0.4948791563510895\n",
      "Iteration 52/100, Loss: 0.49108386039733887\n",
      "Iteration 53/100, Loss: 0.48741307854652405\n",
      "Iteration 54/100, Loss: 0.48386046290397644\n",
      "Iteration 55/100, Loss: 0.48042136430740356\n",
      "Iteration 56/100, Loss: 0.4770900309085846\n",
      "Iteration 57/100, Loss: 0.47386106848716736\n",
      "Iteration 58/100, Loss: 0.4707299470901489\n",
      "Iteration 59/100, Loss: 0.46769219636917114\n",
      "Iteration 60/100, Loss: 0.46474310755729675\n",
      "Iteration 61/100, Loss: 0.46187832951545715\n",
      "Iteration 62/100, Loss: 0.4590950012207031\n",
      "Iteration 63/100, Loss: 0.45638924837112427\n",
      "Iteration 64/100, Loss: 0.4537579417228699\n",
      "Iteration 65/100, Loss: 0.45119810104370117\n",
      "Iteration 66/100, Loss: 0.44870632886886597\n",
      "Iteration 67/100, Loss: 0.4462800920009613\n",
      "Iteration 68/100, Loss: 0.44391682744026184\n",
      "Iteration 69/100, Loss: 0.44161367416381836\n",
      "Iteration 70/100, Loss: 0.4393680691719055\n",
      "Iteration 71/100, Loss: 0.43717747926712036\n",
      "Iteration 72/100, Loss: 0.43504002690315247\n",
      "Iteration 73/100, Loss: 0.4329536557197571\n",
      "Iteration 74/100, Loss: 0.430916428565979\n",
      "Iteration 75/100, Loss: 0.4289262890815735\n",
      "Iteration 76/100, Loss: 0.42698195576667786\n",
      "Iteration 77/100, Loss: 0.4250815808773041\n",
      "Iteration 78/100, Loss: 0.4232235550880432\n",
      "Iteration 79/100, Loss: 0.4214061498641968\n",
      "Iteration 80/100, Loss: 0.4196277856826782\n",
      "Iteration 81/100, Loss: 0.4178871214389801\n",
      "Iteration 82/100, Loss: 0.41618314385414124\n",
      "Iteration 83/100, Loss: 0.4145142436027527\n",
      "Iteration 84/100, Loss: 0.41287949681282043\n",
      "Iteration 85/100, Loss: 0.4112774729728699\n",
      "Iteration 86/100, Loss: 0.40970703959465027\n",
      "Iteration 87/100, Loss: 0.40816745162010193\n",
      "Iteration 88/100, Loss: 0.40665775537490845\n",
      "Iteration 89/100, Loss: 0.40517687797546387\n",
      "Iteration 90/100, Loss: 0.4037238359451294\n",
      "Iteration 91/100, Loss: 0.40229788422584534\n",
      "Iteration 92/100, Loss: 0.40089818835258484\n",
      "Iteration 93/100, Loss: 0.3995237350463867\n",
      "Iteration 94/100, Loss: 0.39817389845848083\n",
      "Iteration 95/100, Loss: 0.39684778451919556\n",
      "Iteration 96/100, Loss: 0.39554479718208313\n",
      "Iteration 97/100, Loss: 0.3942639231681824\n",
      "Iteration 98/100, Loss: 0.39300474524497986\n",
      "Iteration 99/100, Loss: 0.3917666971683502\n",
      "Iteration 100/100, Loss: 0.39054903388023376\n",
      "Pruning Step 6\n",
      "Iteration 1/100, Loss: 1.3561984300613403\n",
      "Iteration 2/100, Loss: 1.2774499654769897\n",
      "Iteration 3/100, Loss: 1.2182316780090332\n",
      "Iteration 4/100, Loss: 1.1672950983047485\n",
      "Iteration 5/100, Loss: 1.121573567390442\n",
      "Iteration 6/100, Loss: 1.079843282699585\n",
      "Iteration 7/100, Loss: 1.04146409034729\n",
      "Iteration 8/100, Loss: 1.0060317516326904\n",
      "Iteration 9/100, Loss: 0.9732441306114197\n",
      "Iteration 10/100, Loss: 0.942855179309845\n",
      "Iteration 11/100, Loss: 0.9146468639373779\n",
      "Iteration 12/100, Loss: 0.888420820236206\n",
      "Iteration 13/100, Loss: 0.8640156984329224\n",
      "Iteration 14/100, Loss: 0.8412656188011169\n",
      "Iteration 15/100, Loss: 0.8200247287750244\n",
      "Iteration 16/100, Loss: 0.800159215927124\n",
      "Iteration 17/100, Loss: 0.7815582156181335\n",
      "Iteration 18/100, Loss: 0.7641145586967468\n",
      "Iteration 19/100, Loss: 0.7477352619171143\n",
      "Iteration 20/100, Loss: 0.7323349714279175\n",
      "Iteration 21/100, Loss: 0.7178350687026978\n",
      "Iteration 22/100, Loss: 0.7041670680046082\n",
      "Iteration 23/100, Loss: 0.6912660598754883\n",
      "Iteration 24/100, Loss: 0.6790714859962463\n",
      "Iteration 25/100, Loss: 0.6675319075584412\n",
      "Iteration 26/100, Loss: 0.6566004157066345\n",
      "Iteration 27/100, Loss: 0.6462326645851135\n",
      "Iteration 28/100, Loss: 0.6363883018493652\n",
      "Iteration 29/100, Loss: 0.627031147480011\n",
      "Iteration 30/100, Loss: 0.61812824010849\n",
      "Iteration 31/100, Loss: 0.6096482872962952\n",
      "Iteration 32/100, Loss: 0.6015636920928955\n",
      "Iteration 33/100, Loss: 0.5938489437103271\n",
      "Iteration 34/100, Loss: 0.5864795446395874\n",
      "Iteration 35/100, Loss: 0.5794333219528198\n",
      "Iteration 36/100, Loss: 0.5726913809776306\n",
      "Iteration 37/100, Loss: 0.5662356615066528\n",
      "Iteration 38/100, Loss: 0.5600472688674927\n",
      "Iteration 39/100, Loss: 0.5541101694107056\n",
      "Iteration 40/100, Loss: 0.5484094023704529\n",
      "Iteration 41/100, Loss: 0.5429331660270691\n",
      "Iteration 42/100, Loss: 0.537668764591217\n",
      "Iteration 43/100, Loss: 0.5326037406921387\n",
      "Iteration 44/100, Loss: 0.5277270078659058\n",
      "Iteration 45/100, Loss: 0.5230278372764587\n",
      "Iteration 46/100, Loss: 0.5184968709945679\n",
      "Iteration 47/100, Loss: 0.5141251683235168\n",
      "Iteration 48/100, Loss: 0.509904682636261\n",
      "Iteration 49/100, Loss: 0.5058277249336243\n",
      "Iteration 50/100, Loss: 0.5018869042396545\n",
      "Iteration 51/100, Loss: 0.4980755150318146\n",
      "Iteration 52/100, Loss: 0.4943869709968567\n",
      "Iteration 53/100, Loss: 0.49081510305404663\n",
      "Iteration 54/100, Loss: 0.48735472559928894\n",
      "Iteration 55/100, Loss: 0.484001100063324\n",
      "Iteration 56/100, Loss: 0.4807492792606354\n",
      "Iteration 57/100, Loss: 0.4775947332382202\n",
      "Iteration 58/100, Loss: 0.4745325744152069\n",
      "Iteration 59/100, Loss: 0.47155866026878357\n",
      "Iteration 60/100, Loss: 0.46866875886917114\n",
      "Iteration 61/100, Loss: 0.4658597707748413\n",
      "Iteration 62/100, Loss: 0.46312791109085083\n",
      "Iteration 63/100, Loss: 0.46046996116638184\n",
      "Iteration 64/100, Loss: 0.45788320899009705\n",
      "Iteration 65/100, Loss: 0.45536479353904724\n",
      "Iteration 66/100, Loss: 0.4529115557670593\n",
      "Iteration 67/100, Loss: 0.45052075386047363\n",
      "Iteration 68/100, Loss: 0.4481904208660126\n",
      "Iteration 69/100, Loss: 0.4459182024002075\n",
      "Iteration 70/100, Loss: 0.44370171427726746\n",
      "Iteration 71/100, Loss: 0.4415386915206909\n",
      "Iteration 72/100, Loss: 0.43942728638648987\n",
      "Iteration 73/100, Loss: 0.43736544251441956\n",
      "Iteration 74/100, Loss: 0.43535101413726807\n",
      "Iteration 75/100, Loss: 0.43338242173194885\n",
      "Iteration 76/100, Loss: 0.43145814538002014\n",
      "Iteration 77/100, Loss: 0.42957642674446106\n",
      "Iteration 78/100, Loss: 0.4277358055114746\n",
      "Iteration 79/100, Loss: 0.4259350001811981\n",
      "Iteration 80/100, Loss: 0.4241725206375122\n",
      "Iteration 81/100, Loss: 0.42244696617126465\n",
      "Iteration 82/100, Loss: 0.4207572042942047\n",
      "Iteration 83/100, Loss: 0.419101744890213\n",
      "Iteration 84/100, Loss: 0.41747987270355225\n",
      "Iteration 85/100, Loss: 0.41589033603668213\n",
      "Iteration 86/100, Loss: 0.4143320322036743\n",
      "Iteration 87/100, Loss: 0.41280397772789\n",
      "Iteration 88/100, Loss: 0.41130518913269043\n",
      "Iteration 89/100, Loss: 0.40983471274375916\n",
      "Iteration 90/100, Loss: 0.4083917737007141\n",
      "Iteration 91/100, Loss: 0.4069753885269165\n",
      "Iteration 92/100, Loss: 0.4055849015712738\n",
      "Iteration 93/100, Loss: 0.40421953797340393\n",
      "Iteration 94/100, Loss: 0.4028784930706024\n",
      "Iteration 95/100, Loss: 0.4015609622001648\n",
      "Iteration 96/100, Loss: 0.40026628971099854\n",
      "Iteration 97/100, Loss: 0.3989936113357544\n",
      "Iteration 98/100, Loss: 0.39774250984191895\n",
      "Iteration 99/100, Loss: 0.39651229977607727\n",
      "Iteration 100/100, Loss: 0.39530235528945923\n",
      "Pruning Step 7\n",
      "Iteration 1/100, Loss: 1.279198169708252\n",
      "Iteration 2/100, Loss: 1.204716444015503\n",
      "Iteration 3/100, Loss: 1.1517585515975952\n",
      "Iteration 4/100, Loss: 1.10788094997406\n",
      "Iteration 5/100, Loss: 1.0691176652908325\n",
      "Iteration 6/100, Loss: 1.0338497161865234\n",
      "Iteration 7/100, Loss: 1.0013229846954346\n",
      "Iteration 8/100, Loss: 0.9711002707481384\n",
      "Iteration 9/100, Loss: 0.9429306387901306\n",
      "Iteration 10/100, Loss: 0.916620135307312\n",
      "Iteration 11/100, Loss: 0.8920028805732727\n",
      "Iteration 12/100, Loss: 0.8689493536949158\n",
      "Iteration 13/100, Loss: 0.8473297953605652\n",
      "Iteration 14/100, Loss: 0.8270338773727417\n",
      "Iteration 15/100, Loss: 0.8079606294631958\n",
      "Iteration 16/100, Loss: 0.790018618106842\n",
      "Iteration 17/100, Loss: 0.7731207013130188\n",
      "Iteration 18/100, Loss: 0.7571896910667419\n",
      "Iteration 19/100, Loss: 0.7421531081199646\n",
      "Iteration 20/100, Loss: 0.7279462814331055\n",
      "Iteration 21/100, Loss: 0.7145061492919922\n",
      "Iteration 22/100, Loss: 0.7017806768417358\n",
      "Iteration 23/100, Loss: 0.6897185444831848\n",
      "Iteration 24/100, Loss: 0.6782741546630859\n",
      "Iteration 25/100, Loss: 0.6674062013626099\n",
      "Iteration 26/100, Loss: 0.6570751667022705\n",
      "Iteration 27/100, Loss: 0.6472455859184265\n",
      "Iteration 28/100, Loss: 0.637883186340332\n",
      "Iteration 29/100, Loss: 0.6289586424827576\n",
      "Iteration 30/100, Loss: 0.620444118976593\n",
      "Iteration 31/100, Loss: 0.6123133897781372\n",
      "Iteration 32/100, Loss: 0.6045417785644531\n",
      "Iteration 33/100, Loss: 0.5971074104309082\n",
      "Iteration 34/100, Loss: 0.5899907946586609\n",
      "Iteration 35/100, Loss: 0.583172619342804\n",
      "Iteration 36/100, Loss: 0.5766347646713257\n",
      "Iteration 37/100, Loss: 0.5703614950180054\n",
      "Iteration 38/100, Loss: 0.5643376111984253\n",
      "Iteration 39/100, Loss: 0.5585498809814453\n",
      "Iteration 40/100, Loss: 0.5529845952987671\n",
      "Iteration 41/100, Loss: 0.5476294159889221\n",
      "Iteration 42/100, Loss: 0.5424724817276001\n",
      "Iteration 43/100, Loss: 0.5375030636787415\n",
      "Iteration 44/100, Loss: 0.5327111482620239\n",
      "Iteration 45/100, Loss: 0.528088390827179\n",
      "Iteration 46/100, Loss: 0.523626446723938\n",
      "Iteration 47/100, Loss: 0.5193172693252563\n",
      "Iteration 48/100, Loss: 0.5151522755622864\n",
      "Iteration 49/100, Loss: 0.5111246109008789\n",
      "Iteration 50/100, Loss: 0.5072279572486877\n",
      "Iteration 51/100, Loss: 0.503456175327301\n",
      "Iteration 52/100, Loss: 0.4998031258583069\n",
      "Iteration 53/100, Loss: 0.4962634742259979\n",
      "Iteration 54/100, Loss: 0.4928319752216339\n",
      "Iteration 55/100, Loss: 0.4895033538341522\n",
      "Iteration 56/100, Loss: 0.48627328872680664\n",
      "Iteration 57/100, Loss: 0.4831370711326599\n",
      "Iteration 58/100, Loss: 0.4800909459590912\n",
      "Iteration 59/100, Loss: 0.47713086009025574\n",
      "Iteration 60/100, Loss: 0.4742531180381775\n",
      "Iteration 61/100, Loss: 0.4714541733264923\n",
      "Iteration 62/100, Loss: 0.4687308669090271\n",
      "Iteration 63/100, Loss: 0.466079980134964\n",
      "Iteration 64/100, Loss: 0.4634985029697418\n",
      "Iteration 65/100, Loss: 0.46098393201828003\n",
      "Iteration 66/100, Loss: 0.4585333466529846\n",
      "Iteration 67/100, Loss: 0.4561445415019989\n",
      "Iteration 68/100, Loss: 0.45381495356559753\n",
      "Iteration 69/100, Loss: 0.45154237747192383\n",
      "Iteration 70/100, Loss: 0.4493243992328644\n",
      "Iteration 71/100, Loss: 0.4471592903137207\n",
      "Iteration 72/100, Loss: 0.4450451135635376\n",
      "Iteration 73/100, Loss: 0.4429798126220703\n",
      "Iteration 74/100, Loss: 0.4409615993499756\n",
      "Iteration 75/100, Loss: 0.43898874521255493\n",
      "Iteration 76/100, Loss: 0.43705958127975464\n",
      "Iteration 77/100, Loss: 0.43517258763313293\n",
      "Iteration 78/100, Loss: 0.4333264231681824\n",
      "Iteration 79/100, Loss: 0.43151959776878357\n",
      "Iteration 80/100, Loss: 0.42975085973739624\n",
      "Iteration 81/100, Loss: 0.4280189573764801\n",
      "Iteration 82/100, Loss: 0.4263225495815277\n",
      "Iteration 83/100, Loss: 0.42466047406196594\n",
      "Iteration 84/100, Loss: 0.4230317175388336\n",
      "Iteration 85/100, Loss: 0.42143499851226807\n",
      "Iteration 86/100, Loss: 0.41986942291259766\n",
      "Iteration 87/100, Loss: 0.4183342754840851\n",
      "Iteration 88/100, Loss: 0.4168283939361572\n",
      "Iteration 89/100, Loss: 0.4153508245944977\n",
      "Iteration 90/100, Loss: 0.4139006435871124\n",
      "Iteration 91/100, Loss: 0.4124770760536194\n",
      "Iteration 92/100, Loss: 0.41107943654060364\n",
      "Iteration 93/100, Loss: 0.40970683097839355\n",
      "Iteration 94/100, Loss: 0.4083586037158966\n",
      "Iteration 95/100, Loss: 0.4070340394973755\n",
      "Iteration 96/100, Loss: 0.4057323634624481\n",
      "Iteration 97/100, Loss: 0.40445294976234436\n",
      "Iteration 98/100, Loss: 0.403195321559906\n",
      "Iteration 99/100, Loss: 0.4019585847854614\n",
      "Iteration 100/100, Loss: 0.4007423222064972\n",
      "Pruning Step 8\n",
      "Iteration 1/100, Loss: 1.2010835409164429\n",
      "Iteration 2/100, Loss: 1.143188714981079\n",
      "Iteration 3/100, Loss: 1.1004843711853027\n",
      "Iteration 4/100, Loss: 1.0639628171920776\n",
      "Iteration 5/100, Loss: 1.0310307741165161\n",
      "Iteration 6/100, Loss: 1.0007115602493286\n",
      "Iteration 7/100, Loss: 0.972530722618103\n",
      "Iteration 8/100, Loss: 0.9462082386016846\n",
      "Iteration 9/100, Loss: 0.9215548038482666\n",
      "Iteration 10/100, Loss: 0.8984190821647644\n",
      "Iteration 11/100, Loss: 0.876665472984314\n",
      "Iteration 12/100, Loss: 0.8561871647834778\n",
      "Iteration 13/100, Loss: 0.8368882536888123\n",
      "Iteration 14/100, Loss: 0.818679928779602\n",
      "Iteration 15/100, Loss: 0.8014807105064392\n",
      "Iteration 16/100, Loss: 0.7852181792259216\n",
      "Iteration 17/100, Loss: 0.7698266506195068\n",
      "Iteration 18/100, Loss: 0.7552445530891418\n",
      "Iteration 19/100, Loss: 0.7414143085479736\n",
      "Iteration 20/100, Loss: 0.7282875180244446\n",
      "Iteration 21/100, Loss: 0.7158183455467224\n",
      "Iteration 22/100, Loss: 0.7039615511894226\n",
      "Iteration 23/100, Loss: 0.692675769329071\n",
      "Iteration 24/100, Loss: 0.6819247007369995\n",
      "Iteration 25/100, Loss: 0.6716747283935547\n",
      "Iteration 26/100, Loss: 0.6618950366973877\n",
      "Iteration 27/100, Loss: 0.6525564193725586\n",
      "Iteration 28/100, Loss: 0.6436319947242737\n",
      "Iteration 29/100, Loss: 0.6350973844528198\n",
      "Iteration 30/100, Loss: 0.6269289255142212\n",
      "Iteration 31/100, Loss: 0.6191056370735168\n",
      "Iteration 32/100, Loss: 0.611606776714325\n",
      "Iteration 33/100, Loss: 0.6044130921363831\n",
      "Iteration 34/100, Loss: 0.5975079536437988\n",
      "Iteration 35/100, Loss: 0.5908748507499695\n",
      "Iteration 36/100, Loss: 0.5844998955726624\n",
      "Iteration 37/100, Loss: 0.5783687233924866\n",
      "Iteration 38/100, Loss: 0.5724683403968811\n",
      "Iteration 39/100, Loss: 0.5667863488197327\n",
      "Iteration 40/100, Loss: 0.5613105893135071\n",
      "Iteration 41/100, Loss: 0.5560311675071716\n",
      "Iteration 42/100, Loss: 0.5509374141693115\n",
      "Iteration 43/100, Loss: 0.5460205078125\n",
      "Iteration 44/100, Loss: 0.5412717461585999\n",
      "Iteration 45/100, Loss: 0.5366829633712769\n",
      "Iteration 46/100, Loss: 0.5322461724281311\n",
      "Iteration 47/100, Loss: 0.5279543399810791\n",
      "Iteration 48/100, Loss: 0.5238007307052612\n",
      "Iteration 49/100, Loss: 0.5197787284851074\n",
      "Iteration 50/100, Loss: 0.5158820748329163\n",
      "Iteration 51/100, Loss: 0.5121051669120789\n",
      "Iteration 52/100, Loss: 0.5084425210952759\n",
      "Iteration 53/100, Loss: 0.5048892498016357\n",
      "Iteration 54/100, Loss: 0.5014405846595764\n",
      "Iteration 55/100, Loss: 0.4980918765068054\n",
      "Iteration 56/100, Loss: 0.4948389232158661\n",
      "Iteration 57/100, Loss: 0.4916776418685913\n",
      "Iteration 58/100, Loss: 0.4886043667793274\n",
      "Iteration 59/100, Loss: 0.4856153726577759\n",
      "Iteration 60/100, Loss: 0.4827069044113159\n",
      "Iteration 61/100, Loss: 0.4798760414123535\n",
      "Iteration 62/100, Loss: 0.4771190285682678\n",
      "Iteration 63/100, Loss: 0.4744335412979126\n",
      "Iteration 64/100, Loss: 0.47181642055511475\n",
      "Iteration 65/100, Loss: 0.4692654311656952\n",
      "Iteration 66/100, Loss: 0.4667779803276062\n",
      "Iteration 67/100, Loss: 0.4643518328666687\n",
      "Iteration 68/100, Loss: 0.4619844853878021\n",
      "Iteration 69/100, Loss: 0.4596739113330841\n",
      "Iteration 70/100, Loss: 0.4574178457260132\n",
      "Iteration 71/100, Loss: 0.4552145302295685\n",
      "Iteration 72/100, Loss: 0.45306211709976196\n",
      "Iteration 73/100, Loss: 0.450958788394928\n",
      "Iteration 74/100, Loss: 0.4489027261734009\n",
      "Iteration 75/100, Loss: 0.44689226150512695\n",
      "Iteration 76/100, Loss: 0.44492581486701965\n",
      "Iteration 77/100, Loss: 0.4430018365383148\n",
      "Iteration 78/100, Loss: 0.4411190152168274\n",
      "Iteration 79/100, Loss: 0.43927595019340515\n",
      "Iteration 80/100, Loss: 0.43747153878211975\n",
      "Iteration 81/100, Loss: 0.4357044994831085\n",
      "Iteration 82/100, Loss: 0.43397316336631775\n",
      "Iteration 83/100, Loss: 0.4322766959667206\n",
      "Iteration 84/100, Loss: 0.4306139647960663\n",
      "Iteration 85/100, Loss: 0.42898404598236084\n",
      "Iteration 86/100, Loss: 0.4273858666419983\n",
      "Iteration 87/100, Loss: 0.4258182942867279\n",
      "Iteration 88/100, Loss: 0.42428040504455566\n",
      "Iteration 89/100, Loss: 0.42277127504348755\n",
      "Iteration 90/100, Loss: 0.4212900400161743\n",
      "Iteration 91/100, Loss: 0.4198359251022339\n",
      "Iteration 92/100, Loss: 0.4184080958366394\n",
      "Iteration 93/100, Loss: 0.41700589656829834\n",
      "Iteration 94/100, Loss: 0.4156283736228943\n",
      "Iteration 95/100, Loss: 0.4142748713493347\n",
      "Iteration 96/100, Loss: 0.4129447340965271\n",
      "Iteration 97/100, Loss: 0.41163742542266846\n",
      "Iteration 98/100, Loss: 0.41035234928131104\n",
      "Iteration 99/100, Loss: 0.409088671207428\n",
      "Iteration 100/100, Loss: 0.4078458249568939\n",
      "Pruning Step 9\n",
      "Iteration 1/100, Loss: 1.1775391101837158\n",
      "Iteration 2/100, Loss: 1.117184042930603\n",
      "Iteration 3/100, Loss: 1.0754930973052979\n",
      "Iteration 4/100, Loss: 1.041641354560852\n",
      "Iteration 5/100, Loss: 1.011993169784546\n",
      "Iteration 6/100, Loss: 0.9850281476974487\n",
      "Iteration 7/100, Loss: 0.9600456953048706\n",
      "Iteration 8/100, Loss: 0.9366685152053833\n",
      "Iteration 9/100, Loss: 0.9146589636802673\n",
      "Iteration 10/100, Loss: 0.8938710689544678\n",
      "Iteration 11/100, Loss: 0.8742002844810486\n",
      "Iteration 12/100, Loss: 0.8555588126182556\n",
      "Iteration 13/100, Loss: 0.8378721475601196\n",
      "Iteration 14/100, Loss: 0.8210738301277161\n",
      "Iteration 15/100, Loss: 0.8051067590713501\n",
      "Iteration 16/100, Loss: 0.7899191379547119\n",
      "Iteration 17/100, Loss: 0.7754640579223633\n",
      "Iteration 18/100, Loss: 0.761698842048645\n",
      "Iteration 19/100, Loss: 0.7485787868499756\n",
      "Iteration 20/100, Loss: 0.7360650300979614\n",
      "Iteration 21/100, Loss: 0.724123477935791\n",
      "Iteration 22/100, Loss: 0.712719738483429\n",
      "Iteration 23/100, Loss: 0.7018222212791443\n",
      "Iteration 24/100, Loss: 0.691403865814209\n",
      "Iteration 25/100, Loss: 0.6814348697662354\n",
      "Iteration 26/100, Loss: 0.671890377998352\n",
      "Iteration 27/100, Loss: 0.6627479195594788\n",
      "Iteration 28/100, Loss: 0.6539857983589172\n",
      "Iteration 29/100, Loss: 0.6455827951431274\n",
      "Iteration 30/100, Loss: 0.6375190019607544\n",
      "Iteration 31/100, Loss: 0.6297767162322998\n",
      "Iteration 32/100, Loss: 0.6223385334014893\n",
      "Iteration 33/100, Loss: 0.6151883006095886\n",
      "Iteration 34/100, Loss: 0.6083110570907593\n",
      "Iteration 35/100, Loss: 0.6016926169395447\n",
      "Iteration 36/100, Loss: 0.5953195095062256\n",
      "Iteration 37/100, Loss: 0.5891796946525574\n",
      "Iteration 38/100, Loss: 0.5832611322402954\n",
      "Iteration 39/100, Loss: 0.5775531530380249\n",
      "Iteration 40/100, Loss: 0.5720453262329102\n",
      "Iteration 41/100, Loss: 0.5667275786399841\n",
      "Iteration 42/100, Loss: 0.5615907311439514\n",
      "Iteration 43/100, Loss: 0.55662602186203\n",
      "Iteration 44/100, Loss: 0.5518258213996887\n",
      "Iteration 45/100, Loss: 0.5471824407577515\n",
      "Iteration 46/100, Loss: 0.5426883697509766\n",
      "Iteration 47/100, Loss: 0.5383369326591492\n",
      "Iteration 48/100, Loss: 0.5341218709945679\n",
      "Iteration 49/100, Loss: 0.5300371050834656\n",
      "Iteration 50/100, Loss: 0.5260767340660095\n",
      "Iteration 51/100, Loss: 0.5222353339195251\n",
      "Iteration 52/100, Loss: 0.5185076594352722\n",
      "Iteration 53/100, Loss: 0.5148890018463135\n",
      "Iteration 54/100, Loss: 0.5113747119903564\n",
      "Iteration 55/100, Loss: 0.5079604387283325\n",
      "Iteration 56/100, Loss: 0.5046420097351074\n",
      "Iteration 57/100, Loss: 0.5014156699180603\n",
      "Iteration 58/100, Loss: 0.49827760457992554\n",
      "Iteration 59/100, Loss: 0.49522414803504944\n",
      "Iteration 60/100, Loss: 0.4922522008419037\n",
      "Iteration 61/100, Loss: 0.489358514547348\n",
      "Iteration 62/100, Loss: 0.48654016852378845\n",
      "Iteration 63/100, Loss: 0.48379388451576233\n",
      "Iteration 64/100, Loss: 0.48111701011657715\n",
      "Iteration 65/100, Loss: 0.4785071611404419\n",
      "Iteration 66/100, Loss: 0.47596174478530884\n",
      "Iteration 67/100, Loss: 0.47347837686538696\n",
      "Iteration 68/100, Loss: 0.47105467319488525\n",
      "Iteration 69/100, Loss: 0.46868839859962463\n",
      "Iteration 70/100, Loss: 0.4663775861263275\n",
      "Iteration 71/100, Loss: 0.46412035822868347\n",
      "Iteration 72/100, Loss: 0.4619145691394806\n",
      "Iteration 73/100, Loss: 0.4597587287425995\n",
      "Iteration 74/100, Loss: 0.45765116810798645\n",
      "Iteration 75/100, Loss: 0.4555900990962982\n",
      "Iteration 76/100, Loss: 0.45357397198677063\n",
      "Iteration 77/100, Loss: 0.45160138607025146\n",
      "Iteration 78/100, Loss: 0.4496709704399109\n",
      "Iteration 79/100, Loss: 0.44778120517730713\n",
      "Iteration 80/100, Loss: 0.44593098759651184\n",
      "Iteration 81/100, Loss: 0.44411882758140564\n",
      "Iteration 82/100, Loss: 0.44234350323677063\n",
      "Iteration 83/100, Loss: 0.4406038522720337\n",
      "Iteration 84/100, Loss: 0.43889889121055603\n",
      "Iteration 85/100, Loss: 0.43722739815711975\n",
      "Iteration 86/100, Loss: 0.43558844923973083\n",
      "Iteration 87/100, Loss: 0.43398091197013855\n",
      "Iteration 88/100, Loss: 0.4324039816856384\n",
      "Iteration 89/100, Loss: 0.4308565855026245\n",
      "Iteration 90/100, Loss: 0.42933782935142517\n",
      "Iteration 91/100, Loss: 0.4278472065925598\n",
      "Iteration 92/100, Loss: 0.4263835847377777\n",
      "Iteration 93/100, Loss: 0.4249463677406311\n",
      "Iteration 94/100, Loss: 0.4235345125198364\n",
      "Iteration 95/100, Loss: 0.4221474528312683\n",
      "Iteration 96/100, Loss: 0.4207846224308014\n",
      "Iteration 97/100, Loss: 0.419445276260376\n",
      "Iteration 98/100, Loss: 0.4181287884712219\n",
      "Iteration 99/100, Loss: 0.41683444380760193\n",
      "Iteration 100/100, Loss: 0.4155614674091339\n",
      "Pruning Step 10\n",
      "Iteration 1/100, Loss: 1.12286376953125\n",
      "Iteration 2/100, Loss: 1.0812948942184448\n",
      "Iteration 3/100, Loss: 1.0491740703582764\n",
      "Iteration 4/100, Loss: 1.0214064121246338\n",
      "Iteration 5/100, Loss: 0.9961457252502441\n",
      "Iteration 6/100, Loss: 0.9726564884185791\n",
      "Iteration 7/100, Loss: 0.9505868554115295\n",
      "Iteration 8/100, Loss: 0.9297376275062561\n",
      "Iteration 9/100, Loss: 0.9099814891815186\n",
      "Iteration 10/100, Loss: 0.8912174105644226\n",
      "Iteration 11/100, Loss: 0.8733691573143005\n",
      "Iteration 12/100, Loss: 0.856371283531189\n",
      "Iteration 13/100, Loss: 0.8401699066162109\n",
      "Iteration 14/100, Loss: 0.824712872505188\n",
      "Iteration 15/100, Loss: 0.8099583983421326\n",
      "Iteration 16/100, Loss: 0.7958647012710571\n",
      "Iteration 17/100, Loss: 0.7823980450630188\n",
      "Iteration 18/100, Loss: 0.769523024559021\n",
      "Iteration 19/100, Loss: 0.7572034001350403\n",
      "Iteration 20/100, Loss: 0.7454107403755188\n",
      "Iteration 21/100, Loss: 0.7341170310974121\n",
      "Iteration 22/100, Loss: 0.7232954502105713\n",
      "Iteration 23/100, Loss: 0.7129220962524414\n",
      "Iteration 24/100, Loss: 0.7029721140861511\n",
      "Iteration 25/100, Loss: 0.6934233903884888\n",
      "Iteration 26/100, Loss: 0.6842548251152039\n",
      "Iteration 27/100, Loss: 0.6754471063613892\n",
      "Iteration 28/100, Loss: 0.6669811606407166\n",
      "Iteration 29/100, Loss: 0.6588397026062012\n",
      "Iteration 30/100, Loss: 0.6510064601898193\n",
      "Iteration 31/100, Loss: 0.6434652805328369\n",
      "Iteration 32/100, Loss: 0.6362016797065735\n",
      "Iteration 33/100, Loss: 0.6292020678520203\n",
      "Iteration 34/100, Loss: 0.6224536299705505\n",
      "Iteration 35/100, Loss: 0.6159443855285645\n",
      "Iteration 36/100, Loss: 0.6096630096435547\n",
      "Iteration 37/100, Loss: 0.6035991907119751\n",
      "Iteration 38/100, Loss: 0.5977421402931213\n",
      "Iteration 39/100, Loss: 0.5920819044113159\n",
      "Iteration 40/100, Loss: 0.5866093635559082\n",
      "Iteration 41/100, Loss: 0.5813161730766296\n",
      "Iteration 42/100, Loss: 0.5761938691139221\n",
      "Iteration 43/100, Loss: 0.5712343454360962\n",
      "Iteration 44/100, Loss: 0.5664309859275818\n",
      "Iteration 45/100, Loss: 0.5617771744728088\n",
      "Iteration 46/100, Loss: 0.5572656393051147\n",
      "Iteration 47/100, Loss: 0.5528908371925354\n",
      "Iteration 48/100, Loss: 0.548646867275238\n",
      "Iteration 49/100, Loss: 0.5445277690887451\n",
      "Iteration 50/100, Loss: 0.5405283570289612\n",
      "Iteration 51/100, Loss: 0.5366436839103699\n",
      "Iteration 52/100, Loss: 0.532869279384613\n",
      "Iteration 53/100, Loss: 0.5292003750801086\n",
      "Iteration 54/100, Loss: 0.5256332159042358\n",
      "Iteration 55/100, Loss: 0.5221635103225708\n",
      "Iteration 56/100, Loss: 0.5187874436378479\n",
      "Iteration 57/100, Loss: 0.5155007839202881\n",
      "Iteration 58/100, Loss: 0.5123006105422974\n",
      "Iteration 59/100, Loss: 0.5091834664344788\n",
      "Iteration 60/100, Loss: 0.5061461329460144\n",
      "Iteration 61/100, Loss: 0.5031856298446655\n",
      "Iteration 62/100, Loss: 0.5002992153167725\n",
      "Iteration 63/100, Loss: 0.49748435616493225\n",
      "Iteration 64/100, Loss: 0.4947383403778076\n",
      "Iteration 65/100, Loss: 0.4920588433742523\n",
      "Iteration 66/100, Loss: 0.4894433617591858\n",
      "Iteration 67/100, Loss: 0.48688945174217224\n",
      "Iteration 68/100, Loss: 0.48439523577690125\n",
      "Iteration 69/100, Loss: 0.48195865750312805\n",
      "Iteration 70/100, Loss: 0.4795776605606079\n",
      "Iteration 71/100, Loss: 0.47725024819374084\n",
      "Iteration 72/100, Loss: 0.474974662065506\n",
      "Iteration 73/100, Loss: 0.4727492332458496\n",
      "Iteration 74/100, Loss: 0.47057220339775085\n",
      "Iteration 75/100, Loss: 0.46844184398651123\n",
      "Iteration 76/100, Loss: 0.4663565754890442\n",
      "Iteration 77/100, Loss: 0.4643150568008423\n",
      "Iteration 78/100, Loss: 0.4623159170150757\n",
      "Iteration 79/100, Loss: 0.4603579640388489\n",
      "Iteration 80/100, Loss: 0.45844006538391113\n",
      "Iteration 81/100, Loss: 0.4565608501434326\n",
      "Iteration 82/100, Loss: 0.45471906661987305\n",
      "Iteration 83/100, Loss: 0.45291370153427124\n",
      "Iteration 84/100, Loss: 0.4511435031890869\n",
      "Iteration 85/100, Loss: 0.4494074285030365\n",
      "Iteration 86/100, Loss: 0.44770437479019165\n",
      "Iteration 87/100, Loss: 0.4460334777832031\n",
      "Iteration 88/100, Loss: 0.4443938434123993\n",
      "Iteration 89/100, Loss: 0.44278451800346375\n",
      "Iteration 90/100, Loss: 0.44120457768440247\n",
      "Iteration 91/100, Loss: 0.43965351581573486\n",
      "Iteration 92/100, Loss: 0.43813037872314453\n",
      "Iteration 93/100, Loss: 0.43663421273231506\n",
      "Iteration 94/100, Loss: 0.43516436219215393\n",
      "Iteration 95/100, Loss: 0.43372011184692383\n",
      "Iteration 96/100, Loss: 0.43230077624320984\n",
      "Iteration 97/100, Loss: 0.4309057295322418\n",
      "Iteration 98/100, Loss: 0.42953425645828247\n",
      "Iteration 99/100, Loss: 0.42818567156791687\n",
      "Iteration 100/100, Loss: 0.42685917019844055\n",
      "Pruning Step 11\n",
      "Iteration 1/100, Loss: 1.105051875114441\n",
      "Iteration 2/100, Loss: 1.0648547410964966\n",
      "Iteration 3/100, Loss: 1.032886028289795\n",
      "Iteration 4/100, Loss: 1.0058295726776123\n",
      "Iteration 5/100, Loss: 0.9818709492683411\n",
      "Iteration 6/100, Loss: 0.9600218534469604\n",
      "Iteration 7/100, Loss: 0.9397425055503845\n",
      "Iteration 8/100, Loss: 0.9207011461257935\n",
      "Iteration 9/100, Loss: 0.9027021527290344\n",
      "Iteration 10/100, Loss: 0.885613739490509\n",
      "Iteration 11/100, Loss: 0.8693403601646423\n",
      "Iteration 12/100, Loss: 0.8538117408752441\n",
      "Iteration 13/100, Loss: 0.8389676809310913\n",
      "Iteration 14/100, Loss: 0.8247591257095337\n",
      "Iteration 15/100, Loss: 0.8111485838890076\n",
      "Iteration 16/100, Loss: 0.7980994582176208\n",
      "Iteration 17/100, Loss: 0.7855795621871948\n",
      "Iteration 18/100, Loss: 0.7735608816146851\n",
      "Iteration 19/100, Loss: 0.7620188593864441\n",
      "Iteration 20/100, Loss: 0.7509291172027588\n",
      "Iteration 21/100, Loss: 0.7402702569961548\n",
      "Iteration 22/100, Loss: 0.7300201654434204\n",
      "Iteration 23/100, Loss: 0.7201584577560425\n",
      "Iteration 24/100, Loss: 0.7106672525405884\n",
      "Iteration 25/100, Loss: 0.701528787612915\n",
      "Iteration 26/100, Loss: 0.6927257776260376\n",
      "Iteration 27/100, Loss: 0.684242308139801\n",
      "Iteration 28/100, Loss: 0.6760640740394592\n",
      "Iteration 29/100, Loss: 0.6681767106056213\n",
      "Iteration 30/100, Loss: 0.6605662107467651\n",
      "Iteration 31/100, Loss: 0.6532204151153564\n",
      "Iteration 32/100, Loss: 0.6461277604103088\n",
      "Iteration 33/100, Loss: 0.6392761468887329\n",
      "Iteration 34/100, Loss: 0.6326554417610168\n",
      "Iteration 35/100, Loss: 0.6262550950050354\n",
      "Iteration 36/100, Loss: 0.6200649738311768\n",
      "Iteration 37/100, Loss: 0.6140758991241455\n",
      "Iteration 38/100, Loss: 0.6082792282104492\n",
      "Iteration 39/100, Loss: 0.6026663184165955\n",
      "Iteration 40/100, Loss: 0.5972295999526978\n",
      "Iteration 41/100, Loss: 0.5919619202613831\n",
      "Iteration 42/100, Loss: 0.586855947971344\n",
      "Iteration 43/100, Loss: 0.5819048881530762\n",
      "Iteration 44/100, Loss: 0.5771030187606812\n",
      "Iteration 45/100, Loss: 0.5724435448646545\n",
      "Iteration 46/100, Loss: 0.567920982837677\n",
      "Iteration 47/100, Loss: 0.5635290145874023\n",
      "Iteration 48/100, Loss: 0.5592631697654724\n",
      "Iteration 49/100, Loss: 0.555118203163147\n",
      "Iteration 50/100, Loss: 0.551089346408844\n",
      "Iteration 51/100, Loss: 0.5471715927124023\n",
      "Iteration 52/100, Loss: 0.543361246585846\n",
      "Iteration 53/100, Loss: 0.539654552936554\n",
      "Iteration 54/100, Loss: 0.5360468626022339\n",
      "Iteration 55/100, Loss: 0.5325343608856201\n",
      "Iteration 56/100, Loss: 0.5291138291358948\n",
      "Iteration 57/100, Loss: 0.5257814526557922\n",
      "Iteration 58/100, Loss: 0.5225344896316528\n",
      "Iteration 59/100, Loss: 0.5193697214126587\n",
      "Iteration 60/100, Loss: 0.5162838697433472\n",
      "Iteration 61/100, Loss: 0.5132744908332825\n",
      "Iteration 62/100, Loss: 0.5103384852409363\n",
      "Iteration 63/100, Loss: 0.507473349571228\n",
      "Iteration 64/100, Loss: 0.504676878452301\n",
      "Iteration 65/100, Loss: 0.5019464492797852\n",
      "Iteration 66/100, Loss: 0.49928000569343567\n",
      "Iteration 67/100, Loss: 0.4966752827167511\n",
      "Iteration 68/100, Loss: 0.49413028359413147\n",
      "Iteration 69/100, Loss: 0.49164289236068726\n",
      "Iteration 70/100, Loss: 0.4892110824584961\n",
      "Iteration 71/100, Loss: 0.4868330657482147\n",
      "Iteration 72/100, Loss: 0.484507292509079\n",
      "Iteration 73/100, Loss: 0.4822320342063904\n",
      "Iteration 74/100, Loss: 0.4800054728984833\n",
      "Iteration 75/100, Loss: 0.4778260886669159\n",
      "Iteration 76/100, Loss: 0.475692480802536\n",
      "Iteration 77/100, Loss: 0.4736033082008362\n",
      "Iteration 78/100, Loss: 0.47155728936195374\n",
      "Iteration 79/100, Loss: 0.46955299377441406\n",
      "Iteration 80/100, Loss: 0.4675889313220978\n",
      "Iteration 81/100, Loss: 0.46566399931907654\n",
      "Iteration 82/100, Loss: 0.46377721428871155\n",
      "Iteration 83/100, Loss: 0.4619273841381073\n",
      "Iteration 84/100, Loss: 0.4601134657859802\n",
      "Iteration 85/100, Loss: 0.4583342969417572\n",
      "Iteration 86/100, Loss: 0.45658886432647705\n",
      "Iteration 87/100, Loss: 0.4548763036727905\n",
      "Iteration 88/100, Loss: 0.4531956613063812\n",
      "Iteration 89/100, Loss: 0.45154592394828796\n",
      "Iteration 90/100, Loss: 0.4499261975288391\n",
      "Iteration 91/100, Loss: 0.448335736989975\n",
      "Iteration 92/100, Loss: 0.4467737376689911\n",
      "Iteration 93/100, Loss: 0.4452393651008606\n",
      "Iteration 94/100, Loss: 0.4437318444252014\n",
      "Iteration 95/100, Loss: 0.44225063920021057\n",
      "Iteration 96/100, Loss: 0.44079485535621643\n",
      "Iteration 97/100, Loss: 0.4393639862537384\n",
      "Iteration 98/100, Loss: 0.4379573166370392\n",
      "Iteration 99/100, Loss: 0.43657419085502625\n",
      "Iteration 100/100, Loss: 0.43521392345428467\n",
      "Pruning Step 12\n",
      "Iteration 1/100, Loss: 1.1127187013626099\n",
      "Iteration 2/100, Loss: 1.0637609958648682\n",
      "Iteration 3/100, Loss: 1.031722068786621\n",
      "Iteration 4/100, Loss: 1.0066075325012207\n",
      "Iteration 5/100, Loss: 0.9849355220794678\n",
      "Iteration 6/100, Loss: 0.9652860760688782\n",
      "Iteration 7/100, Loss: 0.9470099806785583\n",
      "Iteration 8/100, Loss: 0.9297751188278198\n",
      "Iteration 9/100, Loss: 0.9133890867233276\n",
      "Iteration 10/100, Loss: 0.8977375030517578\n",
      "Iteration 11/100, Loss: 0.882737934589386\n",
      "Iteration 12/100, Loss: 0.868334174156189\n",
      "Iteration 13/100, Loss: 0.8544860482215881\n",
      "Iteration 14/100, Loss: 0.8411608338356018\n",
      "Iteration 15/100, Loss: 0.8283283710479736\n",
      "Iteration 16/100, Loss: 0.8159638047218323\n",
      "Iteration 17/100, Loss: 0.8040449023246765\n",
      "Iteration 18/100, Loss: 0.7925520539283752\n",
      "Iteration 19/100, Loss: 0.7814647555351257\n",
      "Iteration 20/100, Loss: 0.7707666158676147\n",
      "Iteration 21/100, Loss: 0.7604402303695679\n",
      "Iteration 22/100, Loss: 0.7504705786705017\n",
      "Iteration 23/100, Loss: 0.7408421635627747\n",
      "Iteration 24/100, Loss: 0.7315419316291809\n",
      "Iteration 25/100, Loss: 0.7225560545921326\n",
      "Iteration 26/100, Loss: 0.7138711810112\n",
      "Iteration 27/100, Loss: 0.7054761052131653\n",
      "Iteration 28/100, Loss: 0.6973587870597839\n",
      "Iteration 29/100, Loss: 0.6895078420639038\n",
      "Iteration 30/100, Loss: 0.6819117665290833\n",
      "Iteration 31/100, Loss: 0.674560010433197\n",
      "Iteration 32/100, Loss: 0.667442798614502\n",
      "Iteration 33/100, Loss: 0.6605504155158997\n",
      "Iteration 34/100, Loss: 0.6538742780685425\n",
      "Iteration 35/100, Loss: 0.6474053859710693\n",
      "Iteration 36/100, Loss: 0.6411356925964355\n",
      "Iteration 37/100, Loss: 0.6350575089454651\n",
      "Iteration 38/100, Loss: 0.6291635632514954\n",
      "Iteration 39/100, Loss: 0.623445987701416\n",
      "Iteration 40/100, Loss: 0.6178979873657227\n",
      "Iteration 41/100, Loss: 0.6125128865242004\n",
      "Iteration 42/100, Loss: 0.6072849631309509\n",
      "Iteration 43/100, Loss: 0.6022079586982727\n",
      "Iteration 44/100, Loss: 0.5972756743431091\n",
      "Iteration 45/100, Loss: 0.5924828052520752\n",
      "Iteration 46/100, Loss: 0.5878239870071411\n",
      "Iteration 47/100, Loss: 0.583294153213501\n",
      "Iteration 48/100, Loss: 0.5788887739181519\n",
      "Iteration 49/100, Loss: 0.5746033787727356\n",
      "Iteration 50/100, Loss: 0.5704328417778015\n",
      "Iteration 51/100, Loss: 0.5663730502128601\n",
      "Iteration 52/100, Loss: 0.5624196529388428\n",
      "Iteration 53/100, Loss: 0.5585691332817078\n",
      "Iteration 54/100, Loss: 0.5548177361488342\n",
      "Iteration 55/100, Loss: 0.5511620044708252\n",
      "Iteration 56/100, Loss: 0.5475984215736389\n",
      "Iteration 57/100, Loss: 0.5441235899925232\n",
      "Iteration 58/100, Loss: 0.5407344698905945\n",
      "Iteration 59/100, Loss: 0.5374282598495483\n",
      "Iteration 60/100, Loss: 0.5342024564743042\n",
      "Iteration 61/100, Loss: 0.531053900718689\n",
      "Iteration 62/100, Loss: 0.5279801487922668\n",
      "Iteration 63/100, Loss: 0.5249784588813782\n",
      "Iteration 64/100, Loss: 0.5220463275909424\n",
      "Iteration 65/100, Loss: 0.5191817283630371\n",
      "Iteration 66/100, Loss: 0.5163825750350952\n",
      "Iteration 67/100, Loss: 0.5136464238166809\n",
      "Iteration 68/100, Loss: 0.5109715461730957\n",
      "Iteration 69/100, Loss: 0.5083557963371277\n",
      "Iteration 70/100, Loss: 0.5057973265647888\n",
      "Iteration 71/100, Loss: 0.5032944083213806\n",
      "Iteration 72/100, Loss: 0.5008453130722046\n",
      "Iteration 73/100, Loss: 0.4984484016895294\n",
      "Iteration 74/100, Loss: 0.49610188603401184\n",
      "Iteration 75/100, Loss: 0.49380430579185486\n",
      "Iteration 76/100, Loss: 0.4915541410446167\n",
      "Iteration 77/100, Loss: 0.4893498718738556\n",
      "Iteration 78/100, Loss: 0.48719024658203125\n",
      "Iteration 79/100, Loss: 0.4850737452507019\n",
      "Iteration 80/100, Loss: 0.48299920558929443\n",
      "Iteration 81/100, Loss: 0.4809655249118805\n",
      "Iteration 82/100, Loss: 0.47897130250930786\n",
      "Iteration 83/100, Loss: 0.4770156443119049\n",
      "Iteration 84/100, Loss: 0.47509753704071045\n",
      "Iteration 85/100, Loss: 0.47321584820747375\n",
      "Iteration 86/100, Loss: 0.47136956453323364\n",
      "Iteration 87/100, Loss: 0.46955767273902893\n",
      "Iteration 88/100, Loss: 0.4677792191505432\n",
      "Iteration 89/100, Loss: 0.4660331904888153\n",
      "Iteration 90/100, Loss: 0.46431875228881836\n",
      "Iteration 91/100, Loss: 0.46263498067855835\n",
      "Iteration 92/100, Loss: 0.46098124980926514\n",
      "Iteration 93/100, Loss: 0.4593566358089447\n",
      "Iteration 94/100, Loss: 0.45776042342185974\n",
      "Iteration 95/100, Loss: 0.45619186758995056\n",
      "Iteration 96/100, Loss: 0.4546501338481903\n",
      "Iteration 97/100, Loss: 0.45313456654548645\n",
      "Iteration 98/100, Loss: 0.4516444802284241\n",
      "Iteration 99/100, Loss: 0.45017921924591064\n",
      "Iteration 100/100, Loss: 0.4487382769584656\n",
      "Pruning Step 13\n",
      "Iteration 1/100, Loss: 1.1113463640213013\n",
      "Iteration 2/100, Loss: 1.0696096420288086\n",
      "Iteration 3/100, Loss: 1.038070797920227\n",
      "Iteration 4/100, Loss: 1.0123945474624634\n",
      "Iteration 5/100, Loss: 0.9902985692024231\n",
      "Iteration 6/100, Loss: 0.9705234169960022\n",
      "Iteration 7/100, Loss: 0.9523530602455139\n",
      "Iteration 8/100, Loss: 0.9353726506233215\n",
      "Iteration 9/100, Loss: 0.9193268418312073\n",
      "Iteration 10/100, Loss: 0.9040557146072388\n",
      "Iteration 11/100, Loss: 0.8894534111022949\n",
      "Iteration 12/100, Loss: 0.8754494190216064\n",
      "Iteration 13/100, Loss: 0.8619876503944397\n",
      "Iteration 14/100, Loss: 0.8490290641784668\n",
      "Iteration 15/100, Loss: 0.8365429639816284\n",
      "Iteration 16/100, Loss: 0.8245023488998413\n",
      "Iteration 17/100, Loss: 0.8128836154937744\n",
      "Iteration 18/100, Loss: 0.8016662001609802\n",
      "Iteration 19/100, Loss: 0.7908325791358948\n",
      "Iteration 20/100, Loss: 0.7803653478622437\n",
      "Iteration 21/100, Loss: 0.77024906873703\n",
      "Iteration 22/100, Loss: 0.760468602180481\n",
      "Iteration 23/100, Loss: 0.7510108947753906\n",
      "Iteration 24/100, Loss: 0.7418622970581055\n",
      "Iteration 25/100, Loss: 0.7330091595649719\n",
      "Iteration 26/100, Loss: 0.7244405746459961\n",
      "Iteration 27/100, Loss: 0.7161451578140259\n",
      "Iteration 28/100, Loss: 0.7081126570701599\n",
      "Iteration 29/100, Loss: 0.7003324031829834\n",
      "Iteration 30/100, Loss: 0.6927944421768188\n",
      "Iteration 31/100, Loss: 0.6854894757270813\n",
      "Iteration 32/100, Loss: 0.678408682346344\n",
      "Iteration 33/100, Loss: 0.6715427041053772\n",
      "Iteration 34/100, Loss: 0.6648833751678467\n",
      "Iteration 35/100, Loss: 0.6584231853485107\n",
      "Iteration 36/100, Loss: 0.6521539092063904\n",
      "Iteration 37/100, Loss: 0.6460682153701782\n",
      "Iteration 38/100, Loss: 0.640159547328949\n",
      "Iteration 39/100, Loss: 0.6344212889671326\n",
      "Iteration 40/100, Loss: 0.6288467645645142\n",
      "Iteration 41/100, Loss: 0.6234301924705505\n",
      "Iteration 42/100, Loss: 0.618165910243988\n",
      "Iteration 43/100, Loss: 0.6130484342575073\n",
      "Iteration 44/100, Loss: 0.6080722808837891\n",
      "Iteration 45/100, Loss: 0.6032319664955139\n",
      "Iteration 46/100, Loss: 0.5985227227210999\n",
      "Iteration 47/100, Loss: 0.5939400792121887\n",
      "Iteration 48/100, Loss: 0.5894791483879089\n",
      "Iteration 49/100, Loss: 0.5851359963417053\n",
      "Iteration 50/100, Loss: 0.5809063911437988\n",
      "Iteration 51/100, Loss: 0.5767862796783447\n",
      "Iteration 52/100, Loss: 0.5727716684341431\n",
      "Iteration 53/100, Loss: 0.5688591003417969\n",
      "Iteration 54/100, Loss: 0.565045177936554\n",
      "Iteration 55/100, Loss: 0.5613261461257935\n",
      "Iteration 56/100, Loss: 0.5576990246772766\n",
      "Iteration 57/100, Loss: 0.5541608333587646\n",
      "Iteration 58/100, Loss: 0.5507084727287292\n",
      "Iteration 59/100, Loss: 0.5473392009735107\n",
      "Iteration 60/100, Loss: 0.5440502166748047\n",
      "Iteration 61/100, Loss: 0.5408387780189514\n",
      "Iteration 62/100, Loss: 0.5377023816108704\n",
      "Iteration 63/100, Loss: 0.5346388220787048\n",
      "Iteration 64/100, Loss: 0.5316455960273743\n",
      "Iteration 65/100, Loss: 0.5287204384803772\n",
      "Iteration 66/100, Loss: 0.5258612036705017\n",
      "Iteration 67/100, Loss: 0.5230659246444702\n",
      "Iteration 68/100, Loss: 0.5203325152397156\n",
      "Iteration 69/100, Loss: 0.5176591277122498\n",
      "Iteration 70/100, Loss: 0.5150437355041504\n",
      "Iteration 71/100, Loss: 0.5124847292900085\n",
      "Iteration 72/100, Loss: 0.5099802613258362\n",
      "Iteration 73/100, Loss: 0.5075287818908691\n",
      "Iteration 74/100, Loss: 0.5051286220550537\n",
      "Iteration 75/100, Loss: 0.5027783513069153\n",
      "Iteration 76/100, Loss: 0.5004762411117554\n",
      "Iteration 77/100, Loss: 0.49822095036506653\n",
      "Iteration 78/100, Loss: 0.4960111975669861\n",
      "Iteration 79/100, Loss: 0.49384570121765137\n",
      "Iteration 80/100, Loss: 0.4917232394218445\n",
      "Iteration 81/100, Loss: 0.4896424412727356\n",
      "Iteration 82/100, Loss: 0.487602174282074\n",
      "Iteration 83/100, Loss: 0.4856013059616089\n",
      "Iteration 84/100, Loss: 0.4836387634277344\n",
      "Iteration 85/100, Loss: 0.48171353340148926\n",
      "Iteration 86/100, Loss: 0.4798244833946228\n",
      "Iteration 87/100, Loss: 0.47797054052352905\n",
      "Iteration 88/100, Loss: 0.47615084052085876\n",
      "Iteration 89/100, Loss: 0.4743644893169403\n",
      "Iteration 90/100, Loss: 0.47261062264442444\n",
      "Iteration 91/100, Loss: 0.4708883464336395\n",
      "Iteration 92/100, Loss: 0.46919673681259155\n",
      "Iteration 93/100, Loss: 0.46753522753715515\n",
      "Iteration 94/100, Loss: 0.4659028947353363\n",
      "Iteration 95/100, Loss: 0.4642990231513977\n",
      "Iteration 96/100, Loss: 0.4627227187156677\n",
      "Iteration 97/100, Loss: 0.46117332577705383\n",
      "Iteration 98/100, Loss: 0.4596502482891083\n",
      "Iteration 99/100, Loss: 0.45815277099609375\n",
      "Iteration 100/100, Loss: 0.4566802382469177\n",
      "Pruning Step 14\n",
      "Iteration 1/100, Loss: 1.2052782773971558\n",
      "Iteration 2/100, Loss: 1.1488481760025024\n",
      "Iteration 3/100, Loss: 1.1095123291015625\n",
      "Iteration 4/100, Loss: 1.0786975622177124\n",
      "Iteration 5/100, Loss: 1.0529301166534424\n",
      "Iteration 6/100, Loss: 1.0304149389266968\n",
      "Iteration 7/100, Loss: 1.0101447105407715\n",
      "Iteration 8/100, Loss: 0.991494357585907\n",
      "Iteration 9/100, Loss: 0.9740742444992065\n",
      "Iteration 10/100, Loss: 0.957626223564148\n",
      "Iteration 11/100, Loss: 0.9419769644737244\n",
      "Iteration 12/100, Loss: 0.9270114302635193\n",
      "Iteration 13/100, Loss: 0.9126461148262024\n",
      "Iteration 14/100, Loss: 0.8988228440284729\n",
      "Iteration 15/100, Loss: 0.885498046875\n",
      "Iteration 16/100, Loss: 0.8726372122764587\n",
      "Iteration 17/100, Loss: 0.8602109551429749\n",
      "Iteration 18/100, Loss: 0.8481968641281128\n",
      "Iteration 19/100, Loss: 0.8365741968154907\n",
      "Iteration 20/100, Loss: 0.8253257870674133\n",
      "Iteration 21/100, Loss: 0.814435601234436\n",
      "Iteration 22/100, Loss: 0.8038890361785889\n",
      "Iteration 23/100, Loss: 0.7936738133430481\n",
      "Iteration 24/100, Loss: 0.7837788462638855\n",
      "Iteration 25/100, Loss: 0.7741909623146057\n",
      "Iteration 26/100, Loss: 0.764898955821991\n",
      "Iteration 27/100, Loss: 0.7558920383453369\n",
      "Iteration 28/100, Loss: 0.7471598982810974\n",
      "Iteration 29/100, Loss: 0.7386921644210815\n",
      "Iteration 30/100, Loss: 0.7304794192314148\n",
      "Iteration 31/100, Loss: 0.7225124835968018\n",
      "Iteration 32/100, Loss: 0.7147822976112366\n",
      "Iteration 33/100, Loss: 0.707280158996582\n",
      "Iteration 34/100, Loss: 0.6999984979629517\n",
      "Iteration 35/100, Loss: 0.6929290890693665\n",
      "Iteration 36/100, Loss: 0.6860647201538086\n",
      "Iteration 37/100, Loss: 0.679397702217102\n",
      "Iteration 38/100, Loss: 0.6729215979576111\n",
      "Iteration 39/100, Loss: 0.6666295528411865\n",
      "Iteration 40/100, Loss: 0.6605148315429688\n",
      "Iteration 41/100, Loss: 0.6545708179473877\n",
      "Iteration 42/100, Loss: 0.6487917304039001\n",
      "Iteration 43/100, Loss: 0.6431721448898315\n",
      "Iteration 44/100, Loss: 0.6377063989639282\n",
      "Iteration 45/100, Loss: 0.6323889493942261\n",
      "Iteration 46/100, Loss: 0.6272147297859192\n",
      "Iteration 47/100, Loss: 0.6221787929534912\n",
      "Iteration 48/100, Loss: 0.6172760128974915\n",
      "Iteration 49/100, Loss: 0.6125022172927856\n",
      "Iteration 50/100, Loss: 0.6078526973724365\n",
      "Iteration 51/100, Loss: 0.603323221206665\n",
      "Iteration 52/100, Loss: 0.5989096760749817\n",
      "Iteration 53/100, Loss: 0.5946081280708313\n",
      "Iteration 54/100, Loss: 0.5904150009155273\n",
      "Iteration 55/100, Loss: 0.5863263607025146\n",
      "Iteration 56/100, Loss: 0.5823391079902649\n",
      "Iteration 57/100, Loss: 0.5784497857093811\n",
      "Iteration 58/100, Loss: 0.5746553540229797\n",
      "Iteration 59/100, Loss: 0.5709523558616638\n",
      "Iteration 60/100, Loss: 0.5673378705978394\n",
      "Iteration 61/100, Loss: 0.5638090968132019\n",
      "Iteration 62/100, Loss: 0.5603634119033813\n",
      "Iteration 63/100, Loss: 0.5569978356361389\n",
      "Iteration 64/100, Loss: 0.5537101626396179\n",
      "Iteration 65/100, Loss: 0.5504977107048035\n",
      "Iteration 66/100, Loss: 0.5473583936691284\n",
      "Iteration 67/100, Loss: 0.5442898869514465\n",
      "Iteration 68/100, Loss: 0.5412898659706116\n",
      "Iteration 69/100, Loss: 0.5383561253547668\n",
      "Iteration 70/100, Loss: 0.5354868173599243\n",
      "Iteration 71/100, Loss: 0.5326800346374512\n",
      "Iteration 72/100, Loss: 0.5299338102340698\n",
      "Iteration 73/100, Loss: 0.5272462964057922\n",
      "Iteration 74/100, Loss: 0.5246157050132751\n",
      "Iteration 75/100, Loss: 0.5220403671264648\n",
      "Iteration 76/100, Loss: 0.5195184946060181\n",
      "Iteration 77/100, Loss: 0.5170487761497498\n",
      "Iteration 78/100, Loss: 0.5146296620368958\n",
      "Iteration 79/100, Loss: 0.5122596621513367\n",
      "Iteration 80/100, Loss: 0.5099372863769531\n",
      "Iteration 81/100, Loss: 0.5076613426208496\n",
      "Iteration 82/100, Loss: 0.5054304003715515\n",
      "Iteration 83/100, Loss: 0.5032432079315186\n",
      "Iteration 84/100, Loss: 0.5010985136032104\n",
      "Iteration 85/100, Loss: 0.49899518489837646\n",
      "Iteration 86/100, Loss: 0.49693214893341064\n",
      "Iteration 87/100, Loss: 0.49490824341773987\n",
      "Iteration 88/100, Loss: 0.4929223954677582\n",
      "Iteration 89/100, Loss: 0.4909736216068268\n",
      "Iteration 90/100, Loss: 0.4890609085559845\n",
      "Iteration 91/100, Loss: 0.48718321323394775\n",
      "Iteration 92/100, Loss: 0.4853396415710449\n",
      "Iteration 93/100, Loss: 0.4835294187068939\n",
      "Iteration 94/100, Loss: 0.4817516505718231\n",
      "Iteration 95/100, Loss: 0.48000553250312805\n",
      "Iteration 96/100, Loss: 0.4782901704311371\n",
      "Iteration 97/100, Loss: 0.47660475969314575\n",
      "Iteration 98/100, Loss: 0.4749484360218048\n",
      "Iteration 99/100, Loss: 0.4733205735683441\n",
      "Iteration 100/100, Loss: 0.47172051668167114\n",
      "Pruning Step 15\n",
      "Iteration 1/100, Loss: 1.2149344682693481\n",
      "Iteration 2/100, Loss: 1.1675395965576172\n",
      "Iteration 3/100, Loss: 1.1310687065124512\n",
      "Iteration 4/100, Loss: 1.101362943649292\n",
      "Iteration 5/100, Loss: 1.076153039932251\n",
      "Iteration 6/100, Loss: 1.0540646314620972\n",
      "Iteration 7/100, Loss: 1.0342179536819458\n",
      "Iteration 8/100, Loss: 1.0160272121429443\n",
      "Iteration 9/100, Loss: 0.9991005063056946\n",
      "Iteration 10/100, Loss: 0.9831600189208984\n",
      "Iteration 11/100, Loss: 0.9680183529853821\n",
      "Iteration 12/100, Loss: 0.9535432457923889\n",
      "Iteration 13/100, Loss: 0.9396365880966187\n",
      "Iteration 14/100, Loss: 0.9262291789054871\n",
      "Iteration 15/100, Loss: 0.9132705926895142\n",
      "Iteration 16/100, Loss: 0.9007259011268616\n",
      "Iteration 17/100, Loss: 0.8885678648948669\n",
      "Iteration 18/100, Loss: 0.8767750263214111\n",
      "Iteration 19/100, Loss: 0.8653298020362854\n",
      "Iteration 20/100, Loss: 0.8542177081108093\n",
      "Iteration 21/100, Loss: 0.8434250354766846\n",
      "Iteration 22/100, Loss: 0.8329417705535889\n",
      "Iteration 23/100, Loss: 0.8227575421333313\n",
      "Iteration 24/100, Loss: 0.8128625154495239\n",
      "Iteration 25/100, Loss: 0.8032474517822266\n",
      "Iteration 26/100, Loss: 0.7939037680625916\n",
      "Iteration 27/100, Loss: 0.7848232388496399\n",
      "Iteration 28/100, Loss: 0.775997519493103\n",
      "Iteration 29/100, Loss: 0.7674186825752258\n",
      "Iteration 30/100, Loss: 0.7590792775154114\n",
      "Iteration 31/100, Loss: 0.7509721517562866\n",
      "Iteration 32/100, Loss: 0.743090033531189\n",
      "Iteration 33/100, Loss: 0.7354259490966797\n",
      "Iteration 34/100, Loss: 0.7279731035232544\n",
      "Iteration 35/100, Loss: 0.7207239866256714\n",
      "Iteration 36/100, Loss: 0.713672399520874\n",
      "Iteration 37/100, Loss: 0.7068122625350952\n",
      "Iteration 38/100, Loss: 0.7001368999481201\n",
      "Iteration 39/100, Loss: 0.6936405301094055\n",
      "Iteration 40/100, Loss: 0.6873172521591187\n",
      "Iteration 41/100, Loss: 0.6811615228652954\n",
      "Iteration 42/100, Loss: 0.6751682758331299\n",
      "Iteration 43/100, Loss: 0.6693321466445923\n",
      "Iteration 44/100, Loss: 0.6636476516723633\n",
      "Iteration 45/100, Loss: 0.6581100225448608\n",
      "Iteration 46/100, Loss: 0.652714729309082\n",
      "Iteration 47/100, Loss: 0.6474570035934448\n",
      "Iteration 48/100, Loss: 0.6423325538635254\n",
      "Iteration 49/100, Loss: 0.6373370885848999\n",
      "Iteration 50/100, Loss: 0.6324662566184998\n",
      "Iteration 51/100, Loss: 0.6277161836624146\n",
      "Iteration 52/100, Loss: 0.6230832934379578\n",
      "Iteration 53/100, Loss: 0.6185636520385742\n",
      "Iteration 54/100, Loss: 0.6141539216041565\n",
      "Iteration 55/100, Loss: 0.6098505258560181\n",
      "Iteration 56/100, Loss: 0.6056501269340515\n",
      "Iteration 57/100, Loss: 0.6015497446060181\n",
      "Iteration 58/100, Loss: 0.5975459814071655\n",
      "Iteration 59/100, Loss: 0.5936360955238342\n",
      "Iteration 60/100, Loss: 0.5898169279098511\n",
      "Iteration 61/100, Loss: 0.5860857963562012\n",
      "Iteration 62/100, Loss: 0.5824400186538696\n",
      "Iteration 63/100, Loss: 0.5788770318031311\n",
      "Iteration 64/100, Loss: 0.575394332408905\n",
      "Iteration 65/100, Loss: 0.5719891786575317\n",
      "Iteration 66/100, Loss: 0.5686596632003784\n",
      "Iteration 67/100, Loss: 0.5654035210609436\n",
      "Iteration 68/100, Loss: 0.5622184872627258\n",
      "Iteration 69/100, Loss: 0.559102475643158\n",
      "Iteration 70/100, Loss: 0.5560534000396729\n",
      "Iteration 71/100, Loss: 0.5530692934989929\n",
      "Iteration 72/100, Loss: 0.5501484274864197\n",
      "Iteration 73/100, Loss: 0.5472889542579651\n",
      "Iteration 74/100, Loss: 0.5444890260696411\n",
      "Iteration 75/100, Loss: 0.5417469143867493\n",
      "Iteration 76/100, Loss: 0.5390612483024597\n",
      "Iteration 77/100, Loss: 0.5364301800727844\n",
      "Iteration 78/100, Loss: 0.533852219581604\n",
      "Iteration 79/100, Loss: 0.5313257575035095\n",
      "Iteration 80/100, Loss: 0.5288495421409607\n",
      "Iteration 81/100, Loss: 0.5264223217964172\n",
      "Iteration 82/100, Loss: 0.5240424871444702\n",
      "Iteration 83/100, Loss: 0.5217088460922241\n",
      "Iteration 84/100, Loss: 0.5194201469421387\n",
      "Iteration 85/100, Loss: 0.517175018787384\n",
      "Iteration 86/100, Loss: 0.514972448348999\n",
      "Iteration 87/100, Loss: 0.5128112435340881\n",
      "Iteration 88/100, Loss: 0.5106903910636902\n",
      "Iteration 89/100, Loss: 0.5086087584495544\n",
      "Iteration 90/100, Loss: 0.506565272808075\n",
      "Iteration 91/100, Loss: 0.5045590996742249\n",
      "Iteration 92/100, Loss: 0.5025890469551086\n",
      "Iteration 93/100, Loss: 0.500654399394989\n",
      "Iteration 94/100, Loss: 0.4987542927265167\n",
      "Iteration 95/100, Loss: 0.49688759446144104\n",
      "Iteration 96/100, Loss: 0.4950536787509918\n",
      "Iteration 97/100, Loss: 0.49325162172317505\n",
      "Iteration 98/100, Loss: 0.4914807975292206\n",
      "Iteration 99/100, Loss: 0.48974016308784485\n",
      "Iteration 100/100, Loss: 0.4880290627479553\n",
      "Pruning Step 0\n",
      "Iteration 1/100, Loss: 2.327918529510498\n",
      "Iteration 2/100, Loss: 2.2524969577789307\n",
      "Iteration 3/100, Loss: 2.185032844543457\n",
      "Iteration 4/100, Loss: 2.1204965114593506\n",
      "Iteration 5/100, Loss: 2.056241273880005\n",
      "Iteration 6/100, Loss: 1.990924596786499\n",
      "Iteration 7/100, Loss: 1.9239336252212524\n",
      "Iteration 8/100, Loss: 1.85539710521698\n",
      "Iteration 9/100, Loss: 1.7854983806610107\n",
      "Iteration 10/100, Loss: 1.7148276567459106\n",
      "Iteration 11/100, Loss: 1.6440469026565552\n",
      "Iteration 12/100, Loss: 1.573962926864624\n",
      "Iteration 13/100, Loss: 1.5052169561386108\n",
      "Iteration 14/100, Loss: 1.438450813293457\n",
      "Iteration 15/100, Loss: 1.3742713928222656\n",
      "Iteration 16/100, Loss: 1.3130409717559814\n",
      "Iteration 17/100, Loss: 1.255123257637024\n",
      "Iteration 18/100, Loss: 1.200688123703003\n",
      "Iteration 19/100, Loss: 1.1497995853424072\n",
      "Iteration 20/100, Loss: 1.1024583578109741\n",
      "Iteration 21/100, Loss: 1.0585873126983643\n",
      "Iteration 22/100, Loss: 1.0180171728134155\n",
      "Iteration 23/100, Loss: 0.98054438829422\n",
      "Iteration 24/100, Loss: 0.9459441304206848\n",
      "Iteration 25/100, Loss: 0.9139752984046936\n",
      "Iteration 26/100, Loss: 0.8844289779663086\n",
      "Iteration 27/100, Loss: 0.8571074604988098\n",
      "Iteration 28/100, Loss: 0.8318148255348206\n",
      "Iteration 29/100, Loss: 0.8083705306053162\n",
      "Iteration 30/100, Loss: 0.786612868309021\n",
      "Iteration 31/100, Loss: 0.7663823962211609\n",
      "Iteration 32/100, Loss: 0.747534453868866\n",
      "Iteration 33/100, Loss: 0.7299327850341797\n",
      "Iteration 34/100, Loss: 0.7134707570075989\n",
      "Iteration 35/100, Loss: 0.6980545520782471\n",
      "Iteration 36/100, Loss: 0.6835903525352478\n",
      "Iteration 37/100, Loss: 0.6699914336204529\n",
      "Iteration 38/100, Loss: 0.6571913957595825\n",
      "Iteration 39/100, Loss: 0.6451249122619629\n",
      "Iteration 40/100, Loss: 0.6337307095527649\n",
      "Iteration 41/100, Loss: 0.6229549646377563\n",
      "Iteration 42/100, Loss: 0.6127519607543945\n",
      "Iteration 43/100, Loss: 0.6030822992324829\n",
      "Iteration 44/100, Loss: 0.593904435634613\n",
      "Iteration 45/100, Loss: 0.5851821899414062\n",
      "Iteration 46/100, Loss: 0.576882004737854\n",
      "Iteration 47/100, Loss: 0.5689782500267029\n",
      "Iteration 48/100, Loss: 0.5614413022994995\n",
      "Iteration 49/100, Loss: 0.554241955280304\n",
      "Iteration 50/100, Loss: 0.5473610162734985\n",
      "Iteration 51/100, Loss: 0.5407780408859253\n",
      "Iteration 52/100, Loss: 0.5344763398170471\n",
      "Iteration 53/100, Loss: 0.5284388661384583\n",
      "Iteration 54/100, Loss: 0.5226478576660156\n",
      "Iteration 55/100, Loss: 0.5170884728431702\n",
      "Iteration 56/100, Loss: 0.5117462277412415\n",
      "Iteration 57/100, Loss: 0.506610631942749\n",
      "Iteration 58/100, Loss: 0.5016680359840393\n",
      "Iteration 59/100, Loss: 0.49690723419189453\n",
      "Iteration 60/100, Loss: 0.49231967329978943\n",
      "Iteration 61/100, Loss: 0.48789575695991516\n",
      "Iteration 62/100, Loss: 0.4836280345916748\n",
      "Iteration 63/100, Loss: 0.4795070290565491\n",
      "Iteration 64/100, Loss: 0.47552603483200073\n",
      "Iteration 65/100, Loss: 0.471676766872406\n",
      "Iteration 66/100, Loss: 0.4679529666900635\n",
      "Iteration 67/100, Loss: 0.4643484950065613\n",
      "Iteration 68/100, Loss: 0.4608582556247711\n",
      "Iteration 69/100, Loss: 0.4574759006500244\n",
      "Iteration 70/100, Loss: 0.45419618487358093\n",
      "Iteration 71/100, Loss: 0.45101526379585266\n",
      "Iteration 72/100, Loss: 0.4479278326034546\n",
      "Iteration 73/100, Loss: 0.44492945075035095\n",
      "Iteration 74/100, Loss: 0.4420173168182373\n",
      "Iteration 75/100, Loss: 0.43918678164482117\n",
      "Iteration 76/100, Loss: 0.4364347755908966\n",
      "Iteration 77/100, Loss: 0.433757483959198\n",
      "Iteration 78/100, Loss: 0.4311518669128418\n",
      "Iteration 79/100, Loss: 0.42861464619636536\n",
      "Iteration 80/100, Loss: 0.42614349722862244\n",
      "Iteration 81/100, Loss: 0.4237355589866638\n",
      "Iteration 82/100, Loss: 0.4213888347148895\n",
      "Iteration 83/100, Loss: 0.41909998655319214\n",
      "Iteration 84/100, Loss: 0.4168669879436493\n",
      "Iteration 85/100, Loss: 0.41468730568885803\n",
      "Iteration 86/100, Loss: 0.41255900263786316\n",
      "Iteration 87/100, Loss: 0.41048046946525574\n",
      "Iteration 88/100, Loss: 0.40845003724098206\n",
      "Iteration 89/100, Loss: 0.40646591782569885\n",
      "Iteration 90/100, Loss: 0.4045267105102539\n",
      "Iteration 91/100, Loss: 0.40263137221336365\n",
      "Iteration 92/100, Loss: 0.4007772207260132\n",
      "Iteration 93/100, Loss: 0.39896270632743835\n",
      "Iteration 94/100, Loss: 0.3971867859363556\n",
      "Iteration 95/100, Loss: 0.39544805884361267\n",
      "Iteration 96/100, Loss: 0.39374491572380066\n",
      "Iteration 97/100, Loss: 0.3920769989490509\n",
      "Iteration 98/100, Loss: 0.3904429078102112\n",
      "Iteration 99/100, Loss: 0.3888413608074188\n",
      "Iteration 100/100, Loss: 0.3872712254524231\n",
      "Pruning Step 1\n",
      "Iteration 1/100, Loss: 2.139874219894409\n",
      "Iteration 2/100, Loss: 2.0456368923187256\n",
      "Iteration 3/100, Loss: 1.9591028690338135\n",
      "Iteration 4/100, Loss: 1.8753297328948975\n",
      "Iteration 5/100, Loss: 1.792872428894043\n",
      "Iteration 6/100, Loss: 1.7117379903793335\n",
      "Iteration 7/100, Loss: 1.6323336362838745\n",
      "Iteration 8/100, Loss: 1.5552937984466553\n",
      "Iteration 9/100, Loss: 1.4811906814575195\n",
      "Iteration 10/100, Loss: 1.4104464054107666\n",
      "Iteration 11/100, Loss: 1.343442678451538\n",
      "Iteration 12/100, Loss: 1.2804917097091675\n",
      "Iteration 13/100, Loss: 1.22171950340271\n",
      "Iteration 14/100, Loss: 1.1671539545059204\n",
      "Iteration 15/100, Loss: 1.1166882514953613\n",
      "Iteration 16/100, Loss: 1.0701409578323364\n",
      "Iteration 17/100, Loss: 1.0273014307022095\n",
      "Iteration 18/100, Loss: 0.9879292249679565\n",
      "Iteration 19/100, Loss: 0.9517660140991211\n",
      "Iteration 20/100, Loss: 0.9185104370117188\n",
      "Iteration 21/100, Loss: 0.8879181742668152\n",
      "Iteration 22/100, Loss: 0.8597300052642822\n",
      "Iteration 23/100, Loss: 0.8337256908416748\n",
      "Iteration 24/100, Loss: 0.809704065322876\n",
      "Iteration 25/100, Loss: 0.7874656319618225\n",
      "Iteration 26/100, Loss: 0.7668339610099792\n",
      "Iteration 27/100, Loss: 0.74766606092453\n",
      "Iteration 28/100, Loss: 0.7298325896263123\n",
      "Iteration 29/100, Loss: 0.7132071256637573\n",
      "Iteration 30/100, Loss: 0.6976804733276367\n",
      "Iteration 31/100, Loss: 0.6831536889076233\n",
      "Iteration 32/100, Loss: 0.6695321798324585\n",
      "Iteration 33/100, Loss: 0.6567341089248657\n",
      "Iteration 34/100, Loss: 0.6446963548660278\n",
      "Iteration 35/100, Loss: 0.6333518028259277\n",
      "Iteration 36/100, Loss: 0.6226442456245422\n",
      "Iteration 37/100, Loss: 0.612524151802063\n",
      "Iteration 38/100, Loss: 0.6029456853866577\n",
      "Iteration 39/100, Loss: 0.593867301940918\n",
      "Iteration 40/100, Loss: 0.5852514505386353\n",
      "Iteration 41/100, Loss: 0.5770604014396667\n",
      "Iteration 42/100, Loss: 0.5692634582519531\n",
      "Iteration 43/100, Loss: 0.5618326663970947\n",
      "Iteration 44/100, Loss: 0.5547409057617188\n",
      "Iteration 45/100, Loss: 0.5479663610458374\n",
      "Iteration 46/100, Loss: 0.5414875745773315\n",
      "Iteration 47/100, Loss: 0.5352861881256104\n",
      "Iteration 48/100, Loss: 0.5293444395065308\n",
      "Iteration 49/100, Loss: 0.5236457586288452\n",
      "Iteration 50/100, Loss: 0.5181751847267151\n",
      "Iteration 51/100, Loss: 0.5129185914993286\n",
      "Iteration 52/100, Loss: 0.5078644156455994\n",
      "Iteration 53/100, Loss: 0.503000795841217\n",
      "Iteration 54/100, Loss: 0.4983177185058594\n",
      "Iteration 55/100, Loss: 0.4938059151172638\n",
      "Iteration 56/100, Loss: 0.4894554018974304\n",
      "Iteration 57/100, Loss: 0.4852575957775116\n",
      "Iteration 58/100, Loss: 0.4812040627002716\n",
      "Iteration 59/100, Loss: 0.4772873818874359\n",
      "Iteration 60/100, Loss: 0.4734996557235718\n",
      "Iteration 61/100, Loss: 0.4698348939418793\n",
      "Iteration 62/100, Loss: 0.46628716588020325\n",
      "Iteration 63/100, Loss: 0.46285104751586914\n",
      "Iteration 64/100, Loss: 0.45952126383781433\n",
      "Iteration 65/100, Loss: 0.4562916159629822\n",
      "Iteration 66/100, Loss: 0.45315808057785034\n",
      "Iteration 67/100, Loss: 0.45011642575263977\n",
      "Iteration 68/100, Loss: 0.4471627175807953\n",
      "Iteration 69/100, Loss: 0.44429367780685425\n",
      "Iteration 70/100, Loss: 0.44150468707084656\n",
      "Iteration 71/100, Loss: 0.43879184126853943\n",
      "Iteration 72/100, Loss: 0.43615221977233887\n",
      "Iteration 73/100, Loss: 0.4335831105709076\n",
      "Iteration 74/100, Loss: 0.43108150362968445\n",
      "Iteration 75/100, Loss: 0.42864495515823364\n",
      "Iteration 76/100, Loss: 0.4262703061103821\n",
      "Iteration 77/100, Loss: 0.4239552319049835\n",
      "Iteration 78/100, Loss: 0.4216975271701813\n",
      "Iteration 79/100, Loss: 0.41949403285980225\n",
      "Iteration 80/100, Loss: 0.4173433780670166\n",
      "Iteration 81/100, Loss: 0.41524356603622437\n",
      "Iteration 82/100, Loss: 0.4131925702095032\n",
      "Iteration 83/100, Loss: 0.4111883342266083\n",
      "Iteration 84/100, Loss: 0.4092295467853546\n",
      "Iteration 85/100, Loss: 0.40731412172317505\n",
      "Iteration 86/100, Loss: 0.40544068813323975\n",
      "Iteration 87/100, Loss: 0.40360739827156067\n",
      "Iteration 88/100, Loss: 0.40181371569633484\n",
      "Iteration 89/100, Loss: 0.40005794167518616\n",
      "Iteration 90/100, Loss: 0.39833831787109375\n",
      "Iteration 91/100, Loss: 0.39665400981903076\n",
      "Iteration 92/100, Loss: 0.39500415325164795\n",
      "Iteration 93/100, Loss: 0.3933873772621155\n",
      "Iteration 94/100, Loss: 0.3918018937110901\n",
      "Iteration 95/100, Loss: 0.3902475833892822\n",
      "Iteration 96/100, Loss: 0.3887229859828949\n",
      "Iteration 97/100, Loss: 0.3872268795967102\n",
      "Iteration 98/100, Loss: 0.38575923442840576\n",
      "Iteration 99/100, Loss: 0.38431867957115173\n",
      "Iteration 100/100, Loss: 0.38290444016456604\n",
      "Pruning Step 2\n",
      "Iteration 1/100, Loss: 1.9266777038574219\n",
      "Iteration 2/100, Loss: 1.8252265453338623\n",
      "Iteration 3/100, Loss: 1.7317003011703491\n",
      "Iteration 4/100, Loss: 1.6429911851882935\n",
      "Iteration 5/100, Loss: 1.5587304830551147\n",
      "Iteration 6/100, Loss: 1.4792393445968628\n",
      "Iteration 7/100, Loss: 1.4047447443008423\n",
      "Iteration 8/100, Loss: 1.335340142250061\n",
      "Iteration 9/100, Loss: 1.2709834575653076\n",
      "Iteration 10/100, Loss: 1.2115317583084106\n",
      "Iteration 11/100, Loss: 1.1567338705062866\n",
      "Iteration 12/100, Loss: 1.1063244342803955\n",
      "Iteration 13/100, Loss: 1.0600590705871582\n",
      "Iteration 14/100, Loss: 1.0176383256912231\n",
      "Iteration 15/100, Loss: 0.9787496328353882\n",
      "Iteration 16/100, Loss: 0.9430847764015198\n",
      "Iteration 17/100, Loss: 0.9103660583496094\n",
      "Iteration 18/100, Loss: 0.8803088665008545\n",
      "Iteration 19/100, Loss: 0.8526484370231628\n",
      "Iteration 20/100, Loss: 0.8271598219871521\n",
      "Iteration 21/100, Loss: 0.8036308884620667\n",
      "Iteration 22/100, Loss: 0.7818711996078491\n",
      "Iteration 23/100, Loss: 0.7617015242576599\n",
      "Iteration 24/100, Loss: 0.7429741024971008\n",
      "Iteration 25/100, Loss: 0.7255483865737915\n",
      "Iteration 26/100, Loss: 0.7093057036399841\n",
      "Iteration 27/100, Loss: 0.6941338777542114\n",
      "Iteration 28/100, Loss: 0.679934561252594\n",
      "Iteration 29/100, Loss: 0.6666204333305359\n",
      "Iteration 30/100, Loss: 0.654115617275238\n",
      "Iteration 31/100, Loss: 0.6423501968383789\n",
      "Iteration 32/100, Loss: 0.6312636733055115\n",
      "Iteration 33/100, Loss: 0.6207995414733887\n",
      "Iteration 34/100, Loss: 0.6109076142311096\n",
      "Iteration 35/100, Loss: 0.6015458106994629\n",
      "Iteration 36/100, Loss: 0.5926688313484192\n",
      "Iteration 37/100, Loss: 0.5842418074607849\n",
      "Iteration 38/100, Loss: 0.5762313604354858\n",
      "Iteration 39/100, Loss: 0.5686069130897522\n",
      "Iteration 40/100, Loss: 0.5613399147987366\n",
      "Iteration 41/100, Loss: 0.5544064044952393\n",
      "Iteration 42/100, Loss: 0.5477840304374695\n",
      "Iteration 43/100, Loss: 0.5414520502090454\n",
      "Iteration 44/100, Loss: 0.5353904962539673\n",
      "Iteration 45/100, Loss: 0.5295825004577637\n",
      "Iteration 46/100, Loss: 0.5240132212638855\n",
      "Iteration 47/100, Loss: 0.5186680555343628\n",
      "Iteration 48/100, Loss: 0.5135337114334106\n",
      "Iteration 49/100, Loss: 0.5085965394973755\n",
      "Iteration 50/100, Loss: 0.5038464069366455\n",
      "Iteration 51/100, Loss: 0.4992726445198059\n",
      "Iteration 52/100, Loss: 0.49486348032951355\n",
      "Iteration 53/100, Loss: 0.49060964584350586\n",
      "Iteration 54/100, Loss: 0.486503005027771\n",
      "Iteration 55/100, Loss: 0.48253652453422546\n",
      "Iteration 56/100, Loss: 0.4787035882472992\n",
      "Iteration 57/100, Loss: 0.4749968647956848\n",
      "Iteration 58/100, Loss: 0.4714093804359436\n",
      "Iteration 59/100, Loss: 0.4679352343082428\n",
      "Iteration 60/100, Loss: 0.4645703434944153\n",
      "Iteration 61/100, Loss: 0.4613085389137268\n",
      "Iteration 62/100, Loss: 0.45814475417137146\n",
      "Iteration 63/100, Loss: 0.4550749957561493\n",
      "Iteration 64/100, Loss: 0.4520947337150574\n",
      "Iteration 65/100, Loss: 0.4491998553276062\n",
      "Iteration 66/100, Loss: 0.4463869035243988\n",
      "Iteration 67/100, Loss: 0.44365182518959045\n",
      "Iteration 68/100, Loss: 0.44099172949790955\n",
      "Iteration 69/100, Loss: 0.43840292096138\n",
      "Iteration 70/100, Loss: 0.4358821213245392\n",
      "Iteration 71/100, Loss: 0.43342655897140503\n",
      "Iteration 72/100, Loss: 0.4310341775417328\n",
      "Iteration 73/100, Loss: 0.42870235443115234\n",
      "Iteration 74/100, Loss: 0.4264284372329712\n",
      "Iteration 75/100, Loss: 0.4242100715637207\n",
      "Iteration 76/100, Loss: 0.4220452606678009\n",
      "Iteration 77/100, Loss: 0.4199323058128357\n",
      "Iteration 78/100, Loss: 0.4178686738014221\n",
      "Iteration 79/100, Loss: 0.41585278511047363\n",
      "Iteration 80/100, Loss: 0.41388317942619324\n",
      "Iteration 81/100, Loss: 0.41195833683013916\n",
      "Iteration 82/100, Loss: 0.4100760519504547\n",
      "Iteration 83/100, Loss: 0.4082351326942444\n",
      "Iteration 84/100, Loss: 0.40643396973609924\n",
      "Iteration 85/100, Loss: 0.4046706259250641\n",
      "Iteration 86/100, Loss: 0.40294450521469116\n",
      "Iteration 87/100, Loss: 0.40125423669815063\n",
      "Iteration 88/100, Loss: 0.39959853887557983\n",
      "Iteration 89/100, Loss: 0.39797675609588623\n",
      "Iteration 90/100, Loss: 0.3963874280452728\n",
      "Iteration 91/100, Loss: 0.3948294222354889\n",
      "Iteration 92/100, Loss: 0.39330172538757324\n",
      "Iteration 93/100, Loss: 0.3918032944202423\n",
      "Iteration 94/100, Loss: 0.39033299684524536\n",
      "Iteration 95/100, Loss: 0.3888900578022003\n",
      "Iteration 96/100, Loss: 0.3874734044075012\n",
      "Iteration 97/100, Loss: 0.38608255982398987\n",
      "Iteration 98/100, Loss: 0.3847164809703827\n",
      "Iteration 99/100, Loss: 0.3833742141723633\n",
      "Iteration 100/100, Loss: 0.3820553421974182\n",
      "Pruning Step 3\n",
      "Iteration 1/100, Loss: 1.7380602359771729\n",
      "Iteration 2/100, Loss: 1.6365569829940796\n",
      "Iteration 3/100, Loss: 1.5481529235839844\n",
      "Iteration 4/100, Loss: 1.4671266078948975\n",
      "Iteration 5/100, Loss: 1.3924249410629272\n",
      "Iteration 6/100, Loss: 1.323583960533142\n",
      "Iteration 7/100, Loss: 1.2601734399795532\n",
      "Iteration 8/100, Loss: 1.2017854452133179\n",
      "Iteration 9/100, Loss: 1.1481130123138428\n",
      "Iteration 10/100, Loss: 1.0988190174102783\n",
      "Iteration 11/100, Loss: 1.0535873174667358\n",
      "Iteration 12/100, Loss: 1.0121111869812012\n",
      "Iteration 13/100, Loss: 0.974086344242096\n",
      "Iteration 14/100, Loss: 0.9392011165618896\n",
      "Iteration 15/100, Loss: 0.9071682095527649\n",
      "Iteration 16/100, Loss: 0.8777288794517517\n",
      "Iteration 17/100, Loss: 0.8506304025650024\n",
      "Iteration 18/100, Loss: 0.8256397247314453\n",
      "Iteration 19/100, Loss: 0.8025575876235962\n",
      "Iteration 20/100, Loss: 0.7811964154243469\n",
      "Iteration 21/100, Loss: 0.7613893151283264\n",
      "Iteration 22/100, Loss: 0.7429888248443604\n",
      "Iteration 23/100, Loss: 0.7258661389350891\n",
      "Iteration 24/100, Loss: 0.7098997831344604\n",
      "Iteration 25/100, Loss: 0.694981038570404\n",
      "Iteration 26/100, Loss: 0.6810128092765808\n",
      "Iteration 27/100, Loss: 0.6679161190986633\n",
      "Iteration 28/100, Loss: 0.6556141972541809\n",
      "Iteration 29/100, Loss: 0.6440353989601135\n",
      "Iteration 30/100, Loss: 0.6331214308738708\n",
      "Iteration 31/100, Loss: 0.622817873954773\n",
      "Iteration 32/100, Loss: 0.6130754947662354\n",
      "Iteration 33/100, Loss: 0.603851318359375\n",
      "Iteration 34/100, Loss: 0.5951061844825745\n",
      "Iteration 35/100, Loss: 0.5868030190467834\n",
      "Iteration 36/100, Loss: 0.5789093375205994\n",
      "Iteration 37/100, Loss: 0.5713945031166077\n",
      "Iteration 38/100, Loss: 0.5642314553260803\n",
      "Iteration 39/100, Loss: 0.5573962330818176\n",
      "Iteration 40/100, Loss: 0.5508669018745422\n",
      "Iteration 41/100, Loss: 0.5446248650550842\n",
      "Iteration 42/100, Loss: 0.5386502742767334\n",
      "Iteration 43/100, Loss: 0.5329253673553467\n",
      "Iteration 44/100, Loss: 0.5274351835250854\n",
      "Iteration 45/100, Loss: 0.522165834903717\n",
      "Iteration 46/100, Loss: 0.5171027779579163\n",
      "Iteration 47/100, Loss: 0.5122341513633728\n",
      "Iteration 48/100, Loss: 0.5075499415397644\n",
      "Iteration 49/100, Loss: 0.5030393004417419\n",
      "Iteration 50/100, Loss: 0.498691588640213\n",
      "Iteration 51/100, Loss: 0.4944976568222046\n",
      "Iteration 52/100, Loss: 0.49044960737228394\n",
      "Iteration 53/100, Loss: 0.486540287733078\n",
      "Iteration 54/100, Loss: 0.48276159167289734\n",
      "Iteration 55/100, Loss: 0.47910746932029724\n",
      "Iteration 56/100, Loss: 0.47557127475738525\n",
      "Iteration 57/100, Loss: 0.472147136926651\n",
      "Iteration 58/100, Loss: 0.4688301682472229\n",
      "Iteration 59/100, Loss: 0.4656152129173279\n",
      "Iteration 60/100, Loss: 0.4624967575073242\n",
      "Iteration 61/100, Loss: 0.45946991443634033\n",
      "Iteration 62/100, Loss: 0.4565314054489136\n",
      "Iteration 63/100, Loss: 0.45367681980133057\n",
      "Iteration 64/100, Loss: 0.4509027898311615\n",
      "Iteration 65/100, Loss: 0.4482055604457855\n",
      "Iteration 66/100, Loss: 0.44558197259902954\n",
      "Iteration 67/100, Loss: 0.4430287480354309\n",
      "Iteration 68/100, Loss: 0.440542995929718\n",
      "Iteration 69/100, Loss: 0.43812140822410583\n",
      "Iteration 70/100, Loss: 0.4357621669769287\n",
      "Iteration 71/100, Loss: 0.43346258997917175\n",
      "Iteration 72/100, Loss: 0.4312201738357544\n",
      "Iteration 73/100, Loss: 0.4290328919887543\n",
      "Iteration 74/100, Loss: 0.4268980920314789\n",
      "Iteration 75/100, Loss: 0.42481377720832825\n",
      "Iteration 76/100, Loss: 0.4227781891822815\n",
      "Iteration 77/100, Loss: 0.4207897484302521\n",
      "Iteration 78/100, Loss: 0.41884586215019226\n",
      "Iteration 79/100, Loss: 0.4169449806213379\n",
      "Iteration 80/100, Loss: 0.4150860011577606\n",
      "Iteration 81/100, Loss: 0.4132673144340515\n",
      "Iteration 82/100, Loss: 0.41148796677589417\n",
      "Iteration 83/100, Loss: 0.4097464382648468\n",
      "Iteration 84/100, Loss: 0.40804117918014526\n",
      "Iteration 85/100, Loss: 0.406370609998703\n",
      "Iteration 86/100, Loss: 0.4047338664531708\n",
      "Iteration 87/100, Loss: 0.4031296968460083\n",
      "Iteration 88/100, Loss: 0.40155699849128723\n",
      "Iteration 89/100, Loss: 0.40001511573791504\n",
      "Iteration 90/100, Loss: 0.3985027074813843\n",
      "Iteration 91/100, Loss: 0.3970189392566681\n",
      "Iteration 92/100, Loss: 0.39556291699409485\n",
      "Iteration 93/100, Loss: 0.39413371682167053\n",
      "Iteration 94/100, Loss: 0.3927304446697235\n",
      "Iteration 95/100, Loss: 0.3913521468639374\n",
      "Iteration 96/100, Loss: 0.3899983763694763\n",
      "Iteration 97/100, Loss: 0.38866838812828064\n",
      "Iteration 98/100, Loss: 0.3873612582683563\n",
      "Iteration 99/100, Loss: 0.38607650995254517\n",
      "Iteration 100/100, Loss: 0.384813517332077\n",
      "Pruning Step 4\n",
      "Iteration 1/100, Loss: 1.5735987424850464\n",
      "Iteration 2/100, Loss: 1.4874073266983032\n",
      "Iteration 3/100, Loss: 1.4098117351531982\n",
      "Iteration 4/100, Loss: 1.3392647504806519\n",
      "Iteration 5/100, Loss: 1.2748216390609741\n",
      "Iteration 6/100, Loss: 1.2158035039901733\n",
      "Iteration 7/100, Loss: 1.1617432832717896\n",
      "Iteration 8/100, Loss: 1.11226487159729\n",
      "Iteration 9/100, Loss: 1.066993236541748\n",
      "Iteration 10/100, Loss: 1.0255194902420044\n",
      "Iteration 11/100, Loss: 0.9874883890151978\n",
      "Iteration 12/100, Loss: 0.9525683522224426\n",
      "Iteration 13/100, Loss: 0.9204682111740112\n",
      "Iteration 14/100, Loss: 0.8909165859222412\n",
      "Iteration 15/100, Loss: 0.8636670708656311\n",
      "Iteration 16/100, Loss: 0.8384941816329956\n",
      "Iteration 17/100, Loss: 0.815205991268158\n",
      "Iteration 18/100, Loss: 0.793621301651001\n",
      "Iteration 19/100, Loss: 0.773578405380249\n",
      "Iteration 20/100, Loss: 0.754932701587677\n",
      "Iteration 21/100, Loss: 0.737557590007782\n",
      "Iteration 22/100, Loss: 0.7213374972343445\n",
      "Iteration 23/100, Loss: 0.7061683535575867\n",
      "Iteration 24/100, Loss: 0.6919599771499634\n",
      "Iteration 25/100, Loss: 0.6786243319511414\n",
      "Iteration 26/100, Loss: 0.6660875082015991\n",
      "Iteration 27/100, Loss: 0.654283344745636\n",
      "Iteration 28/100, Loss: 0.6431511640548706\n",
      "Iteration 29/100, Loss: 0.6326359510421753\n",
      "Iteration 30/100, Loss: 0.6226896047592163\n",
      "Iteration 31/100, Loss: 0.6132679581642151\n",
      "Iteration 32/100, Loss: 0.604332685470581\n",
      "Iteration 33/100, Loss: 0.5958470702171326\n",
      "Iteration 34/100, Loss: 0.587778627872467\n",
      "Iteration 35/100, Loss: 0.5800970792770386\n",
      "Iteration 36/100, Loss: 0.5727758407592773\n",
      "Iteration 37/100, Loss: 0.5657918453216553\n",
      "Iteration 38/100, Loss: 0.5591221451759338\n",
      "Iteration 39/100, Loss: 0.5527458786964417\n",
      "Iteration 40/100, Loss: 0.5466437935829163\n",
      "Iteration 41/100, Loss: 0.5407987236976624\n",
      "Iteration 42/100, Loss: 0.5351942181587219\n",
      "Iteration 43/100, Loss: 0.5298160910606384\n",
      "Iteration 44/100, Loss: 0.5246503949165344\n",
      "Iteration 45/100, Loss: 0.5196843147277832\n",
      "Iteration 46/100, Loss: 0.5149054527282715\n",
      "Iteration 47/100, Loss: 0.5103041529655457\n",
      "Iteration 48/100, Loss: 0.5058704614639282\n",
      "Iteration 49/100, Loss: 0.5015949606895447\n",
      "Iteration 50/100, Loss: 0.49747008085250854\n",
      "Iteration 51/100, Loss: 0.49348723888397217\n",
      "Iteration 52/100, Loss: 0.4896387457847595\n",
      "Iteration 53/100, Loss: 0.48591750860214233\n",
      "Iteration 54/100, Loss: 0.48231711983680725\n",
      "Iteration 55/100, Loss: 0.47883114218711853\n",
      "Iteration 56/100, Loss: 0.4754546284675598\n",
      "Iteration 57/100, Loss: 0.4721823036670685\n",
      "Iteration 58/100, Loss: 0.4690093696117401\n",
      "Iteration 59/100, Loss: 0.46593087911605835\n",
      "Iteration 60/100, Loss: 0.46294310688972473\n",
      "Iteration 61/100, Loss: 0.4600411355495453\n",
      "Iteration 62/100, Loss: 0.45722097158432007\n",
      "Iteration 63/100, Loss: 0.4544791579246521\n",
      "Iteration 64/100, Loss: 0.4518126845359802\n",
      "Iteration 65/100, Loss: 0.44921794533729553\n",
      "Iteration 66/100, Loss: 0.44669145345687866\n",
      "Iteration 67/100, Loss: 0.44423139095306396\n",
      "Iteration 68/100, Loss: 0.4418350160121918\n",
      "Iteration 69/100, Loss: 0.43949922919273376\n",
      "Iteration 70/100, Loss: 0.4372215270996094\n",
      "Iteration 71/100, Loss: 0.43499940633773804\n",
      "Iteration 72/100, Loss: 0.4328311085700989\n",
      "Iteration 73/100, Loss: 0.43071407079696655\n",
      "Iteration 74/100, Loss: 0.4286464750766754\n",
      "Iteration 75/100, Loss: 0.426626592874527\n",
      "Iteration 76/100, Loss: 0.4246530830860138\n",
      "Iteration 77/100, Loss: 0.42272424697875977\n",
      "Iteration 78/100, Loss: 0.420838326215744\n",
      "Iteration 79/100, Loss: 0.41899389028549194\n",
      "Iteration 80/100, Loss: 0.41718918085098267\n",
      "Iteration 81/100, Loss: 0.41542285680770874\n",
      "Iteration 82/100, Loss: 0.41369354724884033\n",
      "Iteration 83/100, Loss: 0.41200006008148193\n",
      "Iteration 84/100, Loss: 0.41034141182899475\n",
      "Iteration 85/100, Loss: 0.40871620178222656\n",
      "Iteration 86/100, Loss: 0.4071231186389923\n",
      "Iteration 87/100, Loss: 0.40556129813194275\n",
      "Iteration 88/100, Loss: 0.4040297865867615\n",
      "Iteration 89/100, Loss: 0.4025276005268097\n",
      "Iteration 90/100, Loss: 0.40105390548706055\n",
      "Iteration 91/100, Loss: 0.39960750937461853\n",
      "Iteration 92/100, Loss: 0.3981878459453583\n",
      "Iteration 93/100, Loss: 0.39679378271102905\n",
      "Iteration 94/100, Loss: 0.395424485206604\n",
      "Iteration 95/100, Loss: 0.3940792977809906\n",
      "Iteration 96/100, Loss: 0.39275768399238586\n",
      "Iteration 97/100, Loss: 0.3914588689804077\n",
      "Iteration 98/100, Loss: 0.390182226896286\n",
      "Iteration 99/100, Loss: 0.3889269530773163\n",
      "Iteration 100/100, Loss: 0.38769251108169556\n",
      "Pruning Step 5\n",
      "Iteration 1/100, Loss: 1.4277316331863403\n",
      "Iteration 2/100, Loss: 1.3551266193389893\n",
      "Iteration 3/100, Loss: 1.29111909866333\n",
      "Iteration 4/100, Loss: 1.2333835363388062\n",
      "Iteration 5/100, Loss: 1.180773138999939\n",
      "Iteration 6/100, Loss: 1.1325905323028564\n",
      "Iteration 7/100, Loss: 1.0883585214614868\n",
      "Iteration 8/100, Loss: 1.047674536705017\n",
      "Iteration 9/100, Loss: 1.0101815462112427\n",
      "Iteration 10/100, Loss: 0.9755640625953674\n",
      "Iteration 11/100, Loss: 0.9435461759567261\n",
      "Iteration 12/100, Loss: 0.9139020442962646\n",
      "Iteration 13/100, Loss: 0.8864313960075378\n",
      "Iteration 14/100, Loss: 0.8609421253204346\n",
      "Iteration 15/100, Loss: 0.8372506499290466\n",
      "Iteration 16/100, Loss: 0.8152050375938416\n",
      "Iteration 17/100, Loss: 0.7946616411209106\n",
      "Iteration 18/100, Loss: 0.7754945755004883\n",
      "Iteration 19/100, Loss: 0.7575823068618774\n",
      "Iteration 20/100, Loss: 0.7408201694488525\n",
      "Iteration 21/100, Loss: 0.7251110672950745\n",
      "Iteration 22/100, Loss: 0.7103708386421204\n",
      "Iteration 23/100, Loss: 0.6965178847312927\n",
      "Iteration 24/100, Loss: 0.6834772229194641\n",
      "Iteration 25/100, Loss: 0.671184241771698\n",
      "Iteration 26/100, Loss: 0.6595816612243652\n",
      "Iteration 27/100, Loss: 0.6486164331436157\n",
      "Iteration 28/100, Loss: 0.6382396221160889\n",
      "Iteration 29/100, Loss: 0.6284075975418091\n",
      "Iteration 30/100, Loss: 0.6190797090530396\n",
      "Iteration 31/100, Loss: 0.6102204322814941\n",
      "Iteration 32/100, Loss: 0.6017956137657166\n",
      "Iteration 33/100, Loss: 0.5937758088111877\n",
      "Iteration 34/100, Loss: 0.5861333608627319\n",
      "Iteration 35/100, Loss: 0.57884281873703\n",
      "Iteration 36/100, Loss: 0.5718812346458435\n",
      "Iteration 37/100, Loss: 0.565226674079895\n",
      "Iteration 38/100, Loss: 0.5588594675064087\n",
      "Iteration 39/100, Loss: 0.5527614951133728\n",
      "Iteration 40/100, Loss: 0.546916127204895\n",
      "Iteration 41/100, Loss: 0.5413083434104919\n",
      "Iteration 42/100, Loss: 0.5359241366386414\n",
      "Iteration 43/100, Loss: 0.5307495594024658\n",
      "Iteration 44/100, Loss: 0.5257729887962341\n",
      "Iteration 45/100, Loss: 0.5209834575653076\n",
      "Iteration 46/100, Loss: 0.5163705348968506\n",
      "Iteration 47/100, Loss: 0.5119247436523438\n",
      "Iteration 48/100, Loss: 0.5076366066932678\n",
      "Iteration 49/100, Loss: 0.5034974217414856\n",
      "Iteration 50/100, Loss: 0.49949911236763\n",
      "Iteration 51/100, Loss: 0.49563470482826233\n",
      "Iteration 52/100, Loss: 0.49189773201942444\n",
      "Iteration 53/100, Loss: 0.48828184604644775\n",
      "Iteration 54/100, Loss: 0.48478105664253235\n",
      "Iteration 55/100, Loss: 0.481390118598938\n",
      "Iteration 56/100, Loss: 0.4781038165092468\n",
      "Iteration 57/100, Loss: 0.47491657733917236\n",
      "Iteration 58/100, Loss: 0.4718233644962311\n",
      "Iteration 59/100, Loss: 0.46882060170173645\n",
      "Iteration 60/100, Loss: 0.46590423583984375\n",
      "Iteration 61/100, Loss: 0.46307021379470825\n",
      "Iteration 62/100, Loss: 0.4603150486946106\n",
      "Iteration 63/100, Loss: 0.4576353430747986\n",
      "Iteration 64/100, Loss: 0.4550280272960663\n",
      "Iteration 65/100, Loss: 0.45249009132385254\n",
      "Iteration 66/100, Loss: 0.45001834630966187\n",
      "Iteration 67/100, Loss: 0.4476102292537689\n",
      "Iteration 68/100, Loss: 0.4452630281448364\n",
      "Iteration 69/100, Loss: 0.44297417998313904\n",
      "Iteration 70/100, Loss: 0.4407415986061096\n",
      "Iteration 71/100, Loss: 0.4385630190372467\n",
      "Iteration 72/100, Loss: 0.4364362061023712\n",
      "Iteration 73/100, Loss: 0.4343593716621399\n",
      "Iteration 74/100, Loss: 0.4323306679725647\n",
      "Iteration 75/100, Loss: 0.4303482472896576\n",
      "Iteration 76/100, Loss: 0.42841073870658875\n",
      "Iteration 77/100, Loss: 0.42651641368865967\n",
      "Iteration 78/100, Loss: 0.42466336488723755\n",
      "Iteration 79/100, Loss: 0.4228503108024597\n",
      "Iteration 80/100, Loss: 0.4210759103298187\n",
      "Iteration 81/100, Loss: 0.4193388819694519\n",
      "Iteration 82/100, Loss: 0.41763782501220703\n",
      "Iteration 83/100, Loss: 0.4159716069698334\n",
      "Iteration 84/100, Loss: 0.41433900594711304\n",
      "Iteration 85/100, Loss: 0.4127388596534729\n",
      "Iteration 86/100, Loss: 0.41117024421691895\n",
      "Iteration 87/100, Loss: 0.4096324145793915\n",
      "Iteration 88/100, Loss: 0.40812385082244873\n",
      "Iteration 89/100, Loss: 0.40664374828338623\n",
      "Iteration 90/100, Loss: 0.4051913917064667\n",
      "Iteration 91/100, Loss: 0.4037657678127289\n",
      "Iteration 92/100, Loss: 0.4023660719394684\n",
      "Iteration 93/100, Loss: 0.4009915590286255\n",
      "Iteration 94/100, Loss: 0.3996415436267853\n",
      "Iteration 95/100, Loss: 0.39831531047821045\n",
      "Iteration 96/100, Loss: 0.39701178669929504\n",
      "Iteration 97/100, Loss: 0.39573022723197937\n",
      "Iteration 98/100, Loss: 0.3944704830646515\n",
      "Iteration 99/100, Loss: 0.3932318091392517\n",
      "Iteration 100/100, Loss: 0.3920137882232666\n",
      "Pruning Step 6\n",
      "Iteration 1/100, Loss: 1.3507076501846313\n",
      "Iteration 2/100, Loss: 1.2803027629852295\n",
      "Iteration 3/100, Loss: 1.223615050315857\n",
      "Iteration 4/100, Loss: 1.17420494556427\n",
      "Iteration 5/100, Loss: 1.1294804811477661\n",
      "Iteration 6/100, Loss: 1.0883879661560059\n",
      "Iteration 7/100, Loss: 1.0504093170166016\n",
      "Iteration 8/100, Loss: 1.0152275562286377\n",
      "Iteration 9/100, Loss: 0.982590913772583\n",
      "Iteration 10/100, Loss: 0.9522683024406433\n",
      "Iteration 11/100, Loss: 0.9240570068359375\n",
      "Iteration 12/100, Loss: 0.8977647423744202\n",
      "Iteration 13/100, Loss: 0.8732422590255737\n",
      "Iteration 14/100, Loss: 0.8503400683403015\n",
      "Iteration 15/100, Loss: 0.8289231657981873\n",
      "Iteration 16/100, Loss: 0.8088709712028503\n",
      "Iteration 17/100, Loss: 0.7900753021240234\n",
      "Iteration 18/100, Loss: 0.7724364995956421\n",
      "Iteration 19/100, Loss: 0.7558612823486328\n",
      "Iteration 20/100, Loss: 0.7402656078338623\n",
      "Iteration 21/100, Loss: 0.7255780100822449\n",
      "Iteration 22/100, Loss: 0.7117280960083008\n",
      "Iteration 23/100, Loss: 0.6986541748046875\n",
      "Iteration 24/100, Loss: 0.6862983703613281\n",
      "Iteration 25/100, Loss: 0.674608588218689\n",
      "Iteration 26/100, Loss: 0.6635338068008423\n",
      "Iteration 27/100, Loss: 0.6530282497406006\n",
      "Iteration 28/100, Loss: 0.6430532336235046\n",
      "Iteration 29/100, Loss: 0.6335732340812683\n",
      "Iteration 30/100, Loss: 0.6245545148849487\n",
      "Iteration 31/100, Loss: 0.6159657835960388\n",
      "Iteration 32/100, Loss: 0.6077778935432434\n",
      "Iteration 33/100, Loss: 0.5999656915664673\n",
      "Iteration 34/100, Loss: 0.5925042033195496\n",
      "Iteration 35/100, Loss: 0.5853704214096069\n",
      "Iteration 36/100, Loss: 0.5785441398620605\n",
      "Iteration 37/100, Loss: 0.5720070600509644\n",
      "Iteration 38/100, Loss: 0.5657409429550171\n",
      "Iteration 39/100, Loss: 0.5597303509712219\n",
      "Iteration 40/100, Loss: 0.5539607405662537\n",
      "Iteration 41/100, Loss: 0.5484175682067871\n",
      "Iteration 42/100, Loss: 0.5430876016616821\n",
      "Iteration 43/100, Loss: 0.5379588007926941\n",
      "Iteration 44/100, Loss: 0.5330201983451843\n",
      "Iteration 45/100, Loss: 0.5282612442970276\n",
      "Iteration 46/100, Loss: 0.5236720442771912\n",
      "Iteration 47/100, Loss: 0.5192440152168274\n",
      "Iteration 48/100, Loss: 0.5149691700935364\n",
      "Iteration 49/100, Loss: 0.5108385682106018\n",
      "Iteration 50/100, Loss: 0.506846010684967\n",
      "Iteration 51/100, Loss: 0.5029839873313904\n",
      "Iteration 52/100, Loss: 0.49924564361572266\n",
      "Iteration 53/100, Loss: 0.49562543630599976\n",
      "Iteration 54/100, Loss: 0.4921180009841919\n",
      "Iteration 55/100, Loss: 0.4887181520462036\n",
      "Iteration 56/100, Loss: 0.48542094230651855\n",
      "Iteration 57/100, Loss: 0.48222148418426514\n",
      "Iteration 58/100, Loss: 0.47911524772644043\n",
      "Iteration 59/100, Loss: 0.47609832882881165\n",
      "Iteration 60/100, Loss: 0.4731667935848236\n",
      "Iteration 61/100, Loss: 0.47031745314598083\n",
      "Iteration 62/100, Loss: 0.4675464332103729\n",
      "Iteration 63/100, Loss: 0.4648503065109253\n",
      "Iteration 64/100, Loss: 0.4622257947921753\n",
      "Iteration 65/100, Loss: 0.45967021584510803\n",
      "Iteration 66/100, Loss: 0.45718079805374146\n",
      "Iteration 67/100, Loss: 0.45475471019744873\n",
      "Iteration 68/100, Loss: 0.4523894488811493\n",
      "Iteration 69/100, Loss: 0.45008280873298645\n",
      "Iteration 70/100, Loss: 0.44783225655555725\n",
      "Iteration 71/100, Loss: 0.44563591480255127\n",
      "Iteration 72/100, Loss: 0.44349154829978943\n",
      "Iteration 73/100, Loss: 0.44139713048934937\n",
      "Iteration 74/100, Loss: 0.4393509328365326\n",
      "Iteration 75/100, Loss: 0.4373510181903839\n",
      "Iteration 76/100, Loss: 0.43539562821388245\n",
      "Iteration 77/100, Loss: 0.4334835410118103\n",
      "Iteration 78/100, Loss: 0.43161311745643616\n",
      "Iteration 79/100, Loss: 0.4297831356525421\n",
      "Iteration 80/100, Loss: 0.4279918074607849\n",
      "Iteration 81/100, Loss: 0.42623770236968994\n",
      "Iteration 82/100, Loss: 0.42451998591423035\n",
      "Iteration 83/100, Loss: 0.4228372275829315\n",
      "Iteration 84/100, Loss: 0.42118847370147705\n",
      "Iteration 85/100, Loss: 0.419572651386261\n",
      "Iteration 86/100, Loss: 0.4179884195327759\n",
      "Iteration 87/100, Loss: 0.41643497347831726\n",
      "Iteration 88/100, Loss: 0.4149114489555359\n",
      "Iteration 89/100, Loss: 0.4134168028831482\n",
      "Iteration 90/100, Loss: 0.41195014119148254\n",
      "Iteration 91/100, Loss: 0.41051048040390015\n",
      "Iteration 92/100, Loss: 0.4090968370437622\n",
      "Iteration 93/100, Loss: 0.4077087342739105\n",
      "Iteration 94/100, Loss: 0.4063454568386078\n",
      "Iteration 95/100, Loss: 0.4050061106681824\n",
      "Iteration 96/100, Loss: 0.4036900997161865\n",
      "Iteration 97/100, Loss: 0.40239647030830383\n",
      "Iteration 98/100, Loss: 0.4011248052120209\n",
      "Iteration 99/100, Loss: 0.3998744487762451\n",
      "Iteration 100/100, Loss: 0.3986448049545288\n",
      "Pruning Step 7\n",
      "Iteration 1/100, Loss: 1.2537599802017212\n",
      "Iteration 2/100, Loss: 1.195627212524414\n",
      "Iteration 3/100, Loss: 1.149288535118103\n",
      "Iteration 4/100, Loss: 1.1082797050476074\n",
      "Iteration 5/100, Loss: 1.070912480354309\n",
      "Iteration 6/100, Loss: 1.036466360092163\n",
      "Iteration 7/100, Loss: 1.0045236349105835\n",
      "Iteration 8/100, Loss: 0.9747983813285828\n",
      "Iteration 9/100, Loss: 0.9470572471618652\n",
      "Iteration 10/100, Loss: 0.9211165904998779\n",
      "Iteration 11/100, Loss: 0.8968157768249512\n",
      "Iteration 12/100, Loss: 0.8740246295928955\n",
      "Iteration 13/100, Loss: 0.8526202440261841\n",
      "Iteration 14/100, Loss: 0.8324999809265137\n",
      "Iteration 15/100, Loss: 0.8135722279548645\n",
      "Iteration 16/100, Loss: 0.7957466244697571\n",
      "Iteration 17/100, Loss: 0.7789410948753357\n",
      "Iteration 18/100, Loss: 0.7630821466445923\n",
      "Iteration 19/100, Loss: 0.7480990290641785\n",
      "Iteration 20/100, Loss: 0.7339291572570801\n",
      "Iteration 21/100, Loss: 0.7205183506011963\n",
      "Iteration 22/100, Loss: 0.7078133821487427\n",
      "Iteration 23/100, Loss: 0.6957652568817139\n",
      "Iteration 24/100, Loss: 0.6843276023864746\n",
      "Iteration 25/100, Loss: 0.6734603643417358\n",
      "Iteration 26/100, Loss: 0.6631235480308533\n",
      "Iteration 27/100, Loss: 0.6532840132713318\n",
      "Iteration 28/100, Loss: 0.6439084410667419\n",
      "Iteration 29/100, Loss: 0.6349648833274841\n",
      "Iteration 30/100, Loss: 0.6264275908470154\n",
      "Iteration 31/100, Loss: 0.6182713508605957\n",
      "Iteration 32/100, Loss: 0.6104730367660522\n",
      "Iteration 33/100, Loss: 0.6030113101005554\n",
      "Iteration 34/100, Loss: 0.5958656668663025\n",
      "Iteration 35/100, Loss: 0.5890174508094788\n",
      "Iteration 36/100, Loss: 0.5824487805366516\n",
      "Iteration 37/100, Loss: 0.5761428475379944\n",
      "Iteration 38/100, Loss: 0.5700848698616028\n",
      "Iteration 39/100, Loss: 0.5642610788345337\n",
      "Iteration 40/100, Loss: 0.5586593747138977\n",
      "Iteration 41/100, Loss: 0.5532675981521606\n",
      "Iteration 42/100, Loss: 0.5480736494064331\n",
      "Iteration 43/100, Loss: 0.5430673956871033\n",
      "Iteration 44/100, Loss: 0.5382382869720459\n",
      "Iteration 45/100, Loss: 0.5335772633552551\n",
      "Iteration 46/100, Loss: 0.5290759801864624\n",
      "Iteration 47/100, Loss: 0.5247266888618469\n",
      "Iteration 48/100, Loss: 0.5205217599868774\n",
      "Iteration 49/100, Loss: 0.5164538025856018\n",
      "Iteration 50/100, Loss: 0.5125162601470947\n",
      "Iteration 51/100, Loss: 0.5087034702301025\n",
      "Iteration 52/100, Loss: 0.5050095319747925\n",
      "Iteration 53/100, Loss: 0.5014289617538452\n",
      "Iteration 54/100, Loss: 0.497956782579422\n",
      "Iteration 55/100, Loss: 0.4945875406265259\n",
      "Iteration 56/100, Loss: 0.49131718277931213\n",
      "Iteration 57/100, Loss: 0.4881414771080017\n",
      "Iteration 58/100, Loss: 0.4850563108921051\n",
      "Iteration 59/100, Loss: 0.48205825686454773\n",
      "Iteration 60/100, Loss: 0.4791431725025177\n",
      "Iteration 61/100, Loss: 0.4763069748878479\n",
      "Iteration 62/100, Loss: 0.4735466539859772\n",
      "Iteration 63/100, Loss: 0.47085943818092346\n",
      "Iteration 64/100, Loss: 0.4682420492172241\n",
      "Iteration 65/100, Loss: 0.46569204330444336\n",
      "Iteration 66/100, Loss: 0.46320658922195435\n",
      "Iteration 67/100, Loss: 0.4607829451560974\n",
      "Iteration 68/100, Loss: 0.4584188759326935\n",
      "Iteration 69/100, Loss: 0.4561118483543396\n",
      "Iteration 70/100, Loss: 0.45385995507240295\n",
      "Iteration 71/100, Loss: 0.4516613185405731\n",
      "Iteration 72/100, Loss: 0.4495140612125397\n",
      "Iteration 73/100, Loss: 0.4474162757396698\n",
      "Iteration 74/100, Loss: 0.4453660547733307\n",
      "Iteration 75/100, Loss: 0.4433616101741791\n",
      "Iteration 76/100, Loss: 0.4414016008377075\n",
      "Iteration 77/100, Loss: 0.4394846260547638\n",
      "Iteration 78/100, Loss: 0.43760886788368225\n",
      "Iteration 79/100, Loss: 0.43577301502227783\n",
      "Iteration 80/100, Loss: 0.4339756369590759\n",
      "Iteration 81/100, Loss: 0.43221575021743774\n",
      "Iteration 82/100, Loss: 0.43049201369285583\n",
      "Iteration 83/100, Loss: 0.4288032352924347\n",
      "Iteration 84/100, Loss: 0.4271480441093445\n",
      "Iteration 85/100, Loss: 0.4255251884460449\n",
      "Iteration 86/100, Loss: 0.4239339232444763\n",
      "Iteration 87/100, Loss: 0.4223734438419342\n",
      "Iteration 88/100, Loss: 0.4208427965641022\n",
      "Iteration 89/100, Loss: 0.41934090852737427\n",
      "Iteration 90/100, Loss: 0.41786685585975647\n",
      "Iteration 91/100, Loss: 0.41641974449157715\n",
      "Iteration 92/100, Loss: 0.41499894857406616\n",
      "Iteration 93/100, Loss: 0.41360363364219666\n",
      "Iteration 94/100, Loss: 0.4122329652309418\n",
      "Iteration 95/100, Loss: 0.41088634729385376\n",
      "Iteration 96/100, Loss: 0.4095631241798401\n",
      "Iteration 97/100, Loss: 0.4082626700401306\n",
      "Iteration 98/100, Loss: 0.40698423981666565\n",
      "Iteration 99/100, Loss: 0.4057272672653198\n",
      "Iteration 100/100, Loss: 0.4044910669326782\n",
      "Pruning Step 8\n",
      "Iteration 1/100, Loss: 1.1921093463897705\n",
      "Iteration 2/100, Loss: 1.141144037246704\n",
      "Iteration 3/100, Loss: 1.1004666090011597\n",
      "Iteration 4/100, Loss: 1.0651931762695312\n",
      "Iteration 5/100, Loss: 1.033280611038208\n",
      "Iteration 6/100, Loss: 1.0038108825683594\n",
      "Iteration 7/100, Loss: 0.9763387441635132\n",
      "Iteration 8/100, Loss: 0.9506044387817383\n",
      "Iteration 9/100, Loss: 0.9264281988143921\n",
      "Iteration 10/100, Loss: 0.9036674499511719\n",
      "Iteration 11/100, Loss: 0.8822186589241028\n",
      "Iteration 12/100, Loss: 0.8619871139526367\n",
      "Iteration 13/100, Loss: 0.8428846597671509\n",
      "Iteration 14/100, Loss: 0.8248313069343567\n",
      "Iteration 15/100, Loss: 0.8077563643455505\n",
      "Iteration 16/100, Loss: 0.7915893793106079\n",
      "Iteration 17/100, Loss: 0.776269793510437\n",
      "Iteration 18/100, Loss: 0.7617397308349609\n",
      "Iteration 19/100, Loss: 0.7479462027549744\n",
      "Iteration 20/100, Loss: 0.7348429560661316\n",
      "Iteration 21/100, Loss: 0.7223858833312988\n",
      "Iteration 22/100, Loss: 0.7105318307876587\n",
      "Iteration 23/100, Loss: 0.6992424726486206\n",
      "Iteration 24/100, Loss: 0.6884833574295044\n",
      "Iteration 25/100, Loss: 0.6782204508781433\n",
      "Iteration 26/100, Loss: 0.6684229969978333\n",
      "Iteration 27/100, Loss: 0.6590643525123596\n",
      "Iteration 28/100, Loss: 0.650115966796875\n",
      "Iteration 29/100, Loss: 0.6415528059005737\n",
      "Iteration 30/100, Loss: 0.6333537101745605\n",
      "Iteration 31/100, Loss: 0.6254972815513611\n",
      "Iteration 32/100, Loss: 0.6179642081260681\n",
      "Iteration 33/100, Loss: 0.6107356548309326\n",
      "Iteration 34/100, Loss: 0.6037949323654175\n",
      "Iteration 35/100, Loss: 0.5971258282661438\n",
      "Iteration 36/100, Loss: 0.5907143354415894\n",
      "Iteration 37/100, Loss: 0.5845450758934021\n",
      "Iteration 38/100, Loss: 0.5786064267158508\n",
      "Iteration 39/100, Loss: 0.5728855729103088\n",
      "Iteration 40/100, Loss: 0.5673709511756897\n",
      "Iteration 41/100, Loss: 0.5620522499084473\n",
      "Iteration 42/100, Loss: 0.5569193959236145\n",
      "Iteration 43/100, Loss: 0.5519629120826721\n",
      "Iteration 44/100, Loss: 0.5471746325492859\n",
      "Iteration 45/100, Loss: 0.5425459742546082\n",
      "Iteration 46/100, Loss: 0.5380691885948181\n",
      "Iteration 47/100, Loss: 0.5337371230125427\n",
      "Iteration 48/100, Loss: 0.5295425653457642\n",
      "Iteration 49/100, Loss: 0.525479257106781\n",
      "Iteration 50/100, Loss: 0.5215411186218262\n",
      "Iteration 51/100, Loss: 0.5177229046821594\n",
      "Iteration 52/100, Loss: 0.5140189528465271\n",
      "Iteration 53/100, Loss: 0.5104244947433472\n",
      "Iteration 54/100, Loss: 0.5069348812103271\n",
      "Iteration 55/100, Loss: 0.5035455226898193\n",
      "Iteration 56/100, Loss: 0.5002515912055969\n",
      "Iteration 57/100, Loss: 0.4970494210720062\n",
      "Iteration 58/100, Loss: 0.4939349591732025\n",
      "Iteration 59/100, Loss: 0.49090445041656494\n",
      "Iteration 60/100, Loss: 0.48795491456985474\n",
      "Iteration 61/100, Loss: 0.48508310317993164\n",
      "Iteration 62/100, Loss: 0.4822860658168793\n",
      "Iteration 63/100, Loss: 0.4795607626438141\n",
      "Iteration 64/100, Loss: 0.4769045114517212\n",
      "Iteration 65/100, Loss: 0.4743143618106842\n",
      "Iteration 66/100, Loss: 0.4717879593372345\n",
      "Iteration 67/100, Loss: 0.4693228602409363\n",
      "Iteration 68/100, Loss: 0.4669167399406433\n",
      "Iteration 69/100, Loss: 0.4645674526691437\n",
      "Iteration 70/100, Loss: 0.46227312088012695\n",
      "Iteration 71/100, Loss: 0.46003156900405884\n",
      "Iteration 72/100, Loss: 0.4578413963317871\n",
      "Iteration 73/100, Loss: 0.45570066571235657\n",
      "Iteration 74/100, Loss: 0.4536076486110687\n",
      "Iteration 75/100, Loss: 0.451560378074646\n",
      "Iteration 76/100, Loss: 0.449557363986969\n",
      "Iteration 77/100, Loss: 0.4475972652435303\n",
      "Iteration 78/100, Loss: 0.4456787109375\n",
      "Iteration 79/100, Loss: 0.44380033016204834\n",
      "Iteration 80/100, Loss: 0.44196054339408875\n",
      "Iteration 81/100, Loss: 0.44015827775001526\n",
      "Iteration 82/100, Loss: 0.4383922517299652\n",
      "Iteration 83/100, Loss: 0.4366614818572998\n",
      "Iteration 84/100, Loss: 0.43496495485305786\n",
      "Iteration 85/100, Loss: 0.4333016574382782\n",
      "Iteration 86/100, Loss: 0.4316706359386444\n",
      "Iteration 87/100, Loss: 0.4300707280635834\n",
      "Iteration 88/100, Loss: 0.4285009205341339\n",
      "Iteration 89/100, Loss: 0.426960289478302\n",
      "Iteration 90/100, Loss: 0.4254477620124817\n",
      "Iteration 91/100, Loss: 0.42396265268325806\n",
      "Iteration 92/100, Loss: 0.4225042462348938\n",
      "Iteration 93/100, Loss: 0.4210715889930725\n",
      "Iteration 94/100, Loss: 0.4196641743183136\n",
      "Iteration 95/100, Loss: 0.41828110814094543\n",
      "Iteration 96/100, Loss: 0.41692185401916504\n",
      "Iteration 97/100, Loss: 0.4155857563018799\n",
      "Iteration 98/100, Loss: 0.41427212953567505\n",
      "Iteration 99/100, Loss: 0.41298046708106995\n",
      "Iteration 100/100, Loss: 0.41170990467071533\n",
      "Pruning Step 9\n",
      "Iteration 1/100, Loss: 1.1571900844573975\n",
      "Iteration 2/100, Loss: 1.1094489097595215\n",
      "Iteration 3/100, Loss: 1.072352409362793\n",
      "Iteration 4/100, Loss: 1.0404393672943115\n",
      "Iteration 5/100, Loss: 1.0116832256317139\n",
      "Iteration 6/100, Loss: 0.985221266746521\n",
      "Iteration 7/100, Loss: 0.9605722427368164\n",
      "Iteration 8/100, Loss: 0.9374656081199646\n",
      "Iteration 9/100, Loss: 0.9157122373580933\n",
      "Iteration 10/100, Loss: 0.8951834440231323\n",
      "Iteration 11/100, Loss: 0.8757727146148682\n",
      "Iteration 12/100, Loss: 0.8573951125144958\n",
      "Iteration 13/100, Loss: 0.8399780988693237\n",
      "Iteration 14/100, Loss: 0.8234509825706482\n",
      "Iteration 15/100, Loss: 0.8077517151832581\n",
      "Iteration 16/100, Loss: 0.7928273677825928\n",
      "Iteration 17/100, Loss: 0.7786242365837097\n",
      "Iteration 18/100, Loss: 0.7650966048240662\n",
      "Iteration 19/100, Loss: 0.7522027492523193\n",
      "Iteration 20/100, Loss: 0.7399051785469055\n",
      "Iteration 21/100, Loss: 0.7281679511070251\n",
      "Iteration 22/100, Loss: 0.716957688331604\n",
      "Iteration 23/100, Loss: 0.7062433958053589\n",
      "Iteration 24/100, Loss: 0.6959956288337708\n",
      "Iteration 25/100, Loss: 0.686187744140625\n",
      "Iteration 26/100, Loss: 0.6767950654029846\n",
      "Iteration 27/100, Loss: 0.6677935719490051\n",
      "Iteration 28/100, Loss: 0.6591610908508301\n",
      "Iteration 29/100, Loss: 0.650878369808197\n",
      "Iteration 30/100, Loss: 0.6429264545440674\n",
      "Iteration 31/100, Loss: 0.6352872252464294\n",
      "Iteration 32/100, Loss: 0.62794429063797\n",
      "Iteration 33/100, Loss: 0.6208821535110474\n",
      "Iteration 34/100, Loss: 0.6140855550765991\n",
      "Iteration 35/100, Loss: 0.6075411438941956\n",
      "Iteration 36/100, Loss: 0.6012362241744995\n",
      "Iteration 37/100, Loss: 0.5951579809188843\n",
      "Iteration 38/100, Loss: 0.589294970035553\n",
      "Iteration 39/100, Loss: 0.5836367011070251\n",
      "Iteration 40/100, Loss: 0.5781735181808472\n",
      "Iteration 41/100, Loss: 0.5728959441184998\n",
      "Iteration 42/100, Loss: 0.567794919013977\n",
      "Iteration 43/100, Loss: 0.5628626942634583\n",
      "Iteration 44/100, Loss: 0.55809086561203\n",
      "Iteration 45/100, Loss: 0.5534721612930298\n",
      "Iteration 46/100, Loss: 0.548999547958374\n",
      "Iteration 47/100, Loss: 0.544666588306427\n",
      "Iteration 48/100, Loss: 0.5404667258262634\n",
      "Iteration 49/100, Loss: 0.5363941788673401\n",
      "Iteration 50/100, Loss: 0.5324435830116272\n",
      "Iteration 51/100, Loss: 0.5286093354225159\n",
      "Iteration 52/100, Loss: 0.5248865485191345\n",
      "Iteration 53/100, Loss: 0.5212702751159668\n",
      "Iteration 54/100, Loss: 0.5177562236785889\n",
      "Iteration 55/100, Loss: 0.5143404006958008\n",
      "Iteration 56/100, Loss: 0.5110190510749817\n",
      "Iteration 57/100, Loss: 0.5077880620956421\n",
      "Iteration 58/100, Loss: 0.5046440362930298\n",
      "Iteration 59/100, Loss: 0.5015830993652344\n",
      "Iteration 60/100, Loss: 0.4986024498939514\n",
      "Iteration 61/100, Loss: 0.4956987500190735\n",
      "Iteration 62/100, Loss: 0.49286937713623047\n",
      "Iteration 63/100, Loss: 0.49011123180389404\n",
      "Iteration 64/100, Loss: 0.48742160201072693\n",
      "Iteration 65/100, Loss: 0.48479798436164856\n",
      "Iteration 66/100, Loss: 0.48223790526390076\n",
      "Iteration 67/100, Loss: 0.47973906993865967\n",
      "Iteration 68/100, Loss: 0.47729942202568054\n",
      "Iteration 69/100, Loss: 0.47491660714149475\n",
      "Iteration 70/100, Loss: 0.4725886583328247\n",
      "Iteration 71/100, Loss: 0.4703136086463928\n",
      "Iteration 72/100, Loss: 0.4680894911289215\n",
      "Iteration 73/100, Loss: 0.465914785861969\n",
      "Iteration 74/100, Loss: 0.4637877643108368\n",
      "Iteration 75/100, Loss: 0.4617069363594055\n",
      "Iteration 76/100, Loss: 0.45967069268226624\n",
      "Iteration 77/100, Loss: 0.45767736434936523\n",
      "Iteration 78/100, Loss: 0.45572584867477417\n",
      "Iteration 79/100, Loss: 0.4538145959377289\n",
      "Iteration 80/100, Loss: 0.4519423842430115\n",
      "Iteration 81/100, Loss: 0.4501079320907593\n",
      "Iteration 82/100, Loss: 0.4483100175857544\n",
      "Iteration 83/100, Loss: 0.4465474486351013\n",
      "Iteration 84/100, Loss: 0.44481921195983887\n",
      "Iteration 85/100, Loss: 0.44312429428100586\n",
      "Iteration 86/100, Loss: 0.44146138429641724\n",
      "Iteration 87/100, Loss: 0.43982982635498047\n",
      "Iteration 88/100, Loss: 0.4382287561893463\n",
      "Iteration 89/100, Loss: 0.4366571605205536\n",
      "Iteration 90/100, Loss: 0.43511420488357544\n",
      "Iteration 91/100, Loss: 0.433599054813385\n",
      "Iteration 92/100, Loss: 0.43211087584495544\n",
      "Iteration 93/100, Loss: 0.43064889311790466\n",
      "Iteration 94/100, Loss: 0.4292125701904297\n",
      "Iteration 95/100, Loss: 0.4278009831905365\n",
      "Iteration 96/100, Loss: 0.42641353607177734\n",
      "Iteration 97/100, Loss: 0.4250495433807373\n",
      "Iteration 98/100, Loss: 0.42370840907096863\n",
      "Iteration 99/100, Loss: 0.4223893880844116\n",
      "Iteration 100/100, Loss: 0.4210919737815857\n",
      "Pruning Step 10\n",
      "Iteration 1/100, Loss: 1.1267201900482178\n",
      "Iteration 2/100, Loss: 1.0809953212738037\n",
      "Iteration 3/100, Loss: 1.044442057609558\n",
      "Iteration 4/100, Loss: 1.0135316848754883\n",
      "Iteration 5/100, Loss: 0.986427903175354\n",
      "Iteration 6/100, Loss: 0.962037980556488\n",
      "Iteration 7/100, Loss: 0.9396659135818481\n",
      "Iteration 8/100, Loss: 0.9188661575317383\n",
      "Iteration 9/100, Loss: 0.8993595838546753\n",
      "Iteration 10/100, Loss: 0.880958616733551\n",
      "Iteration 11/100, Loss: 0.8635305762290955\n",
      "Iteration 12/100, Loss: 0.8469787240028381\n",
      "Iteration 13/100, Loss: 0.8312285542488098\n",
      "Iteration 14/100, Loss: 0.8162190318107605\n",
      "Iteration 15/100, Loss: 0.8019005656242371\n",
      "Iteration 16/100, Loss: 0.7882301807403564\n",
      "Iteration 17/100, Loss: 0.7751704454421997\n",
      "Iteration 18/100, Loss: 0.7626863121986389\n",
      "Iteration 19/100, Loss: 0.7507433891296387\n",
      "Iteration 20/100, Loss: 0.7393110394477844\n",
      "Iteration 21/100, Loss: 0.7283602952957153\n",
      "Iteration 22/100, Loss: 0.7178663015365601\n",
      "Iteration 23/100, Loss: 0.7078039646148682\n",
      "Iteration 24/100, Loss: 0.6981492042541504\n",
      "Iteration 25/100, Loss: 0.6888822913169861\n",
      "Iteration 26/100, Loss: 0.6799818873405457\n",
      "Iteration 27/100, Loss: 0.671428918838501\n",
      "Iteration 28/100, Loss: 0.6632053256034851\n",
      "Iteration 29/100, Loss: 0.65529465675354\n",
      "Iteration 30/100, Loss: 0.6476811170578003\n",
      "Iteration 31/100, Loss: 0.6403496265411377\n",
      "Iteration 32/100, Loss: 0.6332859396934509\n",
      "Iteration 33/100, Loss: 0.6264769434928894\n",
      "Iteration 34/100, Loss: 0.6199102997779846\n",
      "Iteration 35/100, Loss: 0.6135739088058472\n",
      "Iteration 36/100, Loss: 0.6074567437171936\n",
      "Iteration 37/100, Loss: 0.6015490889549255\n",
      "Iteration 38/100, Loss: 0.5958409905433655\n",
      "Iteration 39/100, Loss: 0.5903226733207703\n",
      "Iteration 40/100, Loss: 0.58498615026474\n",
      "Iteration 41/100, Loss: 0.5798230171203613\n",
      "Iteration 42/100, Loss: 0.5748254060745239\n",
      "Iteration 43/100, Loss: 0.5699856281280518\n",
      "Iteration 44/100, Loss: 0.565296471118927\n",
      "Iteration 45/100, Loss: 0.5607514381408691\n",
      "Iteration 46/100, Loss: 0.5563438534736633\n",
      "Iteration 47/100, Loss: 0.5520683526992798\n",
      "Iteration 48/100, Loss: 0.5479190349578857\n",
      "Iteration 49/100, Loss: 0.5438910126686096\n",
      "Iteration 50/100, Loss: 0.5399790406227112\n",
      "Iteration 51/100, Loss: 0.5361784100532532\n",
      "Iteration 52/100, Loss: 0.5324843525886536\n",
      "Iteration 53/100, Loss: 0.5288926959037781\n",
      "Iteration 54/100, Loss: 0.5253995060920715\n",
      "Iteration 55/100, Loss: 0.5220007300376892\n",
      "Iteration 56/100, Loss: 0.5186926126480103\n",
      "Iteration 57/100, Loss: 0.515471875667572\n",
      "Iteration 58/100, Loss: 0.5123348236083984\n",
      "Iteration 59/100, Loss: 0.5092784762382507\n",
      "Iteration 60/100, Loss: 0.5062996745109558\n",
      "Iteration 61/100, Loss: 0.5033954381942749\n",
      "Iteration 62/100, Loss: 0.5005631446838379\n",
      "Iteration 63/100, Loss: 0.4978000223636627\n",
      "Iteration 64/100, Loss: 0.49510371685028076\n",
      "Iteration 65/100, Loss: 0.4924716055393219\n",
      "Iteration 66/100, Loss: 0.48990190029144287\n",
      "Iteration 67/100, Loss: 0.4873924255371094\n",
      "Iteration 68/100, Loss: 0.48494046926498413\n",
      "Iteration 69/100, Loss: 0.4825443625450134\n",
      "Iteration 70/100, Loss: 0.4802023470401764\n",
      "Iteration 71/100, Loss: 0.47791245579719543\n",
      "Iteration 72/100, Loss: 0.47567299008369446\n",
      "Iteration 73/100, Loss: 0.47348248958587646\n",
      "Iteration 74/100, Loss: 0.4713391661643982\n",
      "Iteration 75/100, Loss: 0.46924135088920593\n",
      "Iteration 76/100, Loss: 0.46718743443489075\n",
      "Iteration 77/100, Loss: 0.4651760756969452\n",
      "Iteration 78/100, Loss: 0.46320611238479614\n",
      "Iteration 79/100, Loss: 0.4612763524055481\n",
      "Iteration 80/100, Loss: 0.45938536524772644\n",
      "Iteration 81/100, Loss: 0.4575318992137909\n",
      "Iteration 82/100, Loss: 0.45571500062942505\n",
      "Iteration 83/100, Loss: 0.4539334774017334\n",
      "Iteration 84/100, Loss: 0.4521861970424652\n",
      "Iteration 85/100, Loss: 0.4504721164703369\n",
      "Iteration 86/100, Loss: 0.4487903118133545\n",
      "Iteration 87/100, Loss: 0.4471398890018463\n",
      "Iteration 88/100, Loss: 0.4455200135707855\n",
      "Iteration 89/100, Loss: 0.4439297318458557\n",
      "Iteration 90/100, Loss: 0.4423680603504181\n",
      "Iteration 91/100, Loss: 0.4408343732357025\n",
      "Iteration 92/100, Loss: 0.43932777643203735\n",
      "Iteration 93/100, Loss: 0.4378475844860077\n",
      "Iteration 94/100, Loss: 0.43639302253723145\n",
      "Iteration 95/100, Loss: 0.43496355414390564\n",
      "Iteration 96/100, Loss: 0.4335584044456482\n",
      "Iteration 97/100, Loss: 0.43217694759368896\n",
      "Iteration 98/100, Loss: 0.4308183789253235\n",
      "Iteration 99/100, Loss: 0.42948222160339355\n",
      "Iteration 100/100, Loss: 0.4281678795814514\n",
      "Pruning Step 11\n",
      "Iteration 1/100, Loss: 1.1424907445907593\n",
      "Iteration 2/100, Loss: 1.096299648284912\n",
      "Iteration 3/100, Loss: 1.0637332201004028\n",
      "Iteration 4/100, Loss: 1.0367250442504883\n",
      "Iteration 5/100, Loss: 1.0126228332519531\n",
      "Iteration 6/100, Loss: 0.9903756380081177\n",
      "Iteration 7/100, Loss: 0.9694927334785461\n",
      "Iteration 8/100, Loss: 0.949719250202179\n",
      "Iteration 9/100, Loss: 0.9309062361717224\n",
      "Iteration 10/100, Loss: 0.9129572510719299\n",
      "Iteration 11/100, Loss: 0.8958023190498352\n",
      "Iteration 12/100, Loss: 0.879386842250824\n",
      "Iteration 13/100, Loss: 0.8636674284934998\n",
      "Iteration 14/100, Loss: 0.8486075401306152\n",
      "Iteration 15/100, Loss: 0.8341719508171082\n",
      "Iteration 16/100, Loss: 0.8203274011611938\n",
      "Iteration 17/100, Loss: 0.8070475459098816\n",
      "Iteration 18/100, Loss: 0.7943021059036255\n",
      "Iteration 19/100, Loss: 0.7820678353309631\n",
      "Iteration 20/100, Loss: 0.770319938659668\n",
      "Iteration 21/100, Loss: 0.7590320110321045\n",
      "Iteration 22/100, Loss: 0.7481834888458252\n",
      "Iteration 23/100, Loss: 0.7377526164054871\n",
      "Iteration 24/100, Loss: 0.7277188897132874\n",
      "Iteration 25/100, Loss: 0.7180648446083069\n",
      "Iteration 26/100, Loss: 0.7087726593017578\n",
      "Iteration 27/100, Loss: 0.6998248100280762\n",
      "Iteration 28/100, Loss: 0.6912049651145935\n",
      "Iteration 29/100, Loss: 0.6828978061676025\n",
      "Iteration 30/100, Loss: 0.6748893857002258\n",
      "Iteration 31/100, Loss: 0.6671653389930725\n",
      "Iteration 32/100, Loss: 0.6597126126289368\n",
      "Iteration 33/100, Loss: 0.652519166469574\n",
      "Iteration 34/100, Loss: 0.6455731987953186\n",
      "Iteration 35/100, Loss: 0.6388638019561768\n",
      "Iteration 36/100, Loss: 0.6323795318603516\n",
      "Iteration 37/100, Loss: 0.6261105537414551\n",
      "Iteration 38/100, Loss: 0.620047390460968\n",
      "Iteration 39/100, Loss: 0.614180862903595\n",
      "Iteration 40/100, Loss: 0.6085034012794495\n",
      "Iteration 41/100, Loss: 0.60300612449646\n",
      "Iteration 42/100, Loss: 0.5976806282997131\n",
      "Iteration 43/100, Loss: 0.5925200581550598\n",
      "Iteration 44/100, Loss: 0.5875175595283508\n",
      "Iteration 45/100, Loss: 0.582666277885437\n",
      "Iteration 46/100, Loss: 0.5779600143432617\n",
      "Iteration 47/100, Loss: 0.5733928680419922\n",
      "Iteration 48/100, Loss: 0.5689586997032166\n",
      "Iteration 49/100, Loss: 0.564652144908905\n",
      "Iteration 50/100, Loss: 0.560468316078186\n",
      "Iteration 51/100, Loss: 0.5564024448394775\n",
      "Iteration 52/100, Loss: 0.5524500012397766\n",
      "Iteration 53/100, Loss: 0.5486066341400146\n",
      "Iteration 54/100, Loss: 0.5448679327964783\n",
      "Iteration 55/100, Loss: 0.5412296056747437\n",
      "Iteration 56/100, Loss: 0.5376879572868347\n",
      "Iteration 57/100, Loss: 0.5342392325401306\n",
      "Iteration 58/100, Loss: 0.5308802723884583\n",
      "Iteration 59/100, Loss: 0.5276073813438416\n",
      "Iteration 60/100, Loss: 0.5244172811508179\n",
      "Iteration 61/100, Loss: 0.521307110786438\n",
      "Iteration 62/100, Loss: 0.5182740688323975\n",
      "Iteration 63/100, Loss: 0.5153154730796814\n",
      "Iteration 64/100, Loss: 0.5124285221099854\n",
      "Iteration 65/100, Loss: 0.5096103549003601\n",
      "Iteration 66/100, Loss: 0.5068586468696594\n",
      "Iteration 67/100, Loss: 0.5041711330413818\n",
      "Iteration 68/100, Loss: 0.5015458464622498\n",
      "Iteration 69/100, Loss: 0.4989806115627289\n",
      "Iteration 70/100, Loss: 0.496473103761673\n",
      "Iteration 71/100, Loss: 0.4940216839313507\n",
      "Iteration 72/100, Loss: 0.4916245639324188\n",
      "Iteration 73/100, Loss: 0.48927998542785645\n",
      "Iteration 74/100, Loss: 0.4869860112667084\n",
      "Iteration 75/100, Loss: 0.4847409427165985\n",
      "Iteration 76/100, Loss: 0.4825434386730194\n",
      "Iteration 77/100, Loss: 0.48039206862449646\n",
      "Iteration 78/100, Loss: 0.47828516364097595\n",
      "Iteration 79/100, Loss: 0.47622156143188477\n",
      "Iteration 80/100, Loss: 0.4741998612880707\n",
      "Iteration 81/100, Loss: 0.4722188711166382\n",
      "Iteration 82/100, Loss: 0.4702770411968231\n",
      "Iteration 83/100, Loss: 0.4683734178543091\n",
      "Iteration 84/100, Loss: 0.46650680899620056\n",
      "Iteration 85/100, Loss: 0.4646760821342468\n",
      "Iteration 86/100, Loss: 0.4628801941871643\n",
      "Iteration 87/100, Loss: 0.4611181616783142\n",
      "Iteration 88/100, Loss: 0.45938900113105774\n",
      "Iteration 89/100, Loss: 0.4576917886734009\n",
      "Iteration 90/100, Loss: 0.456025630235672\n",
      "Iteration 91/100, Loss: 0.45438945293426514\n",
      "Iteration 92/100, Loss: 0.4527827501296997\n",
      "Iteration 93/100, Loss: 0.45120441913604736\n",
      "Iteration 94/100, Loss: 0.4496538043022156\n",
      "Iteration 95/100, Loss: 0.4481300711631775\n",
      "Iteration 96/100, Loss: 0.44663259387016296\n",
      "Iteration 97/100, Loss: 0.4451606869697571\n",
      "Iteration 98/100, Loss: 0.44371339678764343\n",
      "Iteration 99/100, Loss: 0.44229042530059814\n",
      "Iteration 100/100, Loss: 0.44089093804359436\n",
      "Pruning Step 12\n",
      "Iteration 1/100, Loss: 1.1181830167770386\n",
      "Iteration 2/100, Loss: 1.0733698606491089\n",
      "Iteration 3/100, Loss: 1.0393848419189453\n",
      "Iteration 4/100, Loss: 1.0115488767623901\n",
      "Iteration 5/100, Loss: 0.9875810146331787\n",
      "Iteration 6/100, Loss: 0.9662248492240906\n",
      "Iteration 7/100, Loss: 0.9467377662658691\n",
      "Iteration 8/100, Loss: 0.9286471605300903\n",
      "Iteration 9/100, Loss: 0.9116518497467041\n",
      "Iteration 10/100, Loss: 0.8955545425415039\n",
      "Iteration 11/100, Loss: 0.8802273869514465\n",
      "Iteration 12/100, Loss: 0.865578830242157\n",
      "Iteration 13/100, Loss: 0.8515401482582092\n",
      "Iteration 14/100, Loss: 0.8380625247955322\n",
      "Iteration 15/100, Loss: 0.825110137462616\n",
      "Iteration 16/100, Loss: 0.8126493692398071\n",
      "Iteration 17/100, Loss: 0.8006542325019836\n",
      "Iteration 18/100, Loss: 0.7891035079956055\n",
      "Iteration 19/100, Loss: 0.7779768109321594\n",
      "Iteration 20/100, Loss: 0.767254114151001\n",
      "Iteration 21/100, Loss: 0.7569177150726318\n",
      "Iteration 22/100, Loss: 0.746950089931488\n",
      "Iteration 23/100, Loss: 0.7373347878456116\n",
      "Iteration 24/100, Loss: 0.7280551791191101\n",
      "Iteration 25/100, Loss: 0.7190986275672913\n",
      "Iteration 26/100, Loss: 0.7104519605636597\n",
      "Iteration 27/100, Loss: 0.7021017074584961\n",
      "Iteration 28/100, Loss: 0.6940341591835022\n",
      "Iteration 29/100, Loss: 0.6862377524375916\n",
      "Iteration 30/100, Loss: 0.6787008047103882\n",
      "Iteration 31/100, Loss: 0.6714122891426086\n",
      "Iteration 32/100, Loss: 0.6643626093864441\n",
      "Iteration 33/100, Loss: 0.6575412750244141\n",
      "Iteration 34/100, Loss: 0.6509391665458679\n",
      "Iteration 35/100, Loss: 0.6445468068122864\n",
      "Iteration 36/100, Loss: 0.6383553147315979\n",
      "Iteration 37/100, Loss: 0.6323560476303101\n",
      "Iteration 38/100, Loss: 0.6265414357185364\n",
      "Iteration 39/100, Loss: 0.6209038496017456\n",
      "Iteration 40/100, Loss: 0.6154364347457886\n",
      "Iteration 41/100, Loss: 0.6101323962211609\n",
      "Iteration 42/100, Loss: 0.6049855351448059\n",
      "Iteration 43/100, Loss: 0.5999891757965088\n",
      "Iteration 44/100, Loss: 0.5951371192932129\n",
      "Iteration 45/100, Loss: 0.590423583984375\n",
      "Iteration 46/100, Loss: 0.5858430862426758\n",
      "Iteration 47/100, Loss: 0.5813910365104675\n",
      "Iteration 48/100, Loss: 0.5770618915557861\n",
      "Iteration 49/100, Loss: 0.5728511810302734\n",
      "Iteration 50/100, Loss: 0.5687540173530579\n",
      "Iteration 51/100, Loss: 0.5647666454315186\n",
      "Iteration 52/100, Loss: 0.5608847737312317\n",
      "Iteration 53/100, Loss: 0.5571047067642212\n",
      "Iteration 54/100, Loss: 0.5534225702285767\n",
      "Iteration 55/100, Loss: 0.5498349070549011\n",
      "Iteration 56/100, Loss: 0.5463380217552185\n",
      "Iteration 57/100, Loss: 0.5429288148880005\n",
      "Iteration 58/100, Loss: 0.5396040081977844\n",
      "Iteration 59/100, Loss: 0.5363605618476868\n",
      "Iteration 60/100, Loss: 0.5331956148147583\n",
      "Iteration 61/100, Loss: 0.5301064848899841\n",
      "Iteration 62/100, Loss: 0.5270906686782837\n",
      "Iteration 63/100, Loss: 0.5241456627845764\n",
      "Iteration 64/100, Loss: 0.5212692618370056\n",
      "Iteration 65/100, Loss: 0.5184592008590698\n",
      "Iteration 66/100, Loss: 0.5157130360603333\n",
      "Iteration 67/100, Loss: 0.5130288004875183\n",
      "Iteration 68/100, Loss: 0.5104042291641235\n",
      "Iteration 69/100, Loss: 0.5078374147415161\n",
      "Iteration 70/100, Loss: 0.5053263902664185\n",
      "Iteration 71/100, Loss: 0.5028693079948425\n",
      "Iteration 72/100, Loss: 0.500464677810669\n",
      "Iteration 73/100, Loss: 0.4981107711791992\n",
      "Iteration 74/100, Loss: 0.49580612778663635\n",
      "Iteration 75/100, Loss: 0.4935492277145386\n",
      "Iteration 76/100, Loss: 0.4913384020328522\n",
      "Iteration 77/100, Loss: 0.4891723692417145\n",
      "Iteration 78/100, Loss: 0.4870496690273285\n",
      "Iteration 79/100, Loss: 0.4849690794944763\n",
      "Iteration 80/100, Loss: 0.4829294979572296\n",
      "Iteration 81/100, Loss: 0.480929434299469\n",
      "Iteration 82/100, Loss: 0.4789677858352661\n",
      "Iteration 83/100, Loss: 0.4770435094833374\n",
      "Iteration 84/100, Loss: 0.47515565156936646\n",
      "Iteration 85/100, Loss: 0.47330302000045776\n",
      "Iteration 86/100, Loss: 0.47148483991622925\n",
      "Iteration 87/100, Loss: 0.469699889421463\n",
      "Iteration 88/100, Loss: 0.46794745326042175\n",
      "Iteration 89/100, Loss: 0.4662265181541443\n",
      "Iteration 90/100, Loss: 0.4645363390445709\n",
      "Iteration 91/100, Loss: 0.4628757834434509\n",
      "Iteration 92/100, Loss: 0.4612441956996918\n",
      "Iteration 93/100, Loss: 0.4596407413482666\n",
      "Iteration 94/100, Loss: 0.45806464552879333\n",
      "Iteration 95/100, Loss: 0.4565151631832123\n",
      "Iteration 96/100, Loss: 0.45499181747436523\n",
      "Iteration 97/100, Loss: 0.4534938931465149\n",
      "Iteration 98/100, Loss: 0.4520207345485687\n",
      "Iteration 99/100, Loss: 0.4505716860294342\n",
      "Iteration 100/100, Loss: 0.44914621114730835\n",
      "Pruning Step 13\n",
      "Iteration 1/100, Loss: 1.1428359746932983\n",
      "Iteration 2/100, Loss: 1.096950888633728\n",
      "Iteration 3/100, Loss: 1.065089225769043\n",
      "Iteration 4/100, Loss: 1.0397007465362549\n",
      "Iteration 5/100, Loss: 1.0178135633468628\n",
      "Iteration 6/100, Loss: 0.9980257749557495\n",
      "Iteration 7/100, Loss: 0.9796282649040222\n",
      "Iteration 8/100, Loss: 0.9622387886047363\n",
      "Iteration 9/100, Loss: 0.9456434845924377\n",
      "Iteration 10/100, Loss: 0.9297208786010742\n",
      "Iteration 11/100, Loss: 0.9143990874290466\n",
      "Iteration 12/100, Loss: 0.8996305465698242\n",
      "Iteration 13/100, Loss: 0.8853817582130432\n",
      "Iteration 14/100, Loss: 0.8716278672218323\n",
      "Iteration 15/100, Loss: 0.8583471179008484\n",
      "Iteration 16/100, Loss: 0.8455217480659485\n",
      "Iteration 17/100, Loss: 0.8331339955329895\n",
      "Iteration 18/100, Loss: 0.8211662173271179\n",
      "Iteration 19/100, Loss: 0.8096031546592712\n",
      "Iteration 20/100, Loss: 0.7984306216239929\n",
      "Iteration 21/100, Loss: 0.7876331806182861\n",
      "Iteration 22/100, Loss: 0.7771962285041809\n",
      "Iteration 23/100, Loss: 0.7671073079109192\n",
      "Iteration 24/100, Loss: 0.7573538422584534\n",
      "Iteration 25/100, Loss: 0.7479222416877747\n",
      "Iteration 26/100, Loss: 0.7387999892234802\n",
      "Iteration 27/100, Loss: 0.7299759984016418\n",
      "Iteration 28/100, Loss: 0.7214385271072388\n",
      "Iteration 29/100, Loss: 0.7131760120391846\n",
      "Iteration 30/100, Loss: 0.7051783800125122\n",
      "Iteration 31/100, Loss: 0.6974344253540039\n",
      "Iteration 32/100, Loss: 0.6899352669715881\n",
      "Iteration 33/100, Loss: 0.6826712489128113\n",
      "Iteration 34/100, Loss: 0.6756331920623779\n",
      "Iteration 35/100, Loss: 0.6688122153282166\n",
      "Iteration 36/100, Loss: 0.6621997356414795\n",
      "Iteration 37/100, Loss: 0.6557877063751221\n",
      "Iteration 38/100, Loss: 0.6495686173439026\n",
      "Iteration 39/100, Loss: 0.6435343623161316\n",
      "Iteration 40/100, Loss: 0.6376782655715942\n",
      "Iteration 41/100, Loss: 0.6319934725761414\n",
      "Iteration 42/100, Loss: 0.6264731884002686\n",
      "Iteration 43/100, Loss: 0.621111273765564\n",
      "Iteration 44/100, Loss: 0.6159017086029053\n",
      "Iteration 45/100, Loss: 0.6108386516571045\n",
      "Iteration 46/100, Loss: 0.6059169173240662\n",
      "Iteration 47/100, Loss: 0.6011314392089844\n",
      "Iteration 48/100, Loss: 0.5964770913124084\n",
      "Iteration 49/100, Loss: 0.5919490456581116\n",
      "Iteration 50/100, Loss: 0.587542712688446\n",
      "Iteration 51/100, Loss: 0.583253800868988\n",
      "Iteration 52/100, Loss: 0.5790781378746033\n",
      "Iteration 53/100, Loss: 0.5750114321708679\n",
      "Iteration 54/100, Loss: 0.5710497498512268\n",
      "Iteration 55/100, Loss: 0.5671893954277039\n",
      "Iteration 56/100, Loss: 0.5634268522262573\n",
      "Iteration 57/100, Loss: 0.559758722782135\n",
      "Iteration 58/100, Loss: 0.5561816692352295\n",
      "Iteration 59/100, Loss: 0.552692174911499\n",
      "Iteration 60/100, Loss: 0.549287736415863\n",
      "Iteration 61/100, Loss: 0.5459651947021484\n",
      "Iteration 62/100, Loss: 0.5427217483520508\n",
      "Iteration 63/100, Loss: 0.5395548343658447\n",
      "Iteration 64/100, Loss: 0.5364619493484497\n",
      "Iteration 65/100, Loss: 0.5334407687187195\n",
      "Iteration 66/100, Loss: 0.530488908290863\n",
      "Iteration 67/100, Loss: 0.5276041626930237\n",
      "Iteration 68/100, Loss: 0.5247842669487\n",
      "Iteration 69/100, Loss: 0.5220270156860352\n",
      "Iteration 70/100, Loss: 0.5193307399749756\n",
      "Iteration 71/100, Loss: 0.5166932940483093\n",
      "Iteration 72/100, Loss: 0.5141128301620483\n",
      "Iteration 73/100, Loss: 0.511587381362915\n",
      "Iteration 74/100, Loss: 0.5091155171394348\n",
      "Iteration 75/100, Loss: 0.5066953301429749\n",
      "Iteration 76/100, Loss: 0.5043255090713501\n",
      "Iteration 77/100, Loss: 0.5020046234130859\n",
      "Iteration 78/100, Loss: 0.49973100423812866\n",
      "Iteration 79/100, Loss: 0.4975033104419708\n",
      "Iteration 80/100, Loss: 0.4953201115131378\n",
      "Iteration 81/100, Loss: 0.49318015575408936\n",
      "Iteration 82/100, Loss: 0.4910820424556732\n",
      "Iteration 83/100, Loss: 0.4890246093273163\n",
      "Iteration 84/100, Loss: 0.4870067834854126\n",
      "Iteration 85/100, Loss: 0.4850272834300995\n",
      "Iteration 86/100, Loss: 0.48308515548706055\n",
      "Iteration 87/100, Loss: 0.4811793565750122\n",
      "Iteration 88/100, Loss: 0.47930893301963806\n",
      "Iteration 89/100, Loss: 0.4774729311466217\n",
      "Iteration 90/100, Loss: 0.4756702780723572\n",
      "Iteration 91/100, Loss: 0.4738999903202057\n",
      "Iteration 92/100, Loss: 0.47216135263442993\n",
      "Iteration 93/100, Loss: 0.47045350074768066\n",
      "Iteration 94/100, Loss: 0.4687756597995758\n",
      "Iteration 95/100, Loss: 0.4671270549297333\n",
      "Iteration 96/100, Loss: 0.46550682187080383\n",
      "Iteration 97/100, Loss: 0.4639141857624054\n",
      "Iteration 98/100, Loss: 0.46234849095344543\n",
      "Iteration 99/100, Loss: 0.4608089327812195\n",
      "Iteration 100/100, Loss: 0.459294855594635\n",
      "Pruning Step 14\n",
      "Iteration 1/100, Loss: 1.1668020486831665\n",
      "Iteration 2/100, Loss: 1.125588297843933\n",
      "Iteration 3/100, Loss: 1.092302918434143\n",
      "Iteration 4/100, Loss: 1.0641486644744873\n",
      "Iteration 5/100, Loss: 1.0395569801330566\n",
      "Iteration 6/100, Loss: 1.0175565481185913\n",
      "Iteration 7/100, Loss: 0.9975142478942871\n",
      "Iteration 8/100, Loss: 0.9789926409721375\n",
      "Iteration 9/100, Loss: 0.961690366268158\n",
      "Iteration 10/100, Loss: 0.9453856945037842\n",
      "Iteration 11/100, Loss: 0.9299208521842957\n",
      "Iteration 12/100, Loss: 0.9151738882064819\n",
      "Iteration 13/100, Loss: 0.9010586142539978\n",
      "Iteration 14/100, Loss: 0.8875043392181396\n",
      "Iteration 15/100, Loss: 0.8744614124298096\n",
      "Iteration 16/100, Loss: 0.8618876338005066\n",
      "Iteration 17/100, Loss: 0.8497510552406311\n",
      "Iteration 18/100, Loss: 0.8380246162414551\n",
      "Iteration 19/100, Loss: 0.8266879320144653\n",
      "Iteration 20/100, Loss: 0.8157220482826233\n",
      "Iteration 21/100, Loss: 0.8051090240478516\n",
      "Iteration 22/100, Loss: 0.7948349714279175\n",
      "Iteration 23/100, Loss: 0.784885585308075\n",
      "Iteration 24/100, Loss: 0.7752481698989868\n",
      "Iteration 25/100, Loss: 0.7659115791320801\n",
      "Iteration 26/100, Loss: 0.7568648457527161\n",
      "Iteration 27/100, Loss: 0.7480973601341248\n",
      "Iteration 28/100, Loss: 0.7395983338356018\n",
      "Iteration 29/100, Loss: 0.7313582897186279\n",
      "Iteration 30/100, Loss: 0.72336745262146\n",
      "Iteration 31/100, Loss: 0.7156170606613159\n",
      "Iteration 32/100, Loss: 0.7080985903739929\n",
      "Iteration 33/100, Loss: 0.7008037567138672\n",
      "Iteration 34/100, Loss: 0.6937244534492493\n",
      "Iteration 35/100, Loss: 0.6868526339530945\n",
      "Iteration 36/100, Loss: 0.6801807284355164\n",
      "Iteration 37/100, Loss: 0.6737015247344971\n",
      "Iteration 38/100, Loss: 0.6674073934555054\n",
      "Iteration 39/100, Loss: 0.6612920761108398\n",
      "Iteration 40/100, Loss: 0.6553494334220886\n",
      "Iteration 41/100, Loss: 0.6495733261108398\n",
      "Iteration 42/100, Loss: 0.6439576148986816\n",
      "Iteration 43/100, Loss: 0.6384966969490051\n",
      "Iteration 44/100, Loss: 0.6331846117973328\n",
      "Iteration 45/100, Loss: 0.6280159950256348\n",
      "Iteration 46/100, Loss: 0.6229866147041321\n",
      "Iteration 47/100, Loss: 0.6180912256240845\n",
      "Iteration 48/100, Loss: 0.613324761390686\n",
      "Iteration 49/100, Loss: 0.6086828708648682\n",
      "Iteration 50/100, Loss: 0.6041614413261414\n",
      "Iteration 51/100, Loss: 0.5997565388679504\n",
      "Iteration 52/100, Loss: 0.5954635143280029\n",
      "Iteration 53/100, Loss: 0.5912790298461914\n",
      "Iteration 54/100, Loss: 0.5871990919113159\n",
      "Iteration 55/100, Loss: 0.5832202434539795\n",
      "Iteration 56/100, Loss: 0.5793389081954956\n",
      "Iteration 57/100, Loss: 0.5755515694618225\n",
      "Iteration 58/100, Loss: 0.5718554854393005\n",
      "Iteration 59/100, Loss: 0.5682472586631775\n",
      "Iteration 60/100, Loss: 0.5647246837615967\n",
      "Iteration 61/100, Loss: 0.5612843632698059\n",
      "Iteration 62/100, Loss: 0.5579239726066589\n",
      "Iteration 63/100, Loss: 0.5546407699584961\n",
      "Iteration 64/100, Loss: 0.551432728767395\n",
      "Iteration 65/100, Loss: 0.5482972860336304\n",
      "Iteration 66/100, Loss: 0.5452319383621216\n",
      "Iteration 67/100, Loss: 0.5422345995903015\n",
      "Iteration 68/100, Loss: 0.5393028855323792\n",
      "Iteration 69/100, Loss: 0.5364348292350769\n",
      "Iteration 70/100, Loss: 0.5336286425590515\n",
      "Iteration 71/100, Loss: 0.5308824181556702\n",
      "Iteration 72/100, Loss: 0.5281942486763\n",
      "Iteration 73/100, Loss: 0.5255624055862427\n",
      "Iteration 74/100, Loss: 0.5229851007461548\n",
      "Iteration 75/100, Loss: 0.5204606652259827\n",
      "Iteration 76/100, Loss: 0.5179874897003174\n",
      "Iteration 77/100, Loss: 0.5155644416809082\n",
      "Iteration 78/100, Loss: 0.5131898522377014\n",
      "Iteration 79/100, Loss: 0.510862410068512\n",
      "Iteration 80/100, Loss: 0.5085808038711548\n",
      "Iteration 81/100, Loss: 0.5063436627388\n",
      "Iteration 82/100, Loss: 0.5041497349739075\n",
      "Iteration 83/100, Loss: 0.5019977688789368\n",
      "Iteration 84/100, Loss: 0.4998866319656372\n",
      "Iteration 85/100, Loss: 0.4978151023387909\n",
      "Iteration 86/100, Loss: 0.4957822263240814\n",
      "Iteration 87/100, Loss: 0.4937867820262909\n",
      "Iteration 88/100, Loss: 0.49182799458503723\n",
      "Iteration 89/100, Loss: 0.48990488052368164\n",
      "Iteration 90/100, Loss: 0.48801639676094055\n",
      "Iteration 91/100, Loss: 0.48616158962249756\n",
      "Iteration 92/100, Loss: 0.4843393862247467\n",
      "Iteration 93/100, Loss: 0.4825492203235626\n",
      "Iteration 94/100, Loss: 0.48079022765159607\n",
      "Iteration 95/100, Loss: 0.47906145453453064\n",
      "Iteration 96/100, Loss: 0.47736218571662903\n",
      "Iteration 97/100, Loss: 0.4756917953491211\n",
      "Iteration 98/100, Loss: 0.47404950857162476\n",
      "Iteration 99/100, Loss: 0.4724345803260803\n",
      "Iteration 100/100, Loss: 0.4708462357521057\n",
      "Pruning Step 15\n",
      "Iteration 1/100, Loss: 1.1664999723434448\n",
      "Iteration 2/100, Loss: 1.1347769498825073\n",
      "Iteration 3/100, Loss: 1.1083050966262817\n",
      "Iteration 4/100, Loss: 1.0852543115615845\n",
      "Iteration 5/100, Loss: 1.064551830291748\n",
      "Iteration 6/100, Loss: 1.0455251932144165\n",
      "Iteration 7/100, Loss: 1.0277386903762817\n",
      "Iteration 8/100, Loss: 1.010910153388977\n",
      "Iteration 9/100, Loss: 0.9948500394821167\n",
      "Iteration 10/100, Loss: 0.9794308543205261\n",
      "Iteration 11/100, Loss: 0.9645676016807556\n",
      "Iteration 12/100, Loss: 0.9502041935920715\n",
      "Iteration 13/100, Loss: 0.9362953305244446\n",
      "Iteration 14/100, Loss: 0.92281174659729\n",
      "Iteration 15/100, Loss: 0.9097314476966858\n",
      "Iteration 16/100, Loss: 0.8970378637313843\n",
      "Iteration 17/100, Loss: 0.8847163319587708\n",
      "Iteration 18/100, Loss: 0.8727541565895081\n",
      "Iteration 19/100, Loss: 0.8611404299736023\n",
      "Iteration 20/100, Loss: 0.8498646020889282\n",
      "Iteration 21/100, Loss: 0.8389167189598083\n",
      "Iteration 22/100, Loss: 0.8282870054244995\n",
      "Iteration 23/100, Loss: 0.8179658055305481\n",
      "Iteration 24/100, Loss: 0.8079433441162109\n",
      "Iteration 25/100, Loss: 0.7982106804847717\n",
      "Iteration 26/100, Loss: 0.7887595295906067\n",
      "Iteration 27/100, Loss: 0.77958083152771\n",
      "Iteration 28/100, Loss: 0.7706661820411682\n",
      "Iteration 29/100, Loss: 0.7620072960853577\n",
      "Iteration 30/100, Loss: 0.7535969018936157\n",
      "Iteration 31/100, Loss: 0.745427131652832\n",
      "Iteration 32/100, Loss: 0.7374901175498962\n",
      "Iteration 33/100, Loss: 0.7297786474227905\n",
      "Iteration 34/100, Loss: 0.7222852110862732\n",
      "Iteration 35/100, Loss: 0.7150025963783264\n",
      "Iteration 36/100, Loss: 0.707923948764801\n",
      "Iteration 37/100, Loss: 0.7010424733161926\n",
      "Iteration 38/100, Loss: 0.6943516731262207\n",
      "Iteration 39/100, Loss: 0.6878452301025391\n",
      "Iteration 40/100, Loss: 0.6815173625946045\n",
      "Iteration 41/100, Loss: 0.6753621101379395\n",
      "Iteration 42/100, Loss: 0.6693736910820007\n",
      "Iteration 43/100, Loss: 0.6635465025901794\n",
      "Iteration 44/100, Loss: 0.6578751802444458\n",
      "Iteration 45/100, Loss: 0.6523547768592834\n",
      "Iteration 46/100, Loss: 0.6469801068305969\n",
      "Iteration 47/100, Loss: 0.6417459845542908\n",
      "Iteration 48/100, Loss: 0.6366481781005859\n",
      "Iteration 49/100, Loss: 0.6316819190979004\n",
      "Iteration 50/100, Loss: 0.6268429756164551\n",
      "Iteration 51/100, Loss: 0.6221268177032471\n",
      "Iteration 52/100, Loss: 0.6175294518470764\n",
      "Iteration 53/100, Loss: 0.6130474209785461\n",
      "Iteration 54/100, Loss: 0.608676552772522\n",
      "Iteration 55/100, Loss: 0.6044133901596069\n",
      "Iteration 56/100, Loss: 0.600254476070404\n",
      "Iteration 57/100, Loss: 0.5961965918540955\n",
      "Iteration 58/100, Loss: 0.5922362804412842\n",
      "Iteration 59/100, Loss: 0.5883705019950867\n",
      "Iteration 60/100, Loss: 0.5845962166786194\n",
      "Iteration 61/100, Loss: 0.5809106230735779\n",
      "Iteration 62/100, Loss: 0.5773106813430786\n",
      "Iteration 63/100, Loss: 0.5737937688827515\n",
      "Iteration 64/100, Loss: 0.5703572630882263\n",
      "Iteration 65/100, Loss: 0.5669988989830017\n",
      "Iteration 66/100, Loss: 0.5637161731719971\n",
      "Iteration 67/100, Loss: 0.5605067610740662\n",
      "Iteration 68/100, Loss: 0.5573683977127075\n",
      "Iteration 69/100, Loss: 0.5542989373207092\n",
      "Iteration 70/100, Loss: 0.5512962937355042\n",
      "Iteration 71/100, Loss: 0.5483583211898804\n",
      "Iteration 72/100, Loss: 0.5454832315444946\n",
      "Iteration 73/100, Loss: 0.5426691770553589\n",
      "Iteration 74/100, Loss: 0.5399141311645508\n",
      "Iteration 75/100, Loss: 0.5372166037559509\n",
      "Iteration 76/100, Loss: 0.5345748662948608\n",
      "Iteration 77/100, Loss: 0.5319871306419373\n",
      "Iteration 78/100, Loss: 0.5294519066810608\n",
      "Iteration 79/100, Loss: 0.5269675850868225\n",
      "Iteration 80/100, Loss: 0.5245327949523926\n",
      "Iteration 81/100, Loss: 0.5221462845802307\n",
      "Iteration 82/100, Loss: 0.5198065638542175\n",
      "Iteration 83/100, Loss: 0.517512321472168\n",
      "Iteration 84/100, Loss: 0.5152623057365417\n",
      "Iteration 85/100, Loss: 0.5130552649497986\n",
      "Iteration 86/100, Loss: 0.5108901262283325\n",
      "Iteration 87/100, Loss: 0.5087655186653137\n",
      "Iteration 88/100, Loss: 0.5066806077957153\n",
      "Iteration 89/100, Loss: 0.5046342015266418\n",
      "Iteration 90/100, Loss: 0.502625048160553\n",
      "Iteration 91/100, Loss: 0.500652551651001\n",
      "Iteration 92/100, Loss: 0.49871543049812317\n",
      "Iteration 93/100, Loss: 0.4968128204345703\n",
      "Iteration 94/100, Loss: 0.49494388699531555\n",
      "Iteration 95/100, Loss: 0.49310770630836487\n",
      "Iteration 96/100, Loss: 0.4913034439086914\n",
      "Iteration 97/100, Loss: 0.4895302653312683\n",
      "Iteration 98/100, Loss: 0.4877875745296478\n",
      "Iteration 99/100, Loss: 0.4860743582248688\n",
      "Iteration 100/100, Loss: 0.48438990116119385\n"
     ]
    }
   ],
   "source": [
    "summary: ExperimentSummary = run_experiments(num_models=C.TEST_NUM_MODELS,\n",
    "                                             training_iterations=100,\n",
    "                                             pruning_percents=C.TEST_PRUNING_PERCENTS,\n",
    "                                             pruning_steps=C.TEST_PRUNING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.harness.experiment.ExperimentSummary object at 0x179810850>\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
