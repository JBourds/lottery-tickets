{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "main.ipynb\n",
    "\n",
    "Main file for recreating lottery ticket experiments done in randomly initialized dense neural networks.\n",
    "\n",
    "Authors: Jordan Bourdeau, Casey Forey\n",
    "Date Created: 3/8/24\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imports.\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'models/' created successfully.\n",
      "Directory 'checkpoints/' created successfully.\n",
      "Directory 'logs/fit/' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Constants for where to output model data\n",
    "MODEL_DIRECTORY: str = 'models/'\n",
    "CHECKPOINT_DIRECTORY: str = 'checkpoints/'\n",
    "FIT_DIRECTORY: str = 'logs/fit/'\n",
    "\n",
    "def create_path(path: str):\n",
    "    \"\"\"\n",
    "    Helper function to create a path and all its subdirectories.\n",
    "    :param path: String containing the target path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory '{path}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{path}' already exists.\")\n",
    "\n",
    "# Create directories if they aren't already created\n",
    "for directory in [MODEL_DIRECTORY, CHECKPOINT_DIRECTORY, FIT_DIRECTORY]:\n",
    "    create_path(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(28, 28) image shape\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(28, 28, 1) image shape\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and process the data.\n",
    "Code Source: https://colab.research.google.com/github/maticvl/dataHacker/blob/master/CNN/LeNet_5_TensorFlow_2_0_datahacker.ipynb#scrollTo=UA2ehjxgF7bY\n",
    "\"\"\"\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = datasets.mnist.load_data()\n",
    "\n",
    "# Verify output looks right\n",
    "print('x_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(X_train[0].shape, 'image shape')\n",
    "\n",
    "# Add a new axis\n",
    "X_train = X_train[:, :, :, np.newaxis]\n",
    "X_test = X_test[:, :, :, np.newaxis]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(X_train[0].shape, 'image shape')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "\n",
    "num_classes = 10\n",
    "Y_train = to_categorical(Y_train, num_classes)\n",
    "Y_test = to_categorical(Y_test, num_classes)\n",
    "\n",
    "# Data normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class definition for the network architecture.\n",
    "Code Source: https://colab.research.google.com/github/maticvl/dataHacker/blob/master/CNN/LeNet_5_TensorFlow_2_0_datahacker.ipynb#scrollTo=UA2ehjxgF7bY\n",
    "\"\"\"\n",
    "\n",
    "# LeNet-5 model derived from this paper: http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf\n",
    "class LeNet(Sequential):\n",
    "    def __init__(self, input_shape: np.array, num_classes: int, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers  \n",
    "        self.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=input_shape, padding=\"same\"))\n",
    "        self.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "        self.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "        self.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "        self.add(Flatten())\n",
    "\n",
    "        # Fully connected output layers\n",
    "        self.add(Dense(120, activation='tanh'))\n",
    "        self.add(Dense(84, activation='tanh'))\n",
    "        self.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer='adam', loss=categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for creating, training + testing, saving, and loading models.\n",
    "\"\"\" \n",
    "\n",
    "def get_model_directory(model_index: int, base_directory: str = \"\",) -> str:\n",
    "    \"\"\"\n",
    "    Function to return the relative directory where a model would go.\n",
    "\n",
    "    :param base_directory: Base directory to append model subdirectory to. Defaults to empty string.\n",
    "    :param model_index: Integer for the index/random seed of the model.\n",
    "\n",
    "    :returns: Returns expected directory for the model.\n",
    "    \"\"\"\n",
    "    return f'{base_directory}model_{model_index}/'\n",
    "\n",
    "def get_model_name(model_index: int, pruning_step: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Function to return the expected name for a model based on its index and pruning step.\n",
    "\n",
    "    :param model_index:   Integer for the index/random seed of the model.\n",
    "    :param pruning_step:  Integer for the pruning iteration.\n",
    "\n",
    "    :returns: Returns expected name for the model.\n",
    "    \"\"\"\n",
    "    return f'model_{model_index}_step_{pruning_step}.keras'\n",
    "\n",
    "def create_model(feature_shape: tuple[int, ...], num_classes: int, random_seed: int) -> tuple[LeNet, list[tf.keras.callbacks]]:\n",
    "    \"\"\"\n",
    "    Method used for setting the random seed(s) and instantiating a model.\n",
    "\n",
    "    :param feature_shape:  Shape of the features.\n",
    "    :param num_classes:    Number of potential classes. 10 for MNIST.\n",
    "    :param random_seed:    Value used to ensure reproducability.\n",
    "\n",
    "    :returns: Model and callbacks.\n",
    "    \"\"\"\n",
    "    # Set seeds\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    tf.random.set_seed(random_seed)\n",
    "\n",
    "    # Initialize the model\n",
    "    model: LeNet = LeNet(feature_shape, num_classes)\n",
    "\n",
    "    # Crate the callbacks\n",
    "    model_name: str = get_model_name(random_seed, 0)\n",
    "    tensorboard_path: str = get_model_directory(random_seed, FIT_DIRECTORY)\n",
    "    checkpoint_path: str = get_model_directory(random_seed, CHECKPOINT_DIRECTORY)\n",
    "\n",
    "    # Create the model checkpoint callback\n",
    "    callbacks: list[tf.keras.callbacks] = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=tensorboard_path, histogram_freq=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True)\n",
    "    ]\n",
    "\n",
    "    return model, callbacks\n",
    "\n",
    "def create_models(X_train: np.array, Y_train: np.array, X_test: np.array, Y_test: np.array, epochs: int, num_models: int):\n",
    "    \"\"\"\n",
    "    Function responsible for training/saving the base, fully parametrized models.\n",
    "\n",
    "    :param X_train:     Training instances.\n",
    "    :param X_test:      Testing instances.\n",
    "    :param Y_train:     Training labels.\n",
    "    :param Y_test:      Testing labels.\n",
    "    :param epochs:      Number of epochs to train the model for.\n",
    "    :param num_models:  Number of models to create.\n",
    "    \"\"\"\n",
    "    assert X_train.shape[0] >= 1, 'Need at least one input to determine feature shape'\n",
    "\n",
    "    # Extract shape of features and the number of classes\n",
    "    feature_shape: tuple[int, ...] = X_train[0].shape\n",
    "    num_classes: int = 10\n",
    "\n",
    "    # Use index as the random seed input\n",
    "    for i in range(num_models):\n",
    "        # Create the model if it does not already exist\n",
    "        if not os.path.exists(get_model_directory(i, MODEL_DIRECTORY) + get_model_name(i, 0)):\n",
    "            # Setup and train the model\n",
    "            model, callbacks = create_model(feature_shape, num_classes, random_seed=i)\n",
    "            model.fit(X_train, Y_train, epochs=epochs, validation_data=(X_test, Y_test), callbacks=callbacks, verbose=1, use_multiprocessing=True) \n",
    "            # Create the model's output directory and save it\n",
    "            output_directory: str = get_model_directory(i, MODEL_DIRECTORY)\n",
    "            create_path(output_directory)\n",
    "            model.save(output_directory + get_model_name(i, 0))\n",
    "\n",
    "def load_model(feature_shape: tuple[int, ...], num_classes: int, model_index: int, pruning_step: int) -> LeNet:\n",
    "    \"\"\"\n",
    "    Function used to load a single trained model.\n",
    "\n",
    "    :param feature_shape:     Tuple of integer dimensions for the feature shape.\n",
    "    :param num_classes:       Number of unique classes for the model.\n",
    "    :param model_index:       Index of the model which was trained.\n",
    "    :param pruning_step:      Integer value for the number of pruning steps which had been completed for the model.\n",
    "\n",
    "    :returns: Model object with weights loaded.\n",
    "    \"\"\"\n",
    "    path: str = get_model_directory(model_index, MODEL_DIRECTORY) + get_model_name(model_index, pruning_step)\n",
    "    model: LeNet = LeNet(feature_shape, num_classes)\n",
    "    model.load_weights(path)\n",
    "    return model\n",
    "\n",
    "def prune_model(model: LeNet, pruning_percentage: float) -> LeNet:\n",
    "    \"\"\"\n",
    "    Function to prune a given model according to the specified percentage.\n",
    "\n",
    "    :param model:                 Model to be pruned.\n",
    "    :param pruning_percentage:    Percentage of weights to prune.\n",
    "\n",
    "    :returns: Pruned model.\n",
    "    \"\"\"\n",
    "    pruning_schedule = tfmot.sparsity.keras.ConstantSparsity(pruning_percentage, 0)\n",
    "    # Prune each layer in the model\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Dense):\n",
    "            tfmot.sparsity.keras.prune_low_magnitude(layer, pruning_schedule=pruning_schedule)\n",
    "    return model\n",
    "\n",
    "def lottery_ticket_hypothesis(X_train: np.array, Y_train: np.array, X_test: np.array, Y_test: np.array, total_pruning_percentage: float, pruning_steps: int, epochs_per_step: int, num_models: int):\n",
    "    \"\"\"\n",
    "    Function to perform the lottery ticket hypothesis with iterative magnitude pruning.\n",
    "\n",
    "    :param X_train:                  Training instances.\n",
    "    :param X_test:                   Testing instances.\n",
    "    :param Y_train:                  Training labels.\n",
    "    :param Y_test:                   Testing labels.\n",
    "    :param total_pruning_percentage: The total percentage of weights to prune.\n",
    "    :param pruning_steps:            Number of pruning steps.\n",
    "    :param epochs_per_step:          Number of epochs to train each model for in each step.\n",
    "    :param num_models:               Number of models to create.\n",
    "\n",
    "    \"\"\"\n",
    "    assert pruning_steps > 0, \"Pruning steps should be greater than 0\"\n",
    "    assert total_pruning_percentage > 0 and total_pruning_percentage <= 1, \"Total pruning percentage should be between 0 and 1\"\n",
    "\n",
    "    # Create the original models if they don't already exist\n",
    "    create_models(X_train, Y_train, X_test, Y_test, epochs, num_models)\n",
    "\n",
    "    feature_shape: tuple[int, ...] = X_train[0].shape\n",
    "    num_classes: int = 10\n",
    "    step_pruning_percent: float = total_pruning_percentage / pruning_steps\n",
    "\n",
    "    # Iterate over each model\n",
    "    for model_index in range(num_models):\n",
    "        # Load the model at pruning step 0\n",
    "        model = load_model(feature_shape, num_classes, model_index, 0)\n",
    "\n",
    "        # Iterate over each pruning step\n",
    "        for step in range(pruning_steps):\n",
    "            output_path: str = get_model_directory(model_index, MODEL_DIRECTORY) + get_model_name(model_index, step + 1)\n",
    "\n",
    "            # Prune the model\n",
    "            pruned_model = prune_model(model, step_pruning_percent)\n",
    "\n",
    "            # Train the pruned model\n",
    "            pruned_model.fit(X_train, Y_train, epochs=epochs_per_step, validation_data=(X_test, Y_test), verbose=1)\n",
    "\n",
    "            # Replace the model\n",
    "            model = pruned_model\n",
    "\n",
    "            # Save the pruned model with the corresponding pruning percentage\n",
    "            print(f'Saving model to {output_path}')\n",
    "            pruned_model.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0922 - accuracy: 0.9713 - val_loss: 0.0709 - val_accuracy: 0.9777\n",
      "Saving model to models/model_0/model_0_step_1.keras\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0583 - accuracy: 0.9823 - val_loss: 0.0595 - val_accuracy: 0.9804\n",
      "Saving model to models/model_0/model_0_step_2.keras\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0855 - accuracy: 0.9732 - val_loss: 0.0704 - val_accuracy: 0.9765\n",
      "Saving model to models/model_1/model_1_step_1.keras\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0565 - accuracy: 0.9825 - val_loss: 0.0609 - val_accuracy: 0.9793\n",
      "Saving model to models/model_1/model_1_step_2.keras\n"
     ]
    }
   ],
   "source": [
    "# Parameters for simulation\n",
    "num_models: int = 2\n",
    "epochs: int = 1\n",
    "total_pruning_percentage: float = 0.99\n",
    "pruning_steps: int = 2\n",
    "\n",
    "# Run the simulation\n",
    "lottery_ticket_hypothesis(X_train, Y_train, X_test, Y_test, total_pruning_percentage, pruning_steps, epochs, num_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_shape: tuple = X_train[0].shape\n",
    "num_classes = 10\n",
    "\n",
    "unpruned = load_model(feature_shape, num_classes, 0, 0)\n",
    "pruned_1_step = load_model(feature_shape, num_classes, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conv2d_120/kernel:0' shape=(5, 5, 1, 6) dtype=float32, numpy=\n",
      "array([[[[ 0.15055081, -0.04648927,  0.13614608, -0.03882761,\n",
      "          -0.16479398, -0.09699005]],\n",
      "\n",
      "        [[ 0.14934656,  0.210729  ,  0.11351886,  0.10385656,\n",
      "          -0.00950568,  0.09820782]],\n",
      "\n",
      "        [[ 0.17029496,  0.00887411, -0.01046964,  0.20520319,\n",
      "          -0.08432572,  0.2863829 ]],\n",
      "\n",
      "        [[ 0.08597324, -0.11030123,  0.13981798,  0.25078136,\n",
      "           0.02439029,  0.28530684]],\n",
      "\n",
      "        [[ 0.23382324,  0.21384415, -0.14387712,  0.15070887,\n",
      "          -0.06674127,  0.20849589]]],\n",
      "\n",
      "\n",
      "       [[[ 0.26064548,  0.03713089,  0.01374489,  0.07009594,\n",
      "          -0.28974587,  0.10976572]],\n",
      "\n",
      "        [[ 0.23156106,  0.24122684,  0.32346332,  0.213563  ,\n",
      "           0.02478233, -0.08258878]],\n",
      "\n",
      "        [[ 0.15418793,  0.22747006,  0.27860987, -0.00909648,\n",
      "           0.13668007,  0.3282338 ]],\n",
      "\n",
      "        [[-0.06090546,  0.18914331,  0.23728716,  0.2059265 ,\n",
      "          -0.01768417,  0.3699549 ]],\n",
      "\n",
      "        [[-0.03193311,  0.27278483, -0.05434632, -0.07781712,\n",
      "          -0.04171237,  0.31569138]]],\n",
      "\n",
      "\n",
      "       [[[-0.04580133,  0.05833143,  0.37916002,  0.11505118,\n",
      "          -0.20340787, -0.03084635]],\n",
      "\n",
      "        [[-0.03982277, -0.02852609,  0.36428225,  0.07599812,\n",
      "          -0.00144771,  0.19583532]],\n",
      "\n",
      "        [[-0.0781731 ,  0.11231523,  0.14198685, -0.10222507,\n",
      "          -0.04814408,  0.06645329]],\n",
      "\n",
      "        [[ 0.02745083,  0.14619641, -0.02019139, -0.08804802,\n",
      "           0.28465205,  0.02007677]],\n",
      "\n",
      "        [[-0.21413031,  0.07290479, -0.13233072, -0.23945838,\n",
      "           0.25135314,  0.16152965]]],\n",
      "\n",
      "\n",
      "       [[[-0.29266664, -0.07563586,  0.28643057, -0.2206688 ,\n",
      "           0.00950061, -0.07595232]],\n",
      "\n",
      "        [[-0.21139443,  0.02049037,  0.24635874, -0.16834372,\n",
      "           0.18752691, -0.09896193]],\n",
      "\n",
      "        [[-0.33917475,  0.14909402,  0.21498877, -0.24727058,\n",
      "           0.16775824, -0.04111822]],\n",
      "\n",
      "        [[-0.2836847 , -0.00790363,  0.03917865, -0.09623198,\n",
      "           0.2216563 , -0.01721159]],\n",
      "\n",
      "        [[-0.10584307,  0.15997703, -0.16626179, -0.3403666 ,\n",
      "           0.04885063, -0.00192389]]],\n",
      "\n",
      "\n",
      "       [[[-0.25315365, -0.10089295,  0.20909362, -0.21805118,\n",
      "           0.02419736, -0.10051761]],\n",
      "\n",
      "        [[-0.31632578, -0.085988  ,  0.02111292, -0.24016789,\n",
      "           0.04955476, -0.08797285]],\n",
      "\n",
      "        [[-0.18015419, -0.11889448, -0.10381483, -0.1648986 ,\n",
      "          -0.01037058, -0.03845019]],\n",
      "\n",
      "        [[-0.21480352,  0.0753956 , -0.23380694, -0.29627395,\n",
      "          -0.03304522, -0.287945  ]],\n",
      "\n",
      "        [[-0.27380925, -0.2681442 , -0.08665811, -0.08954302,\n",
      "           0.06960922, -0.22121674]]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(unpruned.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conv2d_150/kernel:0' shape=(5, 5, 1, 6) dtype=float32, numpy=\n",
      "array([[[[ 0.18826242, -0.04832909,  0.16186419, -0.02997263,\n",
      "          -0.20247193, -0.14148208]],\n",
      "\n",
      "        [[ 0.18260993,  0.21457845,  0.2472481 ,  0.12703685,\n",
      "          -0.06035525,  0.09343925]],\n",
      "\n",
      "        [[ 0.20517176, -0.00828617,  0.09555618,  0.24260606,\n",
      "          -0.13351694,  0.32730266]],\n",
      "\n",
      "        [[ 0.13112812, -0.15664782,  0.1355724 ,  0.2979849 ,\n",
      "          -0.01625589,  0.33901107]],\n",
      "\n",
      "        [[ 0.2896714 ,  0.14561923, -0.21202591,  0.21192057,\n",
      "          -0.13219354,  0.22461501]]],\n",
      "\n",
      "\n",
      "       [[[ 0.32934147,  0.04303245,  0.04833424,  0.11902695,\n",
      "          -0.37549144,  0.07109149]],\n",
      "\n",
      "        [[ 0.2749379 ,  0.2846999 ,  0.5016536 ,  0.27438253,\n",
      "          -0.03898301, -0.06852957]],\n",
      "\n",
      "        [[ 0.18514828,  0.29550833,  0.35536554,  0.04789002,\n",
      "           0.11973194,  0.41269356]],\n",
      "\n",
      "        [[-0.01720571,  0.257133  ,  0.20807095,  0.25683478,\n",
      "          -0.01128332,  0.4826964 ]],\n",
      "\n",
      "        [[ 0.03345275,  0.2958355 , -0.12499644, -0.0177728 ,\n",
      "          -0.07836439,  0.3742959 ]]],\n",
      "\n",
      "\n",
      "       [[[-0.00624931,  0.03791959,  0.43566015,  0.1427346 ,\n",
      "          -0.2665327 , -0.07056513]],\n",
      "\n",
      "        [[-0.05263709, -0.01776485,  0.54416215,  0.08936638,\n",
      "           0.00205989,  0.20168045]],\n",
      "\n",
      "        [[-0.10583708,  0.14837942,  0.1693917 , -0.10968848,\n",
      "           0.02414726,  0.11100839]],\n",
      "\n",
      "        [[ 0.03116111,  0.18441817, -0.05411131, -0.09520636,\n",
      "           0.3724734 ,  0.04969553]],\n",
      "\n",
      "        [[-0.17692901,  0.07004642, -0.19124898, -0.21742153,\n",
      "           0.26468253,  0.13887016]]],\n",
      "\n",
      "\n",
      "       [[[-0.33220574, -0.12524408,  0.40929982, -0.26786694,\n",
      "          -0.01667329, -0.12027692]],\n",
      "\n",
      "        [[-0.3166965 , -0.01774885,  0.37250584, -0.26620576,\n",
      "           0.22592019, -0.11470969]],\n",
      "\n",
      "        [[-0.46056905,  0.11034764,  0.22792245, -0.37514952,\n",
      "           0.27210122, -0.05176597]],\n",
      "\n",
      "        [[-0.35043046, -0.05863248,  0.00184661, -0.18932901,\n",
      "           0.32759485, -0.05952325]],\n",
      "\n",
      "        [[-0.12325682,  0.08871961, -0.21790127, -0.3686501 ,\n",
      "           0.07134837, -0.08113031]]],\n",
      "\n",
      "\n",
      "       [[[-0.2884234 , -0.16184668,  0.3397268 , -0.26004705,\n",
      "          -0.01515283, -0.14620525]],\n",
      "\n",
      "        [[-0.3830871 , -0.15108326,  0.10322329, -0.30929783,\n",
      "           0.04162097, -0.12580152]],\n",
      "\n",
      "        [[-0.23960364, -0.1997324 , -0.10879802, -0.22353762,\n",
      "          -0.00070837, -0.08456565]],\n",
      "\n",
      "        [[-0.23230344, -0.01648505, -0.284273  , -0.30755642,\n",
      "          -0.05113103, -0.3474518 ]],\n",
      "\n",
      "        [[-0.2742165 , -0.36087877, -0.13749237, -0.0737378 ,\n",
      "           0.00470684, -0.28529665]]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(pruned_1_step.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with -9)."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Doesn't work when running but can select \"Launch \"\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for computing metrics on lottery ticket similarities.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for visualizing results.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
